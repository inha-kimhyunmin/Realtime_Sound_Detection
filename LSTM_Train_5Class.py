"""
5í´ë˜ìŠ¤ LSTM ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
================================

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” YAMNet + LSTMì„ ì‚¬ìš©í•˜ì—¬ 5ê°œ í´ë˜ìŠ¤(ë¬´ìŒ, ì •ìƒ, í™”ì¬, ê°€ìŠ¤ëˆ„ì¶œ, ë¹„ëª…)ë¥¼  
ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.

ë°ì´í„° ê°€ì¤‘ì¹˜ ì„¤ì • ë°©ë²•:
- VERSION: ëª¨ë¸ ë²„ì „ (ê²°ê³¼ë¬¼ í´ë”ëª…ì— ì‚¬ìš©)
- SILENCE_SAMPLES: ë¬´ìŒ ë°ì´í„° ìƒ˜í”Œ ìˆ˜
- NORMAL_SAMPLES: ì •ìƒ(ê³µì¥ì†ŒìŒ) ë°ì´í„° ìƒ˜í”Œ ìˆ˜  
- TRANSITION_SAMPLES: ë¬´ìŒâ†’ê³µì¥ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒ˜í”Œ ìˆ˜
- DANGER_TRANSITION_SAMPLES: ë¬´ìŒâ†’ìœ„í—˜ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒ˜í”Œ ìˆ˜ (ìƒˆë¡œ ì¶”ê°€)
- AUTO_WEIGHT_CALCULATION: ìë™ ê°€ì¤‘ì¹˜ ê³„ì‚° ì—¬ë¶€
  - True: envsound í´ë”ì˜ íŒŒì¼ ê°œìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìë™ ê³„ì‚°
  - False: MANUAL_DANGER_WEIGHTS ì‚¬ìš©
- MANUAL_DANGER_WEIGHTS: ìˆ˜ë™ ì„¤ì • ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬
  - 'fire': í™”ì¬ ì†ŒìŒ ê°€ì¤‘ì¹˜
  - 'gas': ê°€ìŠ¤ëˆ„ì¶œ ì†ŒìŒ ê°€ì¤‘ì¹˜  
  - 'scream': ë¹„ëª… ì†ŒìŒ ê°€ì¤‘ì¹˜

ìƒˆë¡œ ì¶”ê°€ëœ ì „í™˜ ë°ì´í„°:
- ë¬´ìŒ ìƒíƒœì—ì„œ ê°‘ìê¸° ê³µì¥ ì†Œë¦¬ê°€ ì‹œì‘ë˜ëŠ” í˜„ì‹¤ì ì¸ ì‹œë‚˜ë¦¬ì˜¤
- ë¬´ìŒ ìƒíƒœì—ì„œ ê°‘ìê¸° ìœ„í—˜ ì†Œë¦¬ê°€ ì‹œì‘ë˜ëŠ” ê¸´ê¸‰ ìƒí™© ì‹œë‚˜ë¦¬ì˜¤ (ìƒˆë¡œ ì¶”ê°€)
- ì „ì²´ ê¸¸ì´ì˜ 20~80% ì§€ì ì—ì„œ ë¬´ìŒâ†’ê³µì¥ì†Œë¦¬/ìœ„í—˜ì†Œë¦¬ ì „í™˜
- 0.5ì´ˆ í˜ì´ë“œì¸ íš¨ê³¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì „í™˜ êµ¬í˜„
- ë¬´ìŒ êµ¬ê°„ì€ í´ë˜ìŠ¤ 0, ì „í™˜ í›„ êµ¬ê°„ì€ í•´ë‹¹ í´ë˜ìŠ¤ë¡œ ë¼ë²¨ë§

ìë™ ê°€ì¤‘ì¹˜ ê³„ì‚°:
- ê° í´ë˜ìŠ¤ë‹¹ ëª©í‘œ ìƒ˜í”Œ ìˆ˜ë¥¼ 250ê°œë¡œ ì„¤ì • (ì¡°ì •ë¨)
- íŒŒì¼ ê°œìˆ˜ê°€ ë§ì€ í´ë˜ìŠ¤ëŠ” ë‚®ì€ ê°€ì¤‘ì¹˜, ì ì€ í´ë˜ìŠ¤ëŠ” ë†’ì€ ê°€ì¤‘ì¹˜
- ê°€ì¤‘ì¹˜ê°€ 1 ë¯¸ë§Œì´ë©´ í™•ë¥ ì  ìƒ˜í”Œë§ ì ìš©

ì¶œë ¥ íŒŒì¼ (ë²„ì „ë³„ í´ë”ì— ì €ì¥):
- yamnet_lstm_model_{VERSION}.h5: í•™ìŠµëœ ëª¨ë¸
- model_info_{VERSION}.pkl: ëª¨ë¸ ì •ë³´ (í´ë˜ìŠ¤ ë§¤í•‘, ì´ë¦„ ë“±)
- model_performance_{VERSION}.txt: ìƒì„¸í•œ í•™ìŠµ ê²°ê³¼ ë³´ê³ ì„œ
- dataset_info_{VERSION}.json: í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ íŒŒì¼ ì •ë³´
- summary_{VERSION}.json: ìš”ì•½ ì •ë³´ (ì„±ëŠ¥, ì„¤ì •ê°’ ë“±)

í´ë” êµ¬ì¡°:
model_results_{VERSION}_{YYYYMMDD_HHMMSS}/
â”œâ”€â”€ yamnet_lstm_model_{VERSION}.h5
â”œâ”€â”€ model_info_{VERSION}.pkl  
â”œâ”€â”€ model_performance_{VERSION}.txt
â”œâ”€â”€ dataset_info_{VERSION}.json
â””â”€â”€ summary_{VERSION}.json
"""

import numpy as np
import librosa
import soundfile as sf
import random
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
import tensorflow_hub as hub
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import os
import glob
import pickle
import json

# ================================
# ë°ì´í„° ìƒì„± ê°€ì¤‘ì¹˜ ì„¤ì • (ìˆ˜ì • ê°€ëŠ¥)
# ================================
SILENCE_SAMPLES = 200      # ë¬´ìŒ ë°ì´í„° ìƒ˜í”Œ ìˆ˜ (ì¦ê°€)
NORMAL_SAMPLES = 180       # ì •ìƒ(ê³µì¥ì†ŒìŒ) ë°ì´í„° ìƒ˜í”Œ ìˆ˜ (ì¦ê°€)
TRANSITION_SAMPLES = 100   # ë¬´ìŒâ†’ê³µì¥ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒ˜í”Œ ìˆ˜ (ì¦ê°€)
DANGER_TRANSITION_SAMPLES = 90  # ë¬´ìŒâ†’ìœ„í—˜ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒ˜í”Œ ìˆ˜ (ì¦ê°€)

# ë²„ì „ ê´€ë¦¬ ì„¤ì •
VERSION = "v1.2"  # ëª¨ë¸ ë²„ì „ (ê²°ê³¼ë¬¼ í´ë”ëª…ì— ì‚¬ìš©)

# ìœ„í—˜ ì†ŒìŒë³„ ê°€ì¤‘ì¹˜ (íŒŒì¼ë‹¹ ìƒì„±í•  ìƒ˜í”Œ ìˆ˜)
# ìë™ ê³„ì‚° ë˜ëŠ” ìˆ˜ë™ ì„¤ì • ê°€ëŠ¥
AUTO_WEIGHT_CALCULATION = True  # True: íŒŒì¼ ê°œìˆ˜ ê¸°ë°˜ ìë™ ê³„ì‚°, False: ìˆ˜ë™ ì„¤ì •

# ìˆ˜ë™ ì„¤ì •ì‹œ ì‚¬ìš©ë˜ëŠ” ê°€ì¤‘ì¹˜ (ì†Œìˆ˜ì  ì¡°ì ˆ ê°€ëŠ¥)
MANUAL_DANGER_WEIGHTS = {
    'fire': 0.3,      # í™”ì¬: í™•ë¥ ì  ìƒ˜í”Œë§ (30% í™•ë¥ )
    'gas': 20.5,      # ê°€ìŠ¤ëˆ„ì¶œ: íŒŒì¼ë‹¹ 20.5ê°œ ìƒ˜í”Œ (ì¼ë¶€ íŒŒì¼ì€ 20ê°œ, ì¼ë¶€ëŠ” 21ê°œ)
    'scream': 12.7    # ë¹„ëª…: íŒŒì¼ë‹¹ 12.7ê°œ ìƒ˜í”Œ (ì¼ë¶€ íŒŒì¼ì€ 12ê°œ, ì¼ë¶€ëŠ” 13ê°œ)
}

def calculate_auto_weights(envsound_folder, target_samples_per_class=250):
    """
    ê° í´ë˜ìŠ¤ì˜ íŒŒì¼ ê°œìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìë™ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        envsound_folder: ìœ„í—˜ ì†ŒìŒ í´ë” ê²½ë¡œ
        target_samples_per_class: ê° í´ë˜ìŠ¤ë‹¹ ëª©í‘œ ìƒ˜í”Œ ìˆ˜ (250ìœ¼ë¡œ ì¡°ì •)
    
    Returns:
        dict: ê° í´ë˜ìŠ¤ë³„ íŒŒì¼ë‹¹ ìƒ˜í”Œ ìˆ˜
    """
    event_folders = ['fire', 'gas', 'scream']  # sparkëŠ” ì œì™¸ (í´ë˜ìŠ¤ ë§¤í•‘ì— ì—†ìŒ)
    file_counts = {}
    weights = {}
    
    print("ğŸ“Š ìœ„í—˜ ì†ŒìŒ íŒŒì¼ ê°œìˆ˜ ë¶„ì„:")
    print("-" * 40)
    
    # ê° í´ë”ì˜ íŒŒì¼ ê°œìˆ˜ ê³„ì‚°
    for folder in event_folders:
        folder_path = os.path.join(envsound_folder, folder)
        if os.path.exists(folder_path):
            wav_files = glob.glob(os.path.join(folder_path, '*.wav'))
            mp3_files = glob.glob(os.path.join(folder_path, '*.mp3'))
            total_files = len(wav_files) + len(mp3_files)
            file_counts[folder] = total_files
            print(f"  {folder}: {total_files}ê°œ íŒŒì¼")
        else:
            file_counts[folder] = 0
            print(f"  {folder}: í´ë” ì—†ìŒ")
    
    print(f"\nğŸ¯ ëª©í‘œ: ê° í´ë˜ìŠ¤ë‹¹ {target_samples_per_class}ê°œ ìƒ˜í”Œ ìƒì„±")
    print("âš–ï¸ ìë™ ê³„ì‚°ëœ ê°€ì¤‘ì¹˜:")
    print("-" * 40)
    
    # ê°€ì¤‘ì¹˜ ê³„ì‚°
    for folder, file_count in file_counts.items():
        if file_count > 0:
            samples_per_file = target_samples_per_class / file_count
            
            # ë„ˆë¬´ ì‘ì€ ê°’ì€ í™•ë¥ ì  ìƒ˜í”Œë§ìœ¼ë¡œ ì²˜ë¦¬
            if samples_per_file < 1:
                weights[folder] = round(samples_per_file, 2)
                print(f"  {folder}: {samples_per_file:.2f} (í™•ë¥ ì  ìƒ˜í”Œë§: {samples_per_file*100:.1f}%)")
            else:
                weights[folder] = max(1, round(samples_per_file))
                print(f"  {folder}: {weights[folder]} (íŒŒì¼ë‹¹ ìƒ˜í”Œ ìˆ˜)")
        else:
            weights[folder] = 0
            print(f"  {folder}: 0 (íŒŒì¼ ì—†ìŒ)")
    
    print(f"\nì˜ˆìƒ ì´ ìƒ˜í”Œ ìˆ˜:")
    total_expected = 0
    for folder, weight in weights.items():
        expected = file_counts[folder] * weight if weight >= 1 else file_counts[folder] * weight
        total_expected += expected
        print(f"  {folder}: {expected:.0f}ê°œ")
    print(f"  ì´í•©: {total_expected:.0f}ê°œ")
    
    return weights

def create_version_folder(version):
    """
    ë²„ì „ë³„ ê²°ê³¼ë¬¼ ì €ì¥ í´ë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        version: ë²„ì „ ë¬¸ìì—´ (ì˜ˆ: "v1.0")
    
    Returns:
        str: ìƒì„±ëœ í´ë” ê²½ë¡œ
    """
    from datetime import datetime
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    folder_name = f"model_results_{version}_{current_time}"
    
    if not os.path.exists(folder_name):
        os.makedirs(folder_name)
        print(f"ğŸ“ ê²°ê³¼ë¬¼ í´ë” ìƒì„±: {folder_name}")
    
    return folder_name

# ---------------------------
# 1) ë¬´ìŒ ì œê±° í•¨ìˆ˜
# ---------------------------
def remove_silence(y, sr, top_db=20):
    intervals = librosa.effects.split(y, top_db=top_db)
    if len(intervals) == 0:
        return y
    non_silent_audio = np.concatenate([y[start:end] for start, end in intervals])
    return non_silent_audio

# ---------------------------
# 2) ë¬´ìŒ ìƒì„± í•¨ìˆ˜
# ---------------------------
def generate_silence(duration_sec, sr):
    """ë¬´ìŒ ë°ì´í„° ìƒì„±"""
    length = int(sr * duration_sec)
    # ì™„ì „í•œ ë¬´ìŒì´ ì•„ë‹Œ ë§¤ìš° ì‘ì€ ë…¸ì´ì¦ˆ ì¶”ê°€ (í˜„ì‹¤ì ì¸ ë¬´ìŒ)
    silence = np.random.normal(0, 0.001, length).astype(np.float32)
    return silence

# ---------------------------
# 3) ì €ìŒëŸ‰ ë°°ê²½ ì†ŒìŒ ìƒì„± í•¨ìˆ˜
# ---------------------------
def generate_background_noise(duration_sec, sr):
    """ì €ìŒëŸ‰ ë°°ê²½ ì†ŒìŒ ìƒì„± (ì—ì–´ì»¨, ë¯¸ì„¸í•œ ì†ŒìŒ ë“±)"""
    length = int(sr * duration_sec)
    # ì €ì£¼íŒŒ ë…¸ì´ì¦ˆ ìƒì„±
    noise = np.random.normal(0, 0.01, length).astype(np.float32)
    # ì €ì—­ í†µê³¼ í•„í„° íš¨ê³¼ (ê°„ë‹¨í•œ ì´ë™í‰ê· )
    window_size = 50
    noise_filtered = np.convolve(noise, np.ones(window_size)/window_size, mode='same')
    return noise_filtered

# ---------------------------
# 4) ê³µì¥ ì†Œë¦¬ì™€ ìœ„í—˜ ì†Œë¦¬ í•©ì„± í•¨ìˆ˜
# ---------------------------
def mix_factory_and_event(factory_audio, event_audio, sr, desired_length=10.0):
    event_len = len(event_audio)
    factory_len = len(factory_audio)
    target_len = int(sr * desired_length)
    
    if factory_len < target_len:
        factory_audio = np.pad(factory_audio, (0, target_len - factory_len))
    else:
        factory_audio = factory_audio[:target_len]
    
    if event_len > target_len:
        event_audio = event_audio[:target_len] 
        event_len = target_len
    
    if event_len == 0:
        return factory_audio, 0, 0
        
    max_start = max(0, target_len - event_len)
    insert_pos = random.randint(0, max_start) if max_start > 0 else 0
    
    rms_factory = np.sqrt(np.mean(factory_audio**2))
    rms_event = np.sqrt(np.mean(event_audio**2))
    if rms_event > 0:
        # ìœ„í—˜ ì†Œë¦¬ì˜ ë³¼ë¥¨ì„ ëœë¤í•˜ê²Œ ì¡°ì • (ë” í˜„ì‹¤ì )
        volume_factor = random.uniform(0.3, 0.8)
        event_audio = event_audio * (rms_factory / rms_event) * volume_factor
    
    mixed_audio = factory_audio.copy()
    mixed_audio[insert_pos:insert_pos+event_len] += event_audio
    
    max_val = np.max(np.abs(mixed_audio))
    if max_val > 1.0:
        mixed_audio = mixed_audio / max_val
    
    return mixed_audio, insert_pos / sr, (insert_pos + event_len) / sr

# ---------------------------
# 5) YAMNet ì„ë² ë”© ì¶”ì¶œ í•¨ìˆ˜
# ---------------------------
def extract_yamnet_embeddings(audio, sr, yamnet_model):
    waveform = tf.convert_to_tensor(audio, dtype=tf.float32)
    waveform = tf.squeeze(waveform)
    yamnet_fn = yamnet_model.signatures['serving_default']
    yamnet_output = yamnet_fn(waveform=waveform)
    embeddings = yamnet_output['output_1'].numpy()  # (frames, 1024)
    return embeddings

# ---------------------------
# 6) ë¼ë²¨ ìƒì„± í•¨ìˆ˜ (5-í´ë˜ìŠ¤)
# ---------------------------
def generate_labels(start_sec, end_sec, class_id, total_duration=10.0, frame_length=0.48):
    """
    class_id: 0=ë¬´ìŒ, 1=ì •ìƒ(ê³µì¥ì†Œë¦¬), 2=í™”ì¬, 3=ê°€ìŠ¤ëˆ„ì¶œ, 4=ë¹„ëª…
    """
    num_frames = int(total_duration / frame_length)
    labels = np.zeros(num_frames, dtype=int)  # ê¸°ë³¸ê°’: ë¬´ìŒ(0)
    
    if class_id > 0:  # ë¬´ìŒì´ ì•„ë‹Œ ê²½ìš°
        if start_sec is not None and end_sec is not None and class_id > 1:
            # ìœ„í—˜ ì†Œë¦¬ê°€ ìˆëŠ” ê²½ìš° (í™”ì¬, ê°€ìŠ¤, ë¹„ëª…)
            start_frame = int(start_sec / frame_length)
            end_frame = int(end_sec / frame_length) + 1
            labels[start_frame:end_frame] = class_id
            # ë‚˜ë¨¸ì§€ êµ¬ê°„ì€ ì •ìƒ(ê³µì¥ì†Œë¦¬)ë¡œ ì„¤ì •
            labels[:start_frame] = 1
            labels[end_frame:] = 1
        else:
            # ì „ì²´ê°€ í•´ë‹¹ í´ë˜ìŠ¤ (ë¬´ìŒ ë˜ëŠ” ì •ìƒ)
            labels[:] = class_id
    
    return labels

# ---------------------------
# 7) LSTM ëª¨ë¸ ì •ì˜ í•¨ìˆ˜ (5-í´ë˜ìŠ¤)
# ---------------------------
def create_lstm_model(input_shape, num_classes=5):
    model = Sequential([
        LSTM(128, input_shape=input_shape, return_sequences=True),
        Dropout(0.4),
        LSTM(64, return_sequences=True),
        Dropout(0.4),
        LSTM(32, return_sequences=True),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dense(num_classes, activation='softmax')
    ])
    
    optimizer = Adam(learning_rate=0.001)
    
    model.compile(optimizer=optimizer,
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# ---------------------------
# 9) ë¬´ìŒì—ì„œ ê³µì¥ì†Œë¦¬ë¡œ ì „í™˜ë˜ëŠ” ë°ì´í„° ìƒì„± í•¨ìˆ˜
# ---------------------------
def create_silence_to_factory_transition(factory_audio, sr, total_duration=10.0):
    """
    ë¬´ìŒ ìƒíƒœì—ì„œ ê³µì¥ ì†Œë¦¬ê°€ ì‹œì‘ë˜ëŠ” ì „í™˜ ë°ì´í„° ìƒì„±
    
    Args:
        factory_audio: ê³µì¥ ì†Œë¦¬ ì˜¤ë””ì˜¤
        sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸
        total_duration: ì´ ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
    
    Returns:
        tuple: (ì „í™˜_ì˜¤ë””ì˜¤, ê³µì¥ì†Œë¦¬_ì‹œì‘ì‹œê°„)
    """
    target_len = int(sr * total_duration)
    
    # ê³µì¥ ì†Œë¦¬ ì¤€ë¹„
    if len(factory_audio) > target_len:
        start_pos = random.randint(0, len(factory_audio) - target_len)
        factory_audio = factory_audio[start_pos:start_pos + target_len]
    else:
        factory_audio = np.pad(factory_audio, (0, max(0, target_len - len(factory_audio))))
    
    # ì „í™˜ ì‹œì  ê²°ì • (ì „ì²´ ê¸¸ì´ì˜ 20%~80% ì§€ì ì—ì„œ ì‹œì‘)
    transition_start_ratio = random.uniform(0.2, 0.8)
    transition_frame = int(target_len * transition_start_ratio)
    
    # ë¬´ìŒ êµ¬ê°„ê³¼ ê³µì¥ì†Œë¦¬ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
    mixed_audio = np.zeros(target_len, dtype=np.float32)
    
    # ì•ë¶€ë¶„ì€ ë¬´ìŒ (ë§¤ìš° ì‘ì€ ë°°ê²½ ë…¸ì´ì¦ˆ)
    silence_part = generate_background_noise(transition_frame / sr, sr)
    mixed_audio[:transition_frame] = silence_part[:transition_frame]
    
    # ë’·ë¶€ë¶„ì€ ê³µì¥ ì†Œë¦¬
    factory_part = factory_audio[transition_frame:]
    mixed_audio[transition_frame:] = factory_part
    
    # ì „í™˜ ì§€ì ì—ì„œ ë¶€ë“œëŸ¬ìš´ fade-in íš¨ê³¼ (ë” í˜„ì‹¤ì )
    fade_duration = int(sr * 0.5)  # 0.5ì´ˆ í˜ì´ë“œì¸
    if transition_frame + fade_duration < target_len:
        fade_samples = min(fade_duration, len(factory_part))
        fade_curve = np.linspace(0, 1, fade_samples)
        mixed_audio[transition_frame:transition_frame + fade_samples] *= fade_curve
    
    # ë³¼ë¥¨ ì •ê·œí™”
    max_val = np.max(np.abs(mixed_audio))
    if max_val > 1.0:
        mixed_audio = mixed_audio / max_val
    
    transition_start_sec = transition_frame / sr
    
    return mixed_audio, transition_start_sec

# ---------------------------
# 10) ë¬´ìŒâ†’ê³µì¥ì†Œë¦¬ ì „í™˜ ë¼ë²¨ ìƒì„± í•¨ìˆ˜
# ---------------------------
def generate_transition_labels(transition_start_sec, total_duration=10.0, frame_length=0.48):
    """
    ë¬´ìŒâ†’ê³µì¥ì†Œë¦¬ ì „í™˜ì— ëŒ€í•œ í”„ë ˆì„ë³„ ë¼ë²¨ ìƒì„±
    
    Args:
        transition_start_sec: ê³µì¥ì†Œë¦¬ ì‹œì‘ ì‹œê°„ (ì´ˆ)
        total_duration: ì´ ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
        frame_length: í”„ë ˆì„ ê¸¸ì´ (ì´ˆ)
    
    Returns:
        numpy.array: í”„ë ˆì„ë³„ í´ë˜ìŠ¤ ë¼ë²¨ (0=ë¬´ìŒ, 1=ì •ìƒ)
    """
    num_frames = int(total_duration / frame_length)
    labels = np.zeros(num_frames, dtype=int)  # ê¸°ë³¸ê°’: ë¬´ìŒ(0)
    
    # ì „í™˜ ì‹œì  ê³„ì‚°
    transition_frame = int(transition_start_sec / frame_length)
    
    # ì „í™˜ ì´í›„ëŠ” ì •ìƒ(ê³µì¥ì†Œë¦¬)ë¡œ ì„¤ì •
    labels[transition_frame:] = 1
    
    return labels

# ---------------------------
# 11) ë¬´ìŒì—ì„œ ìœ„í—˜ì†Œë¦¬ë¡œ ì „í™˜ë˜ëŠ” ë°ì´í„° ìƒì„± í•¨ìˆ˜
# ---------------------------
def create_silence_to_danger_transition(event_audio, sr, class_id, total_duration=10.0):
    """
    ë¬´ìŒ ìƒíƒœì—ì„œ ìœ„í—˜ ì†Œë¦¬ê°€ ì‹œì‘ë˜ëŠ” ì „í™˜ ë°ì´í„° ìƒì„±
    
    Args:
        event_audio: ìœ„í—˜ ì†Œë¦¬ ì˜¤ë””ì˜¤
        sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸
        class_id: ìœ„í—˜ì†Œë¦¬ í´ë˜ìŠ¤ ID (2=í™”ì¬, 3=ê°€ìŠ¤, 4=ë¹„ëª…)
        total_duration: ì´ ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
    
    Returns:
        tuple: (ì „í™˜_ì˜¤ë””ì˜¤, ìœ„í—˜ì†Œë¦¬_ì‹œì‘ì‹œê°„, ìœ„í—˜ì†Œë¦¬_ëì‹œê°„)
    """
    target_len = int(sr * total_duration)
    
    # ìœ„í—˜ ì†Œë¦¬ ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ë‚´ê¸°)
    max_event_len = int(sr * 6.0)  # ìµœëŒ€ 6ì´ˆ
    if len(event_audio) > max_event_len:
        start_pos = random.randint(0, len(event_audio) - max_event_len)
        event_audio = event_audio[start_pos:start_pos + max_event_len]
    
    event_len = len(event_audio)
    
    # ì „í™˜ ì‹œì  ê²°ì • (ì „ì²´ ê¸¸ì´ì˜ 20%~60% ì§€ì ì—ì„œ ì‹œì‘, ìœ„í—˜ì†Œë¦¬ë¥¼ ìœ„í•´ ë” ì¼ì°)
    transition_start_ratio = random.uniform(0.2, 0.6)
    transition_frame = int(target_len * transition_start_ratio)
    
    # ìœ„í—˜ì†Œë¦¬ê°€ ëë‚˜ëŠ” ì§€ì  ê³„ì‚°
    event_end_frame = min(transition_frame + event_len, target_len)
    actual_event_len = event_end_frame - transition_frame
    
    # ì „ì²´ ì˜¤ë””ì˜¤ ìƒì„±
    mixed_audio = np.zeros(target_len, dtype=np.float32)
    
    # ì•ë¶€ë¶„ì€ ë¬´ìŒ (ë§¤ìš° ì‘ì€ ë°°ê²½ ë…¸ì´ì¦ˆ)
    silence_part = generate_background_noise(transition_frame / sr, sr)
    mixed_audio[:transition_frame] = silence_part[:transition_frame]
    
    # ì¤‘ê°„ ë¶€ë¶„ì€ ìœ„í—˜ ì†Œë¦¬
    if actual_event_len > 0:
        event_part = event_audio[:actual_event_len]
        
        # ìœ„í—˜ì†Œë¦¬ ë³¼ë¥¨ ì¡°ì • (ë” í˜„ì‹¤ì ì¸ ë³¼ë¥¨)
        event_rms = np.sqrt(np.mean(event_part**2))
        if event_rms > 0:
            # ìœ„í—˜ì†Œë¦¬ëŠ” ë°°ê²½ë³´ë‹¤ ì¶©ë¶„íˆ í¬ê²Œ (í•˜ì§€ë§Œ í´ë¦¬í•‘ ë°©ì§€)
            target_rms = random.uniform(0.1, 0.3)
            volume_factor = target_rms / event_rms
            event_part = event_part * volume_factor
        
        mixed_audio[transition_frame:event_end_frame] = event_part
        
        # ì „í™˜ ì§€ì ì—ì„œ ë¶€ë“œëŸ¬ìš´ fade-in íš¨ê³¼
        fade_duration = int(sr * 0.3)  # 0.3ì´ˆ í˜ì´ë“œì¸ (ìœ„í—˜ì†Œë¦¬ëŠ” ë¹ ë¥´ê²Œ)
        if transition_frame + fade_duration < event_end_frame:
            fade_samples = min(fade_duration, actual_event_len)
            fade_curve = np.linspace(0, 1, fade_samples)
            mixed_audio[transition_frame:transition_frame + fade_samples] *= fade_curve
    
    # ë’·ë¶€ë¶„ì€ ë‹¤ì‹œ ë¬´ìŒ (ìœ„í—˜ì†Œë¦¬ í›„ ì¡°ìš©í•´ì§)
    if event_end_frame < target_len:
        remaining_silence = generate_background_noise((target_len - event_end_frame) / sr, sr)
        mixed_audio[event_end_frame:] = remaining_silence[:target_len - event_end_frame]
    
    # ë³¼ë¥¨ ì •ê·œí™”
    max_val = np.max(np.abs(mixed_audio))
    if max_val > 1.0:
        mixed_audio = mixed_audio / max_val
    
    transition_start_sec = transition_frame / sr
    transition_end_sec = event_end_frame / sr
    
    return mixed_audio, transition_start_sec, transition_end_sec

# ---------------------------
# 12) ë¬´ìŒâ†’ìœ„í—˜ì†Œë¦¬ ì „í™˜ ë¼ë²¨ ìƒì„± í•¨ìˆ˜
# ---------------------------
def generate_danger_transition_labels(transition_start_sec, transition_end_sec, class_id, total_duration=10.0, frame_length=0.48):
    """
    ë¬´ìŒâ†’ìœ„í—˜ì†Œë¦¬ ì „í™˜ì— ëŒ€í•œ í”„ë ˆì„ë³„ ë¼ë²¨ ìƒì„±
    
    Args:
        transition_start_sec: ìœ„í—˜ì†Œë¦¬ ì‹œì‘ ì‹œê°„ (ì´ˆ)
        transition_end_sec: ìœ„í—˜ì†Œë¦¬ ë ì‹œê°„ (ì´ˆ)
        class_id: ìœ„í—˜ì†Œë¦¬ í´ë˜ìŠ¤ ID (2=í™”ì¬, 3=ê°€ìŠ¤, 4=ë¹„ëª…)
        total_duration: ì´ ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
        frame_length: í”„ë ˆì„ ê¸¸ì´ (ì´ˆ)
    
    Returns:
        numpy.array: í”„ë ˆì„ë³„ í´ë˜ìŠ¤ ë¼ë²¨ (0=ë¬´ìŒ, class_id=ìœ„í—˜ì†Œë¦¬)
    """
    num_frames = int(total_duration / frame_length)
    labels = np.zeros(num_frames, dtype=int)  # ê¸°ë³¸ê°’: ë¬´ìŒ(0)
    
    # ì „í™˜ ì‹œì  ê³„ì‚°
    start_frame = int(transition_start_sec / frame_length)
    end_frame = int(transition_end_sec / frame_length)
    
    # ìœ„í—˜ì†Œë¦¬ êµ¬ê°„ì€ í•´ë‹¹ í´ë˜ìŠ¤ë¡œ ì„¤ì •
    labels[start_frame:end_frame] = class_id
    
    # ë‚˜ë¨¸ì§€ëŠ” ë¬´ìŒ(0)ìœ¼ë¡œ ìœ ì§€
    
    return labels

# ---------------------------
# 13) ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
# ---------------------------
def main():
    sr = 16000
    frame_length = 0.48
    total_duration = 10.0
    num_classes = 5  # ë¬´ìŒ(0), ì •ìƒ(1), í™”ì¬(2), ê°€ìŠ¤ëˆ„ì¶œ(3), ë¹„ëª…(4)
    
    # ë²„ì „ë³„ ê²°ê³¼ë¬¼ í´ë” ìƒì„±
    output_folder = create_version_folder(VERSION)
    
    # í´ë˜ìŠ¤ ë§¤í•‘
    class_mapping = {
        'fire': 2,
        'gas': 3, 
        'scream': 4
    }
    
    # 1) ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    mixture_folder = 'mixture'
    envsound_folder = 'envsound'
    
    # 2) ê³µì¥ ì†Œë¦¬ íŒŒì¼ë“¤ ë¶ˆëŸ¬ì˜¤ê¸°
    factory_paths = glob.glob(os.path.join(mixture_folder, '*.wav'))
    print(f"ê³µì¥ ì†Œë¦¬ íŒŒì¼ ìˆ˜: {len(factory_paths)}")
    
    # 3) ìœ„í—˜ ì†Œë¦¬ íŒŒì¼ë“¤ ë¶ˆëŸ¬ì˜¤ê¸° (í´ë˜ìŠ¤ë³„ë¡œ ë¶„ë¦¬)
    event_folders = ['fire', 'gas', 'scream']
    event_data = {}
    
    for folder in event_folders:
        folder_path = os.path.join(envsound_folder, folder)
        wav_files = glob.glob(os.path.join(folder_path, '*.wav'))
        mp3_files = glob.glob(os.path.join(folder_path, '*.mp3'))
        event_data[folder] = wav_files + mp3_files
        print(f"{folder}: {len(event_data[folder])}ê°œ íŒŒì¼")
    
    # ê°€ì¤‘ì¹˜ ì„¤ì • (ìë™ ë˜ëŠ” ìˆ˜ë™)
    if AUTO_WEIGHT_CALCULATION:
        print(f"\nğŸ”„ ìë™ ê°€ì¤‘ì¹˜ ê³„ì‚° ëª¨ë“œ")
        DANGER_WEIGHTS = calculate_auto_weights(envsound_folder, target_samples_per_class=250)
    else:
        print(f"\nâš™ï¸ ìˆ˜ë™ ê°€ì¤‘ì¹˜ ì„¤ì • ëª¨ë“œ")
        DANGER_WEIGHTS = MANUAL_DANGER_WEIGHTS.copy()
        print("ì„¤ì •ëœ ê°€ì¤‘ì¹˜:")
        for class_name, weight in DANGER_WEIGHTS.items():
            print(f"  {class_name}: {weight}")
    
    print(f"\nìµœì¢… ì‚¬ìš©ë  ê°€ì¤‘ì¹˜: {DANGER_WEIGHTS}")
    
    # YAMNet ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œ
    print("YAMNet ëª¨ë¸ ë¡œë”© ì¤‘...")
    yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'
    yamnet_model = hub.load(yamnet_model_handle)
    print("YAMNet ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    X_data = []
    y_data = []
    data_info = []  # ê° ìƒ˜í”Œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥
    
    # 4) ë¬´ìŒ ë°ì´í„° ìƒì„± (ì¦ê°€)
    print(f"\në¬´ìŒ ë°ì´í„° ìƒì„± ì¤‘... (ì´ {SILENCE_SAMPLES}ê°œ ìƒ˜í”Œ)")
    
    for i in range(SILENCE_SAMPLES):
        try:
            if (i + 1) % 30 == 0:  # ì§„í–‰ë¥  í‘œì‹œ ê°„ê²© ì¡°ì •
                print(f"  ë¬´ìŒ ë°ì´í„° ì§„í–‰ë¥ : {i+1}/{SILENCE_SAMPLES}")
            
            # ì™„ì „ ë¬´ìŒê³¼ ì €ìŒëŸ‰ ë°°ê²½ ì†ŒìŒì„ ì„ì–´ì„œ ìƒì„±
            if i < SILENCE_SAMPLES // 2:
                audio = generate_silence(total_duration, sr)
                silence_type = "ì™„ì „ë¬´ìŒ"
            else:
                audio = generate_background_noise(total_duration, sr)
                silence_type = "ë°°ê²½ì†ŒìŒ"
            
            embeddings = extract_yamnet_embeddings(audio, sr, yamnet_model)
            labels = generate_labels(None, None, 0, total_duration=total_duration, frame_length=frame_length)  # ë¬´ìŒ í´ë˜ìŠ¤
            
            X_data.append(embeddings)
            y_data.append(labels)
            data_info.append({
                'class': 'silence',
                'class_id': 0,
                'type': silence_type,
                'factory_file': None,
                'event_file': None,
                'sample_index': i
            })
            
        except Exception as e:
            print(f"  ë¬´ìŒ ë°ì´í„° ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            continue
    
    # 5) ì •ìƒ ë°ì´í„° ìƒì„± (ê³µì¥ ì†Œë¦¬ë§Œ) - ì¦ê°€
    print(f"\nì •ìƒ ë°ì´í„° ìƒì„± ì¤‘... (ì´ {NORMAL_SAMPLES}ê°œ ìƒ˜í”Œ)")
    
    for i in range(NORMAL_SAMPLES):
        try:
            if (i + 1) % 30 == 0:  # ì§„í–‰ë¥  í‘œì‹œ ê°„ê²© ì¡°ì •
                print(f"  ì •ìƒ ë°ì´í„° ì§„í–‰ë¥ : {i+1}/{NORMAL_SAMPLES}")
                
            factory_path = random.choice(factory_paths)
            factory_audio, _ = librosa.load(factory_path, sr=sr)
            
            target_len = int(sr * total_duration)
            if len(factory_audio) > target_len:
                start_pos = random.randint(0, len(factory_audio) - target_len)
                factory_audio = factory_audio[start_pos:start_pos + target_len]
            else:
                factory_audio = np.pad(factory_audio, (0, max(0, target_len - len(factory_audio))))
            
            embeddings = extract_yamnet_embeddings(factory_audio, sr, yamnet_model)
            labels = generate_labels(None, None, 1, total_duration=total_duration, frame_length=frame_length)  # ì •ìƒ í´ë˜ìŠ¤
            
            X_data.append(embeddings)
            y_data.append(labels)
            data_info.append({
                'class': 'normal',
                'class_id': 1,
                'type': 'ê³µì¥ì†ŒìŒ',
                'factory_file': os.path.basename(factory_path),
                'event_file': None,
                'sample_index': i
            })
            
        except Exception as e:
            print(f"  ì •ìƒ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            continue

    # 6) ë¬´ìŒâ†’ê³µì¥ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒì„± (ìƒˆë¡œ ì¶”ê°€)
    print(f"\në¬´ìŒâ†’ê³µì¥ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒì„± ì¤‘... (ì´ {TRANSITION_SAMPLES}ê°œ ìƒ˜í”Œ)")
    
    for i in range(TRANSITION_SAMPLES):
        try:
            if (i + 1) % 20 == 0:  # ì§„í–‰ë¥  í‘œì‹œ ê°„ê²© ì¡°ì •
                print(f"  ì „í™˜ ë°ì´í„° ì§„í–‰ë¥ : {i+1}/{TRANSITION_SAMPLES}")
                
            factory_path = random.choice(factory_paths)
            factory_audio, _ = librosa.load(factory_path, sr=sr)
            
            # ë¬´ìŒâ†’ê³µì¥ì†Œë¦¬ ì „í™˜ ì˜¤ë””ì˜¤ ìƒì„±
            transition_audio, transition_start_sec = create_silence_to_factory_transition(
                factory_audio, sr, total_duration=total_duration
            )
            
            embeddings = extract_yamnet_embeddings(transition_audio, sr, yamnet_model)
            labels = generate_transition_labels(transition_start_sec, total_duration=total_duration, frame_length=frame_length)
            
            X_data.append(embeddings)
            y_data.append(labels)
            data_info.append({
                'class': 'transition',
                'class_id': 1,  # ìµœì¢…ì ìœ¼ë¡œ ì •ìƒ(ê³µì¥ì†Œë¦¬)ë¡œ ë¶„ë¥˜
                'type': 'ë¬´ìŒâ†’ê³µì¥ì†Œë¦¬ ì „í™˜',
                'factory_file': os.path.basename(factory_path),
                'event_file': None,
                'transition_start_sec': transition_start_sec,
                'sample_index': i
            })
            
        except Exception as e:
            print(f"  ì „í™˜ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            continue

    # 7) ë¬´ìŒâ†’ìœ„í—˜ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒì„± (ìƒˆë¡œ ì¶”ê°€)
    print(f"\në¬´ìŒâ†’ìœ„í—˜ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒì„± ì¤‘... (ì´ {DANGER_TRANSITION_SAMPLES}ê°œ ìƒ˜í”Œ)")
    
    # í´ë˜ìŠ¤ë³„ë¡œ ê· ë“±í•˜ê²Œ ë¶„ë°°
    samples_per_danger_class = DANGER_TRANSITION_SAMPLES // len(event_folders)
    remaining_samples = DANGER_TRANSITION_SAMPLES % len(event_folders)
    
    current_sample_count = 0
    
    for class_idx, class_name in enumerate(event_folders):
        if class_name not in event_data or len(event_data[class_name]) == 0:
            print(f"  âš ï¸ {class_name} í´ë˜ìŠ¤ íŒŒì¼ì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
            
        class_id = class_mapping[class_name]
        
        # ë§ˆì§€ë§‰ í´ë˜ìŠ¤ì— ë‚¨ì€ ìƒ˜í”Œ ì¶”ê°€
        class_samples = samples_per_danger_class
        if class_idx < remaining_samples:
            class_samples += 1
            
        print(f"  {class_name} í´ë˜ìŠ¤ ì „í™˜ ë°ì´í„°: {class_samples}ê°œ ìƒ˜í”Œ")
        
        for i in range(class_samples):
            try:
                current_sample_count += 1
                if current_sample_count % 10 == 0:
                    print(f"    ìœ„í—˜ ì „í™˜ ë°ì´í„° ì§„í–‰ë¥ : {current_sample_count}/{DANGER_TRANSITION_SAMPLES}")
                
                # ëœë¤í•˜ê²Œ ìœ„í—˜ì†Œë¦¬ íŒŒì¼ ì„ íƒ
                event_path = random.choice(event_data[class_name])
                event_audio, _ = librosa.load(event_path, sr=sr)
                event_audio_ns = remove_silence(event_audio, sr, top_db=20)
                
                # ë¬´ìŒâ†’ìœ„í—˜ì†Œë¦¬ ì „í™˜ ì˜¤ë””ì˜¤ ìƒì„±
                transition_audio, transition_start_sec, transition_end_sec = create_silence_to_danger_transition(
                    event_audio_ns, sr, class_id, total_duration=total_duration
                )
                
                embeddings = extract_yamnet_embeddings(transition_audio, sr, yamnet_model)
                labels = generate_danger_transition_labels(
                    transition_start_sec, transition_end_sec, class_id, 
                    total_duration=total_duration, frame_length=frame_length
                )
                
                X_data.append(embeddings)
                y_data.append(labels)
                data_info.append({
                    'class': f'danger_transition_{class_name}',
                    'class_id': class_id,
                    'type': f'ë¬´ìŒâ†’{class_name} ì „í™˜',
                    'factory_file': None,
                    'event_file': os.path.basename(event_path),
                    'transition_start_sec': transition_start_sec,
                    'transition_end_sec': transition_end_sec,
                    'sample_index': i
                })
                
            except Exception as e:
                print(f"    ìœ„í—˜ ì „í™˜ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
    
    # 8) ìœ„í—˜ ì†Œë¦¬ ë°ì´í„° ìƒì„± - ìë™ ê³„ì‚°ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©
    samples_per_class = DANGER_WEIGHTS
    
    for class_name, event_paths in event_data.items():
        class_id = class_mapping[class_name]
        samples_per_event = samples_per_class[class_name]
        print(f"\n{class_name} í´ë˜ìŠ¤ ë°ì´í„° ìƒì„± ì¤‘... (í´ë˜ìŠ¤ ID: {class_id}, íŒŒì¼ë‹¹ {samples_per_event}ê°œ ìƒ˜í”Œ)")
        
        for idx, event_path in enumerate(event_paths):
            try:
                print(f"  ì²˜ë¦¬ ì¤‘: [{idx+1}/{len(event_paths)}] {os.path.basename(event_path)}")
                event_audio, _ = librosa.load(event_path, sr=sr)
                event_audio_ns = remove_silence(event_audio, sr, top_db=20)
                
                # ì†Œìˆ˜ì  ìƒ˜í”Œë§ ì²˜ë¦¬ (1 ì´ìƒì˜ ì†Œìˆ˜ì  í¬í•¨)
                if samples_per_event < 1:
                    # í™•ë¥ ì  ìƒ˜í”Œë§ (ì˜ˆ: 0.3ì´ë©´ 30% í™•ë¥ ë¡œ 1ê°œ ìƒì„±)
                    if random.random() < samples_per_event:
                        factory_path = random.choice(factory_paths)
                        factory_audio, _ = librosa.load(factory_path, sr=sr)
                        
                        mixed_audio, start_sec, end_sec = mix_factory_and_event(factory_audio, event_audio_ns, sr, desired_length=total_duration)
                        embeddings = extract_yamnet_embeddings(mixed_audio, sr, yamnet_model)
                        labels = generate_labels(start_sec, end_sec, class_id, total_duration=total_duration, frame_length=frame_length)
                        
                        X_data.append(embeddings)
                        y_data.append(labels)
                        data_info.append({
                            'class': class_name,
                            'class_id': class_id,
                            'type': 'ìœ„í—˜ì†ŒìŒ',
                            'factory_file': os.path.basename(factory_path),
                            'event_file': os.path.basename(event_path),
                            'event_start_sec': start_sec,
                            'event_end_sec': end_sec,
                            'sample_index': 0
                        })
                else:
                    # ì •ìˆ˜ ë¶€ë¶„ê³¼ ì†Œìˆ˜ì  ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
                    base_samples = int(samples_per_event)  # ì •ìˆ˜ ë¶€ë¶„
                    extra_probability = samples_per_event - base_samples  # ì†Œìˆ˜ì  ë¶€ë¶„
                    
                    # ê¸°ë³¸ ìƒ˜í”Œ ìƒì„± (ì •ìˆ˜ ë¶€ë¶„)
                    for i in range(base_samples):
                        factory_path = random.choice(factory_paths)
                        factory_audio, _ = librosa.load(factory_path, sr=sr)
                        
                        mixed_audio, start_sec, end_sec = mix_factory_and_event(factory_audio, event_audio_ns, sr, desired_length=total_duration)
                        embeddings = extract_yamnet_embeddings(mixed_audio, sr, yamnet_model)
                        labels = generate_labels(start_sec, end_sec, class_id, total_duration=total_duration, frame_length=frame_length)
                        
                        X_data.append(embeddings)
                        y_data.append(labels)
                        data_info.append({
                            'class': class_name,
                            'class_id': class_id,
                            'type': 'ìœ„í—˜ì†ŒìŒ',
                            'factory_file': os.path.basename(factory_path),
                            'event_file': os.path.basename(event_path),
                            'event_start_sec': start_sec,
                            'event_end_sec': end_sec,
                            'sample_index': i
                        })
                    
                    # ì¶”ê°€ ìƒ˜í”Œ ìƒì„± (ì†Œìˆ˜ì  ë¶€ë¶„, í™•ë¥ ì )
                    if extra_probability > 0 and random.random() < extra_probability:
                        factory_path = random.choice(factory_paths)
                        factory_audio, _ = librosa.load(factory_path, sr=sr)
                        
                        mixed_audio, start_sec, end_sec = mix_factory_and_event(factory_audio, event_audio_ns, sr, desired_length=total_duration)
                        embeddings = extract_yamnet_embeddings(mixed_audio, sr, yamnet_model)
                        labels = generate_labels(start_sec, end_sec, class_id, total_duration=total_duration, frame_length=frame_length)
                        
                        X_data.append(embeddings)
                        y_data.append(labels)
                        data_info.append({
                            'class': class_name,
                            'class_id': class_id,
                            'type': 'ìœ„í—˜ì†ŒìŒ',
                            'factory_file': os.path.basename(factory_path),
                            'event_file': os.path.basename(event_path),
                            'event_start_sec': start_sec,
                            'event_end_sec': end_sec,
                            'sample_index': base_samples
                        })
                    
            except Exception as e:
                print(f"    íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {event_path}, ì˜¤ë¥˜: {e}")
                continue
    
    # 9) ë°ì´í„° ë°°ì—´í™” ë° ì „ì²˜ë¦¬
    print(f"\nì´ ë°ì´í„° ìƒ˜í”Œ ìˆ˜: {len(X_data)}")
    
    if len(X_data) == 0:
        print("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    try:
        # ë™ì¼í•œ ê¸¸ì´ë¡œ ë§ì¶”ê¸° (íŒ¨ë”©)
        max_len = max([x.shape[0] for x in X_data])
        print(f"ìµœëŒ€ í”„ë ˆì„ ê¸¸ì´: {max_len}")
        
        X_data_padded = []
        y_data_padded = []
        
        for i in range(len(X_data)):
            x = X_data[i]
            y = y_data[i]
            
            # ë°ì´í„° í˜•íƒœ ê²€ì¦
            if len(x.shape) != 2 or x.shape[1] != 1024:
                print(f"ê²½ê³ : ìƒ˜í”Œ {i}ì˜ X ë°ì´í„° í˜•íƒœê°€ ì´ìƒí•©ë‹ˆë‹¤: {x.shape}")
                continue
                
            if len(y) != x.shape[0]:
                print(f"ê²½ê³ : ìƒ˜í”Œ {i}ì˜ Xì™€ y ê¸¸ì´ê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤: X={x.shape[0]}, y={len(y)}")
                if len(y) < x.shape[0]:
                    y = np.pad(y, (0, x.shape[0] - len(y)), mode='constant')
                else:
                    y = y[:x.shape[0]]
            
            if x.shape[0] < max_len:
                x_padded = np.pad(x, ((0, max_len - x.shape[0]), (0, 0)), mode='constant')
                y_padded = np.pad(y, (0, max_len - len(y)), mode='constant')
            else:
                x_padded = x[:max_len]
                y_padded = y[:max_len]
            
            X_data_padded.append(x_padded)
            y_data_padded.append(y_padded)
        
        print(f"íŒ¨ë”© í›„ ìœ íš¨í•œ ìƒ˜í”Œ ìˆ˜: {len(X_data_padded)}")
        
        X_data = np.array(X_data_padded, dtype=np.float32)
        y_data = np.array(y_data_padded, dtype=np.int32)
        
        print(f"ë°ì´í„° í˜•íƒœ - X: {X_data.shape}, y: {y_data.shape}")
        
        # One-hot ì¸ì½”ë”© (5-í´ë˜ìŠ¤)
        y_data_oh = to_categorical(y_data, num_classes=num_classes)
        print(f"One-hot ì¸ì½”ë”© í›„ y í˜•íƒœ: {y_data_oh.shape}")
        
        # í´ë˜ìŠ¤ë³„ ë°ì´í„° ë¶„í¬ í™•ì¸
        unique, counts = np.unique(y_data, return_counts=True)
        print("í´ë˜ìŠ¤ë³„ í”„ë ˆì„ ìˆ˜:")
        class_names = ['ë¬´ìŒ', 'ì •ìƒ(ê³µì¥)', 'í™”ì¬', 'ê°€ìŠ¤ëˆ„ì¶œ', 'ë¹„ëª…']
        for cls, count in zip(unique, counts):
            if cls < len(class_names):
                print(f"  {class_names[cls]}: {count}ê°œ í”„ë ˆì„")
        
        # í´ë˜ìŠ¤ë³„ ë¹„ìœ¨ í™•ì¸
        total_frames = np.sum(counts)
        print("\ní´ë˜ìŠ¤ë³„ ë¹„ìœ¨:")
        for cls, count in zip(unique, counts):
            if cls < len(class_names):
                ratio = count / total_frames * 100
                print(f"  {class_names[cls]}: {ratio:.1f}%")
        
        # 10) í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬ (60% / 20% / 20%)
        X_temp, X_test, y_temp, y_test, info_temp, info_test = train_test_split(
            X_data, y_data_oh, data_info, test_size=0.2, random_state=42, stratify=[info['class_id'] for info in data_info]
        )
        
        X_train, X_val, y_train, y_val, info_train, info_val = train_test_split(
            X_temp, y_temp, info_temp, test_size=0.25, random_state=42, stratify=[info['class_id'] for info in info_temp]  # 0.25 * 0.8 = 0.2 (ì „ì²´ì˜ 20%)
        )
        
        print(f"\ní›ˆë ¨ ë°ì´í„°: {X_train.shape}, ë ˆì´ë¸”: {y_train.shape}")
        print(f"ê²€ì¦ ë°ì´í„°: {X_val.shape}, ë ˆì´ë¸”: {y_val.shape}")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}, ë ˆì´ë¸”: {y_test.shape}")
        
        # ë°ì´í„°ì…‹ ì •ë³´ ì €ì¥
        dataset_info = {
            'train': info_train,
            'validation': info_val,
            'test': info_test
        }
        
        dataset_file_path = os.path.join(output_folder, f'dataset_info_{VERSION}.json')
        with open(dataset_file_path, 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, ensure_ascii=False, indent=2)
        print(f"ë°ì´í„°ì…‹ ì •ë³´ ì €ì¥ ì™„ë£Œ: {dataset_file_path}")
        
        # ê° ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ë³„ ë¶„í¬ í™•ì¸
        def print_dataset_distribution(y_data, dataset_name, class_names):
            y_labels = np.argmax(y_data, axis=2).flatten()  # one-hotì„ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
            unique, counts = np.unique(y_labels, return_counts=True)
            total = np.sum(counts)
            print(f"\n{dataset_name} ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ë¶„í¬:")
            for cls, count in zip(unique, counts):
                if cls < len(class_names):
                    ratio = count / total * 100
                    print(f"  {class_names[cls]}: {count:,}ê°œ í”„ë ˆì„ ({ratio:.1f}%)")
        
        print_dataset_distribution(y_train, "í›ˆë ¨", class_names)
        print_dataset_distribution(y_val, "ê²€ì¦", class_names)
        print_dataset_distribution(y_test, "í…ŒìŠ¤íŠ¸", class_names)
        
        # 11) LSTM ëª¨ë¸ ìƒì„±
        input_shape = X_train.shape[1:]
        model = create_lstm_model(input_shape, num_classes)
        model.summary()
        
        # ì½œë°± ì„¤ì •
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=5,
                restore_best_weights=True
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=3,
                min_lr=0.00001
            )
        ]
        
        # 12) í•™ìŠµ
        print(f"\nëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        history = model.fit(
            X_train, y_train, 
            epochs=20,
            batch_size=8, 
            validation_data=(X_val, y_val),
            callbacks=callbacks,
            verbose=1
        )
        
        # 13) ëª¨ë¸ í‰ê°€
        print("\n=== ê²€ì¦ ë°ì´í„° í‰ê°€ ===")
        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)
        print(f"ê²€ì¦ ì†ì‹¤: {val_loss:.4f}")
        print(f"ê²€ì¦ ì •í™•ë„: {val_acc:.4f}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€
        print("\n=== í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ ===")
        test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
        print(f"í…ŒìŠ¤íŠ¸ ì†ì‹¤: {test_loss:.4f}")
        print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}")
        
        # ìƒì„¸í•œ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ë¶„ì„
        y_test_pred = model.predict(X_test, verbose=0)
        y_test_pred_classes = np.argmax(y_test_pred, axis=2)
        y_test_true_classes = np.argmax(y_test, axis=2)
        
        # í”„ë ˆì„ ë‹¨ìœ„ í‰ê°€ë¥¼ ìœ„í•´ flatten
        y_test_pred_flat = y_test_pred_classes.flatten()
        y_test_true_flat = y_test_true_classes.flatten()
        
        print("\n=== ìƒì„¸ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ (í”„ë ˆì„ ë‹¨ìœ„) ===")
        print("ë¶„ë¥˜ ë³´ê³ ì„œ:")
        print(classification_report(y_test_true_flat, y_test_pred_flat, 
                                  target_names=class_names, 
                                  zero_division=0))
        
        print("\ní˜¼ë™ í–‰ë ¬:")
        cm = confusion_matrix(y_test_true_flat, y_test_pred_flat)
        print("ì‹¤ì œ\\ì˜ˆì¸¡", end="")
        for name in class_names:
            print(f"\t{name[:6]}", end="")
        print()
        for i, name in enumerate(class_names):
            print(f"{name[:8]}", end="")
            for j in range(len(class_names)):
                print(f"\t{cm[i][j]}", end="")
            print()
        
        # 14) ëª¨ë¸ ì €ì¥
        model_file_path = os.path.join(output_folder, f'yamnet_lstm_model_{VERSION}.h5')
        model.save(model_file_path)
        print(f"\n5í´ë˜ìŠ¤ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_file_path}")
        
        # í´ë˜ìŠ¤ ë§¤í•‘ ì •ë³´ ì €ì¥
        model_info = {
            'class_mapping': {**class_mapping, 'silence': 0, 'normal': 1},
            'class_names': class_names,
            'num_classes': num_classes,
            'version': VERSION,
            'model_file': f'yamnet_lstm_model_{VERSION}.h5'
        }
        
        # pickleì„ ì‚¬ìš©í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ ì €ì¥
        model_info_path = os.path.join(output_folder, f'model_info_{VERSION}.pkl')
        with open(model_info_path, 'wb') as f:
            pickle.dump(model_info, f)
        print(f"ëª¨ë¸ ì •ë³´ ì €ì¥ ì™„ë£Œ: {model_info_path}")
        
        # 15) ëª¨ë¸ ì„±ëŠ¥ ë° ì •ë³´ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
        from datetime import datetime
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        performance_log = f"""
========================================
5í´ë˜ìŠ¤ LSTM ëª¨ë¸ í•™ìŠµ ë³´ê³ ì„œ ({VERSION})
========================================
í•™ìŠµ ì™„ë£Œ ì‹œê°„: {current_time}
ëª¨ë¸ ë²„ì „: {VERSION}
ê²°ê³¼ë¬¼ í´ë”: {output_folder}

=== ëª¨ë¸ êµ¬ì„± ===
- ëª¨ë¸ íƒ€ì…: YAMNet + LSTM
- í´ë˜ìŠ¤ ìˆ˜: {num_classes}ê°œ
- ì…ë ¥ í˜•íƒœ: {input_shape}
- ìŒì„± ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜: {sr} Hz
- í”„ë ˆì„ ê¸¸ì´: {frame_length}ì´ˆ
- ì´ ì˜¤ë””ì˜¤ ê¸¸ì´: {total_duration}ì´ˆ

=== í´ë˜ìŠ¤ ì •ë³´ ===
í´ë˜ìŠ¤ ë§¤í•‘:
  - 0: ë¬´ìŒ (silence)
  - 1: ì •ìƒ (ê³µì¥ì†ŒìŒ)
  - 2: í™”ì¬ (fire)
  - 3: ê°€ìŠ¤ëˆ„ì¶œ (gas)
  - 4: ë¹„ëª… (scream)

=== ë°ì´í„° ìƒì„± ì„¤ì • ===
ë¬´ìŒ ë°ì´í„°: {SILENCE_SAMPLES}ê°œ ìƒ˜í”Œ
ì •ìƒ ë°ì´í„°: {NORMAL_SAMPLES}ê°œ ìƒ˜í”Œ
ì „í™˜ ë°ì´í„°: {TRANSITION_SAMPLES}ê°œ ìƒ˜í”Œ (ë¬´ìŒâ†’ê³µì¥ì†Œë¦¬)
ìœ„í—˜ ì „í™˜ ë°ì´í„°: {DANGER_TRANSITION_SAMPLES}ê°œ ìƒ˜í”Œ (ë¬´ìŒâ†’ìœ„í—˜ì†Œë¦¬)
ê°€ì¤‘ì¹˜ ê³„ì‚° ëª¨ë“œ: {'ìë™' if AUTO_WEIGHT_CALCULATION else 'ìˆ˜ë™'}
ìœ„í—˜ ì†ŒìŒ ê°€ì¤‘ì¹˜:
  - í™”ì¬: {DANGER_WEIGHTS.get('fire', 0)} (íŒŒì¼ë‹¹ ìƒ˜í”Œ ìˆ˜)
  - ê°€ìŠ¤ëˆ„ì¶œ: {DANGER_WEIGHTS.get('gas', 0)} (íŒŒì¼ë‹¹ ìƒ˜í”Œ ìˆ˜)
  - ë¹„ëª…: {DANGER_WEIGHTS.get('scream', 0)} (íŒŒì¼ë‹¹ ìƒ˜í”Œ ìˆ˜)

=== ë°ì´í„° ë¶„í¬ ===
ì´ ìƒ˜í”Œ ìˆ˜: {len(X_data)}ê°œ
í›ˆë ¨ ë°ì´í„°: {X_train.shape[0]}ê°œ ({X_train.shape[0]/len(X_data)*100:.1f}%)
ê²€ì¦ ë°ì´í„°: {X_val.shape[0]}ê°œ ({X_val.shape[0]/len(X_data)*100:.1f}%)
í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]}ê°œ ({X_test.shape[0]/len(X_data)*100:.1f}%)

í´ë˜ìŠ¤ë³„ í”„ë ˆì„ ìˆ˜:
"""
        # í´ë˜ìŠ¤ë³„ ë¶„í¬ ì •ë³´ ì¶”ê°€
        unique, counts = np.unique(y_data, return_counts=True)
        total_frames = np.sum(counts)
        for cls, count in zip(unique, counts):
            if cls < len(class_names):
                ratio = count / total_frames * 100
                performance_log += f"  - {class_names[cls]}: {count:,}ê°œ í”„ë ˆì„ ({ratio:.1f}%)\n"
        
        performance_log += f"""
=== ëª¨ë¸ ì„±ëŠ¥ ===
ê²€ì¦ ì†ì‹¤: {val_loss:.6f}
ê²€ì¦ ì •í™•ë„: {val_acc:.6f} ({val_acc*100:.2f}%)

í…ŒìŠ¤íŠ¸ ì†ì‹¤: {test_loss:.6f}
í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.6f} ({test_acc*100:.2f}%)

=== ìƒì„¸ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ë¶„ì„ ===
{classification_report(y_test_true_flat, y_test_pred_flat, target_names=class_names, zero_division=0)}

=== ë°ì´í„°ì…‹ íŒŒì¼ ì •ë³´ ===
- dataset_info_{VERSION}.json: í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ì‚¬ìš©ëœ ì˜¤ë””ì˜¤ íŒŒì¼ ì •ë³´

=== í•™ìŠµ ì„¤ì • ===
ì—í¬í¬ ìˆ˜: 20
ë°°ì¹˜ í¬ê¸°: 8
ìµœì í™” ì•Œê³ ë¦¬ì¦˜: Adam (learning_rate=0.001)
ì½œë°±:
  - EarlyStopping (patience=5)
  - ReduceLROnPlateau (patience=3, factor=0.5)

=== ì €ì¥ëœ íŒŒì¼ ===
- ëª¨ë¸ íŒŒì¼: yamnet_lstm_model_{VERSION}.h5
- ëª¨ë¸ ì •ë³´: model_info_{VERSION}.pkl
- ì„±ëŠ¥ ë³´ê³ ì„œ: model_performance_{VERSION}.txt
- ë°ì´í„°ì…‹ ì •ë³´: dataset_info_{VERSION}.json

========================================
"""
        
        # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
        performance_report_path = os.path.join(output_folder, f'model_performance_{VERSION}.txt')
        with open(performance_report_path, 'w', encoding='utf-8') as f:
            f.write(performance_log)
        
        print(f"ëª¨ë¸ ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {performance_report_path}")
        
        # ìš”ì•½ ì •ë³´ íŒŒì¼ ìƒì„±
        summary_info = {
            'version': VERSION,
            'timestamp': current_time,
            'folder': output_folder,
            'files': {
                'model': f'yamnet_lstm_model_{VERSION}.h5',
                'model_info': f'model_info_{VERSION}.pkl',
                'performance_report': f'model_performance_{VERSION}.txt',
                'dataset_info': f'dataset_info_{VERSION}.json'
            },
            'performance': {
                'validation_accuracy': float(val_acc),
                'validation_loss': float(val_loss),
                'test_accuracy': float(test_acc),
                'test_loss': float(test_loss)
            },
            'data_summary': {
                'total_samples': len(X_data),
                'train_samples': X_train.shape[0],
                'validation_samples': X_val.shape[0],
                'test_samples': X_test.shape[0],
                'silence_samples': SILENCE_SAMPLES,
                'normal_samples': NORMAL_SAMPLES,
                'transition_samples': TRANSITION_SAMPLES,
                'danger_transition_samples': DANGER_TRANSITION_SAMPLES,
                'auto_weight_calculation': AUTO_WEIGHT_CALCULATION,
                'danger_weights': DANGER_WEIGHTS
            }
        }
        
        summary_path = os.path.join(output_folder, f'summary_{VERSION}.json')
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary_info, f, ensure_ascii=False, indent=2)
        
        print(f"ìš”ì•½ ì •ë³´ ì €ì¥ ì™„ë£Œ: {summary_path}")
        
        # ê²°ê³¼ë¬¼ í´ë” ì •ë³´ ì¶œë ¥
        print(f"\nğŸ‰ ëª¨ë“  ê²°ê³¼ë¬¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“ ê²°ê³¼ë¬¼ í´ë”: {output_folder}")
        print(f"ğŸ“‹ í¬í•¨ëœ íŒŒì¼:")
        print(f"  - yamnet_lstm_model_{VERSION}.h5 (í•™ìŠµëœ ëª¨ë¸)")
        print(f"  - model_info_{VERSION}.pkl (ëª¨ë¸ ì •ë³´)")
        print(f"  - model_performance_{VERSION}.txt (ìƒì„¸ ì„±ëŠ¥ ë³´ê³ ì„œ)")
        print(f"  - dataset_info_{VERSION}.json (ë°ì´í„°ì…‹ ì •ë³´)")
        print(f"  - summary_{VERSION}.json (ìš”ì•½ ì •ë³´)")
        
    except Exception as e:
        print(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
