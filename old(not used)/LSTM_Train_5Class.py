"""
5í´ë˜ìŠ¤ LSTM ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ v1.3
=====================================

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” YAMNet + LSTMì„ ì‚¬ìš©í•˜ì—¬ 5ê°œ í´ë˜ìŠ¤(ë¬´ìŒ, ì •ìƒ, í™”ì¬, ê°€ìŠ¤ëˆ„ì¶œ, ë¹„ëª…)ë¥¼  
ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.

ë°ì´í„° ê°€ì¤‘ì¹˜ ì„¤ì • ë°©ë²•:
- VERSION: ëª¨ë¸ ë²„ì „ (ê²°ê³¼ë¬¼ í´ë”ëª…ì— ì‚¬ìš©)
- SILENCE_SAMPLES: ë¬´ìŒ ë°ì´í„° ìƒ˜í”Œ ìˆ˜
- NORMAL_SAMPLES: ì •ìƒ(ê³µì¥ì†ŒìŒ) ë°ì´í„° ìƒ˜í”Œ ìˆ˜  
- TRANSITION_SAMPLES: ë¬´ìŒâ†’ê³µì¥ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒ˜í”Œ ìˆ˜
- DANGER_TRANSITION_SAMPLES: ë¬´ìŒâ†’ìœ„í—˜ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒ˜í”Œ ìˆ˜
- FACTORY_TRANSITION_SAMPLES: ê³µì¥ì†Œë¦¬â†’ë‹¤ë¥¸ê³µì¥ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒ˜í”Œ ìˆ˜ (v1.3 ì‹ ê·œ)
- SILENCE_VARIATION_SAMPLES: ë¬´ìŒâ†’ë‹¤ë¥¸ë¬´ìŒ ë³€í™” ë°ì´í„° ìƒ˜í”Œ ìˆ˜ (v1.3 ì‹ ê·œ)
- AUTO_WEIGHT_CALCULATION: ìë™ ê°€ì¤‘ì¹˜ ê³„ì‚° ì—¬ë¶€
  - True: envsound í´ë”ì˜ íŒŒì¼ ê°œìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìë™ ê³„ì‚°
  - False: MANUAL_DANGER_WEIGHTS ì‚¬ìš©
- MANUAL_DANGER_WEIGHTS: ìˆ˜ë™ ì„¤ì • ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬
  - 'fire': í™”ì¬ ì†ŒìŒ ê°€ì¤‘ì¹˜
  - 'gas': ê°€ìŠ¤ëˆ„ì¶œ ì†ŒìŒ ê°€ì¤‘ì¹˜  
  - 'scream': ë¹„ëª… ì†ŒìŒ ê°€ì¤‘ì¹˜

v1.3 ìƒˆë¡œ ì¶”ê°€ëœ í›ˆë ¨ ë°ì´í„°:
1. ê³µì¥ì†Œë¦¬â†’ë‹¤ë¥¸ê³µì¥ì†Œë¦¬ ì „í™˜:
   - ì„œë¡œ ë‹¤ë¥¸ ê³µì¥ì†Œë¦¬ íŒŒì¼ ê°„ì˜ ì „í™˜ ì‹œë‚˜ë¦¬ì˜¤
   - 30~70% ì§€ì ì—ì„œ 0.5ì´ˆ í˜ì´ë“œ ì „í™˜
   - ëª¨ë“  í”„ë ˆì„ì´ ì •ìƒ(ê³µì¥) í´ë˜ìŠ¤ë¡œ ë¼ë²¨ë§
   - ëª©ì : ê³µì¥ì†Œë¦¬ ë³€í™” ì‹œì—ë„ ì•ˆì •ì ìœ¼ë¡œ "ì •ìƒ" íŒë‹¨

2. ë¬´ìŒ ë³€í™” (ë‹¤ì–‘í•œ ë°°ê²½ì†ŒìŒ):
   - ìˆœìˆ˜ë¬´ìŒ â†” ì €ì¡ìŒ â†” í™”ì´íŠ¸ë…¸ì´ì¦ˆ ê°„ ë³€í™”
   - 20~80% ì§€ì ì—ì„œ 0.2ì´ˆ í˜ì´ë“œ ì „í™˜
   - ëª¨ë“  í”„ë ˆì„ì´ ë¬´ìŒ í´ë˜ìŠ¤ë¡œ ë¼ë²¨ë§
   - ëª©ì : ë‹¤ì–‘í•œ ë°°ê²½ì†ŒìŒ íŒ¨í„´ì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ "ë¬´ìŒ" íŒë‹¨

ê¸°ì¡´ ì „í™˜ ë°ì´í„°:
- ë¬´ìŒ ìƒíƒœì—ì„œ ê°‘ìê¸° ê³µì¥ ì†Œë¦¬ê°€ ì‹œì‘ë˜ëŠ” í˜„ì‹¤ì ì¸ ì‹œë‚˜ë¦¬ì˜¤
- ë¬´ìŒ ìƒíƒœì—ì„œ ê°‘ìê¸° ìœ„í—˜ ì†Œë¦¬ê°€ ì‹œì‘ë˜ëŠ” ê¸´ê¸‰ ìƒí™© ì‹œë‚˜ë¦¬ì˜¤
- ì „ì²´ ê¸¸ì´ì˜ 20~80% ì§€ì ì—ì„œ ë¬´ìŒâ†’ê³µì¥ì†Œë¦¬/ìœ„í—˜ì†Œë¦¬ ì „í™˜
- 0.5ì´ˆ í˜ì´ë“œì¸ íš¨ê³¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì „í™˜ êµ¬í˜„
- ë¬´ìŒ êµ¬ê°„ì€ í´ë˜ìŠ¤ 0, ì „í™˜ í›„ êµ¬ê°„ì€ í•´ë‹¹ í´ë˜ìŠ¤ë¡œ ë¼ë²¨ë§

ìë™ ê°€ì¤‘ì¹˜ ê³„ì‚°:
- ê° í´ë˜ìŠ¤ë‹¹ ëª©í‘œ ìƒ˜í”Œ ìˆ˜ë¥¼ 250ê°œë¡œ ì„¤ì • (ì¡°ì •ë¨)
- íŒŒì¼ ê°œìˆ˜ê°€ ë§ì€ í´ë˜ìŠ¤ëŠ” ë‚®ì€ ê°€ì¤‘ì¹˜, ì ì€ í´ë˜ìŠ¤ëŠ” ë†’ì€ ê°€ì¤‘ì¹˜
- ê°€ì¤‘ì¹˜ê°€ 1 ë¯¸ë§Œì´ë©´ í™•ë¥ ì  ìƒ˜í”Œë§ ì ìš©

ì¶œë ¥ íŒŒì¼ (ë²„ì „ë³„ í´ë”ì— ì €ì¥):
- yamnet_lstm_model_{VERSION}.h5: í•™ìŠµëœ ëª¨ë¸
- model_info_{VERSION}.pkl: ëª¨ë¸ ì •ë³´ (í´ë˜ìŠ¤ ë§¤í•‘, ì´ë¦„ ë“±)
- model_performance_{VERSION}.txt: ìƒì„¸í•œ í•™ìŠµ ê²°ê³¼ ë³´ê³ ì„œ
- dataset_info_{VERSION}.json: í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ íŒŒì¼ ì •ë³´
- summary_{VERSION}.json: ìš”ì•½ ì •ë³´ (ì„±ëŠ¥, ì„¤ì •ê°’ ë“±)

í´ë” êµ¬ì¡°:
model_results_{VERSION}/
â”œâ”€â”€ yamnet_lstm_model_{VERSION}.h5
â”œâ”€â”€ model_info_{VERSION}.pkl  
â”œâ”€â”€ model_performance_{VERSION}.txt
â”œâ”€â”€ dataset_info_{VERSION}.json
â””â”€â”€ summary_{VERSION}.json
"""

import numpy as np
import librosa
import soundfile as sf
import random
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
import tensorflow_hub as hub
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import os
import glob
import pickle
import json

# ================================
# ë°ì´í„° ìƒì„± ê°€ì¤‘ì¹˜ ì„¤ì • (ìˆ˜ì • ê°€ëŠ¥)
# ================================
# v1.4: ìŠ¤ë§ˆíŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ê¸°ë³¸ ì„¤ì •
TARGET_SAMPLES_PER_CLASS = 180                  # ê° í´ë˜ìŠ¤ë‹¹ ëª©í‘œ ìƒ˜í”Œ ìˆ˜ (ê· ë“± ë¶„í¬)

# v1.4: ëª¨ë“  í´ë˜ìŠ¤ ê· ë“± ë¶„í¬ë¥¼ ìœ„í•œ ì¡°ì •
SILENCE_SAMPLES = TARGET_SAMPLES_PER_CLASS      # ë¬´ìŒ ë°ì´í„° ìƒ˜í”Œ ìˆ˜ (180ê°œ)
NORMAL_SAMPLES = TARGET_SAMPLES_PER_CLASS       # ì •ìƒ(ê³µì¥ì†ŒìŒ) ë°ì´í„° ìƒ˜í”Œ ìˆ˜ (180ê°œ)
TRANSITION_SAMPLES = 80                         # ë¬´ìŒâ†’ê³µì¥ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒ˜í”Œ ìˆ˜ 
DANGER_TRANSITION_SAMPLES = 60                  # ë¬´ìŒâ†’ìœ„í—˜ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒ˜í”Œ ìˆ˜ 

# v1.4 ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„° íƒ€ì… (v1.4ì—ì„œ ì¡°ì •)
FACTORY_TRANSITION_SAMPLES = 60                 # ê³µì¥ì†Œë¦¬â†’ë‹¤ë¥¸ê³µì¥ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒ˜í”Œ ìˆ˜
SILENCE_VARIATION_SAMPLES = 40                  # ë¬´ìŒâ†’ë‹¤ë¥¸ë¬´ìŒ ë³€í™” ë°ì´í„° ìƒ˜í”Œ ìˆ˜

# ë²„ì „ ê´€ë¦¬ ì„¤ì •
VERSION = "v1.4"  # ëª¨ë¸ ë²„ì „ (ê²°ê³¼ë¬¼ í´ë”ëª…ì— ì‚¬ìš©)

# ìœ„í—˜ ì†ŒìŒë³„ ê°€ì¤‘ì¹˜ (íŒŒì¼ë‹¹ ìƒì„±í•  ìƒ˜í”Œ ìˆ˜)
# ìë™ ê³„ì‚° ë˜ëŠ” ìˆ˜ë™ ì„¤ì • ê°€ëŠ¥
AUTO_WEIGHT_CALCULATION = True   # True: ìŠ¤ë§ˆíŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ê¸°ë°˜ ìë™ ê³„ì‚°, False: ìˆ˜ë™ ì„¤ì •

# v1.35 ìƒˆë¡œ ì¶”ê°€: ìŠ¤ë§ˆíŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„¤ì •
ENABLE_SMART_SEGMENTATION = True  # ì‹¤ì œ ì†Œë¦¬ êµ¬ê°„ë§Œ ì¶”ì¶œí•˜ì—¬ í•™ìŠµ
MIN_SEGMENT_DURATION = 2.0        # ìµœì†Œ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)
MAX_SEGMENT_DURATION = 7.0        # ìµœëŒ€ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ) 
SILENCE_THRESHOLD = 0.01          # ë¬´ìŒ íŒì • ì„ê³„ê°’ (RMS ê¸°ì¤€)
MIN_SOUND_RATIO = 0.4             # ì„¸ê·¸ë¨¼íŠ¸ ë‚´ ìµœì†Œ ì†Œë¦¬ ë¹„ìœ¨ (40% ì´ìƒì´ ì†Œë¦¬ì—¬ì•¼ í•¨)
OVERLAP_RATIO = 0.3               # ê¸´ ì˜¤ë””ì˜¤ ë¶„í• ì‹œ ê²¹ì¹¨ ë¹„ìœ¨

# ìˆ˜ë™ ì„¤ì •ì‹œ ì‚¬ìš©ë˜ëŠ” ê°€ì¤‘ì¹˜ (ENABLE_SMART_SEGMENTATION=Falseì¼ ë•Œë§Œ ì‚¬ìš©)
MANUAL_DANGER_WEIGHTS = {
    'fire': 1.2,      # í™”ì¬: íŒŒì¼ë‹¹ 1.2ê°œ ìƒ˜í”Œ 
    'gas': 50,        # ê°€ìŠ¤ëˆ„ì¶œ: íŒŒì¼ë‹¹ 50ê°œ ìƒ˜í”Œ  
    'scream': 22      # ë¹„ëª…: íŒŒì¼ë‹¹ 22ê°œ ìƒ˜í”Œ
}

def calculate_auto_weights(envsound_folder, target_samples_per_class=250):
    """
    ê° í´ë˜ìŠ¤ì˜ íŒŒì¼ ê°œìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìë™ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        envsound_folder: ìœ„í—˜ ì†ŒìŒ í´ë” ê²½ë¡œ
        target_samples_per_class: ê° í´ë˜ìŠ¤ë‹¹ ëª©í‘œ ìƒ˜í”Œ ìˆ˜ (250ìœ¼ë¡œ ì¡°ì •)
    
    Returns:
        dict: ê° í´ë˜ìŠ¤ë³„ íŒŒì¼ë‹¹ ìƒ˜í”Œ ìˆ˜
    """
    event_folders = ['fire', 'gas', 'scream']  # sparkëŠ” ì œì™¸ (í´ë˜ìŠ¤ ë§¤í•‘ì— ì—†ìŒ)
    file_counts = {}
    weights = {}
    
    print("ğŸ“Š ìœ„í—˜ ì†ŒìŒ íŒŒì¼ ê°œìˆ˜ ë¶„ì„:")
    print("-" * 40)
    
    # ê° í´ë”ì˜ íŒŒì¼ ê°œìˆ˜ ê³„ì‚°
    for folder in event_folders:
        folder_path = os.path.join(envsound_folder, folder)
        if os.path.exists(folder_path):
            wav_files = glob.glob(os.path.join(folder_path, '*.wav'))
            mp3_files = glob.glob(os.path.join(folder_path, '*.mp3'))
            total_files = len(wav_files) + len(mp3_files)
            file_counts[folder] = total_files
            print(f"  {folder}: {total_files}ê°œ íŒŒì¼")
        else:
            file_counts[folder] = 0
            print(f"  {folder}: í´ë” ì—†ìŒ")
    
    print(f"\nğŸ¯ ëª©í‘œ: ê° í´ë˜ìŠ¤ë‹¹ {target_samples_per_class}ê°œ ìƒ˜í”Œ ìƒì„±")
    print("âš–ï¸ ìë™ ê³„ì‚°ëœ ê°€ì¤‘ì¹˜:")
    print("-" * 40)
    
    # ê°€ì¤‘ì¹˜ ê³„ì‚°
    for folder, file_count in file_counts.items():
        if file_count > 0:
            samples_per_file = target_samples_per_class / file_count
            
            # ë„ˆë¬´ ì‘ì€ ê°’ì€ í™•ë¥ ì  ìƒ˜í”Œë§ìœ¼ë¡œ ì²˜ë¦¬
            if samples_per_file < 1:
                weights[folder] = round(samples_per_file, 2)
                print(f"  {folder}: {samples_per_file:.2f} (í™•ë¥ ì  ìƒ˜í”Œë§: {samples_per_file*100:.1f}%)")
            else:
                weights[folder] = max(1, round(samples_per_file))
                print(f"  {folder}: {weights[folder]} (íŒŒì¼ë‹¹ ìƒ˜í”Œ ìˆ˜)")
        else:
            weights[folder] = 0
            print(f"  {folder}: 0 (íŒŒì¼ ì—†ìŒ)")
    
    print(f"\nì˜ˆìƒ ì´ ìƒ˜í”Œ ìˆ˜:")
    total_expected = 0
    for folder, weight in weights.items():
        expected = file_counts[folder] * weight if weight >= 1 else file_counts[folder] * weight
        total_expected += expected
        print(f"  {folder}: {expected:.0f}ê°œ")
    print(f"  ì´í•©: {total_expected:.0f}ê°œ")
    
    return weights

def create_version_folder(version):
    """
    ë²„ì „ë³„ ê²°ê³¼ë¬¼ ì €ì¥ í´ë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        version: ë²„ì „ ë¬¸ìì—´ (ì˜ˆ: "v1.0")
    
    Returns:
        str: ìƒì„±ëœ í´ë” ê²½ë¡œ
    """
    folder_name = f"model_results_{version}"
    
    if not os.path.exists(folder_name):
        os.makedirs(folder_name)
        print(f"ğŸ“ ê²°ê³¼ë¬¼ í´ë” ìƒì„±: {folder_name}")
    
    return folder_name

# ---------------------------
# 1) ìŠ¤ë§ˆíŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ í•¨ìˆ˜ë“¤ (v1.35 ìƒˆë¡œ ì¶”ê°€)
# ---------------------------
def detect_sound_activity(audio, sr, threshold=0.01, frame_length=2048, hop_length=512):
    """
    ì˜¤ë””ì˜¤ì—ì„œ ì‹¤ì œ ì†Œë¦¬ê°€ ìˆëŠ” êµ¬ê°„ì„ ê°ì§€í•©ë‹ˆë‹¤.
    
    Args:
        audio: ì˜¤ë””ì˜¤ ë°ì´í„°
        sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸
        threshold: RMS ì„ê³„ê°’
        frame_length: í”„ë ˆì„ ê¸¸ì´
        hop_length: í™‰ ê¸¸ì´
    
    Returns:
        numpy.array: ê° í”„ë ˆì„ë³„ ì†Œë¦¬ í™œë™ ì—¬ë¶€ (True/False)
    """
    # RMS ì—ë„ˆì§€ ê³„ì‚°
    rms = librosa.feature.rms(y=audio, frame_length=frame_length, hop_length=hop_length)[0]
    
    # ì„ê³„ê°’ë³´ë‹¤ í° êµ¬ê°„ì„ í™œì„± êµ¬ê°„ìœ¼ë¡œ íŒì •
    activity = rms > threshold
    
    return activity

def extract_sound_segments(audio, sr, min_duration=2.0, max_duration=7.0, 
                          threshold=0.01, min_sound_ratio=0.4):
    """
    ì˜¤ë””ì˜¤ì—ì„œ ì‹¤ì œ ì†Œë¦¬ê°€ ìˆëŠ” ì˜ë¯¸ìˆëŠ” ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ê°„í—ì  ì†Œë¦¬ íŒ¨í„´(ë¹„ëª…-ë¬´ìŒ-ë¹„ëª…)ë„ í•˜ë‚˜ì˜ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        audio: ì…ë ¥ ì˜¤ë””ì˜¤
        sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸
        min_duration: ìµœì†Œ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)
        max_duration: ìµœëŒ€ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)
        threshold: ì†Œë¦¬ ê°ì§€ ì„ê³„ê°’
        min_sound_ratio: ì„¸ê·¸ë¨¼íŠ¸ ë‚´ ìµœì†Œ ì†Œë¦¬ ë¹„ìœ¨
    
    Returns:
        list: ì¶”ì¶œëœ ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    if len(audio) == 0:
        return []
    
    # ì†Œë¦¬ í™œë™ ê°ì§€
    hop_length = 512
    frame_length = 2048
    activity = detect_sound_activity(audio, sr, threshold, frame_length, hop_length)
    
    # í”„ë ˆì„ì„ ìƒ˜í”Œ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
    frame_to_sample = lambda frame_idx: frame_idx * hop_length
    
    # ì§§ì€ ë¬´ìŒ êµ¬ê°„ì„ ë©”ì›Œì„œ ì—°ê²° (ê°„í—ì  ì†Œë¦¬ íŒ¨í„´ ì²˜ë¦¬)
    max_gap_frames = int(2.0 * sr / hop_length)  # 2ì´ˆ ì´í•˜ì˜ ë¬´ìŒì€ ë©”ì›€
    filled_activity = activity.copy()
    
    # í™œì„± êµ¬ê°„ ì‚¬ì´ì˜ ì§§ì€ ë¬´ìŒì„ ë©”ìš°ê¸°
    active_indices = np.where(activity)[0]
    if len(active_indices) > 1:
        for i in range(len(active_indices) - 1):
            start_gap = active_indices[i] + 1
            end_gap = active_indices[i + 1]
            gap_length = end_gap - start_gap
            
            # 2ì´ˆ ì´í•˜ì˜ ë¬´ìŒ êµ¬ê°„ì€ í™œì„±ìœ¼ë¡œ ë³€ê²½ (ë¹„ëª…-ë¬´ìŒ-ë¹„ëª… íŒ¨í„´ ì²˜ë¦¬)
            if gap_length <= max_gap_frames:
                filled_activity[start_gap:end_gap] = True
    
    # ì—°ì†ëœ í™œì„± êµ¬ê°„ ì°¾ê¸° (ê°œì„ ëœ ë²„ì „)
    active_regions = []
    start_frame = None
    
    for i, is_active in enumerate(filled_activity):
        if is_active and start_frame is None:
            start_frame = i
        elif not is_active and start_frame is not None:
            end_frame = i
            duration = (end_frame - start_frame) * hop_length / sr
            
            if duration >= min_duration:
                start_sample = frame_to_sample(start_frame)
                end_sample = frame_to_sample(end_frame)
                active_regions.append((start_sample, end_sample))
            
            start_frame = None
    
    # ë§ˆì§€ë§‰ êµ¬ê°„ ì²˜ë¦¬
    if start_frame is not None:
        end_frame = len(filled_activity)
        duration = (end_frame - start_frame) * hop_length / sr
        if duration >= min_duration:
            start_sample = frame_to_sample(start_frame)
            end_sample = min(frame_to_sample(end_frame), len(audio))
            active_regions.append((start_sample, end_sample))
    
    # ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
    segments = []
    
    for start_sample, end_sample in active_regions:
        segment_audio = audio[start_sample:end_sample]
        segment_duration = len(segment_audio) / sr
        
        # ë„ˆë¬´ ê¸´ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ë¶„í• 
        if segment_duration > max_duration:
            # ê²¹ì¹¨ì„ ê³ ë ¤í•œ ë¶„í• 
            segment_samples = int(max_duration * sr)
            overlap_samples = int(OVERLAP_RATIO * segment_samples)
            step_samples = segment_samples - overlap_samples
            
            for i in range(0, len(segment_audio) - segment_samples + 1, step_samples):
                sub_segment = segment_audio[i:i + segment_samples]
                
                # ì†Œë¦¬ ë¹„ìœ¨ í™•ì¸ (ì›ë³¸ activity ì‚¬ìš©)
                sub_start_frame = int((start_sample + i) / hop_length)
                sub_end_frame = int((start_sample + i + segment_samples) / hop_length)
                sub_end_frame = min(sub_end_frame, len(activity))
                
                if sub_end_frame > sub_start_frame:
                    sub_activity = activity[sub_start_frame:sub_end_frame]
                    sound_ratio = np.sum(sub_activity) / len(sub_activity) if len(sub_activity) > 0 else 0
                    
                    # ê°„í—ì  ì†Œë¦¬ì˜ ê²½ìš° ê¸°ì¤€ì„ ë‚®ì¶¤ (30%)
                    adjusted_min_ratio = min_sound_ratio * 0.75
                    if sound_ratio >= adjusted_min_ratio:
                        segments.append(sub_segment)
            
            # ë§ˆì§€ë§‰ ë‚¨ì€ ë¶€ë¶„ ì²˜ë¦¬
            remaining_length = len(segment_audio) % step_samples
            if remaining_length > min_duration * sr:
                last_segment = segment_audio[-int(max_duration * sr):]
                last_start_frame = int((end_sample - len(last_segment)) / hop_length)
                last_end_frame = int(end_sample / hop_length)
                last_end_frame = min(last_end_frame, len(activity))
                
                if last_end_frame > last_start_frame:
                    last_activity = activity[last_start_frame:last_end_frame]
                    last_sound_ratio = np.sum(last_activity) / len(last_activity) if len(last_activity) > 0 else 0
                    
                    adjusted_min_ratio = min_sound_ratio * 0.75
                    if last_sound_ratio >= adjusted_min_ratio:
                        segments.append(last_segment)
        else:
            # ì ì ˆí•œ ê¸¸ì´ì˜ ì„¸ê·¸ë¨¼íŠ¸
            start_frame = int(start_sample / hop_length)
            end_frame = int(end_sample / hop_length)
            end_frame = min(end_frame, len(activity))
            
            if end_frame > start_frame:
                segment_activity = activity[start_frame:end_frame]
                sound_ratio = np.sum(segment_activity) / len(segment_activity) if len(segment_activity) > 0 else 0
                
                # ê°„í—ì  ì†Œë¦¬ì˜ ê²½ìš° ê¸°ì¤€ì„ ë‚®ì¶¤ (30%)
                adjusted_min_ratio = min_sound_ratio * 0.75
                if sound_ratio >= adjusted_min_ratio:
                    segments.append(segment_audio)
    
    return segments

def calculate_smart_weights(envsound_folder, target_samples_per_class=180):
    """
    ìŠ¤ë§ˆíŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ê¸°ë°˜ìœ¼ë¡œ ê° í´ë˜ìŠ¤ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        envsound_folder: ìœ„í—˜ ì†ŒìŒ í´ë” ê²½ë¡œ
        target_samples_per_class: ê° í´ë˜ìŠ¤ë‹¹ ëª©í‘œ ìƒ˜í”Œ ìˆ˜
    
    Returns:
        dict: ê° í´ë˜ìŠ¤ë³„ ì •ë³´ ë° ê°€ì¤‘ì¹˜
    """
    event_folders = ['fire', 'gas', 'scream']
    class_info = {}
    
    print("ğŸ” ìŠ¤ë§ˆíŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë¶„ì„ ì‹œì‘...")
    print("=" * 60)
    
    for folder in event_folders:
        folder_path = os.path.join(envsound_folder, folder)
        total_segments = 0
        file_count = 0
        
        if os.path.exists(folder_path):
            # ëª¨ë“  ì˜¤ë””ì˜¤ íŒŒì¼ ë¶„ì„
            audio_files = glob.glob(os.path.join(folder_path, '*.wav')) + \
                         glob.glob(os.path.join(folder_path, '*.mp3'))
            
            print(f"\nğŸ“‚ {folder.upper()} í´ë˜ìŠ¤ ë¶„ì„:")
            print("-" * 40)
            
            for audio_file in audio_files:
                try:
                    # ì˜¤ë””ì˜¤ ë¡œë“œ
                    audio, sr = librosa.load(audio_file, sr=16000)
                    
                    # ìŠ¤ë§ˆíŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì ìš©
                    segments = extract_sound_segments(
                        audio, sr, 
                        min_duration=MIN_SEGMENT_DURATION,
                        max_duration=MAX_SEGMENT_DURATION,
                        threshold=SILENCE_THRESHOLD,
                        min_sound_ratio=MIN_SOUND_RATIO
                    )
                    
                    file_segments = len(segments)
                    total_segments += file_segments
                    file_count += 1
                    
                    # íŒŒì¼ ì •ë³´ ì¶œë ¥ (ë„ˆë¬´ ë§ìœ¼ë©´ ì¼ë¶€ë§Œ)
                    if file_count <= 5 or file_count % 10 == 0:
                        duration = len(audio) / sr
                        print(f"  ğŸ“„ {os.path.basename(audio_file)}: "
                              f"{duration:.1f}ì´ˆ â†’ {file_segments}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
                    
                except Exception as e:
                    print(f"  âŒ {os.path.basename(audio_file)}: ì˜¤ë¥˜ - {e}")
                    continue
            
            avg_segments_per_file = total_segments / file_count if file_count > 0 else 0
            
            print(f"\nğŸ“Š {folder.upper()} í´ë˜ìŠ¤ ìš”ì•½:")
            print(f"  â€¢ íŒŒì¼ ìˆ˜: {file_count}ê°œ")
            print(f"  â€¢ ì¶”ì¶œëœ ì´ ì„¸ê·¸ë¨¼íŠ¸: {total_segments}ê°œ")
            print(f"  â€¢ íŒŒì¼ë‹¹ í‰ê·  ì„¸ê·¸ë¨¼íŠ¸: {avg_segments_per_file:.1f}ê°œ")
            
            # ëª©í‘œ ìƒ˜í”Œ ìˆ˜ì— ë§ëŠ” ì„ íƒ ë¹„ìœ¨ ê³„ì‚°
            if total_segments > 0:
                selection_ratio = min(1.0, target_samples_per_class / total_segments)
                expected_samples = int(total_segments * selection_ratio)
            else:
                selection_ratio = 0
                expected_samples = 0
            
            print(f"  â€¢ ì„ íƒ ë¹„ìœ¨: {selection_ratio:.3f} ({expected_samples}ê°œ ìƒ˜í”Œ ì˜ˆìƒ)")
            
            class_info[folder] = {
                'file_count': file_count,
                'total_segments': total_segments,
                'avg_segments_per_file': avg_segments_per_file,
                'selection_ratio': selection_ratio,
                'expected_samples': expected_samples
            }
        else:
            print(f"\nâŒ {folder.upper()} í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
            class_info[folder] = {
                'file_count': 0,
                'total_segments': 0,
                'avg_segments_per_file': 0,
                'selection_ratio': 0,
                'expected_samples': 0
            }
    
    # ì „ì²´ ìš”ì•½
    print("\nğŸ¯ ìŠ¤ë§ˆíŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìš”ì•½:")
    print("=" * 60)
    total_expected = sum(info['expected_samples'] for info in class_info.values())
    
    for class_name, info in class_info.items():
        ratio = (info['expected_samples'] / total_expected * 100) if total_expected > 0 else 0
        print(f"{class_name.upper():>8}: {info['expected_samples']:>3}ê°œ ìƒ˜í”Œ ({ratio:>5.1f}%)")
    
    print(f"{'ì´í•©':>8}: {total_expected:>3}ê°œ ìƒ˜í”Œ")
    print("=" * 60)
    
    return class_info

# ---------------------------
# 2) ë¬´ìŒ ì œê±° í•¨ìˆ˜ (ê¸°ì¡´)
# ---------------------------
def remove_silence(y, sr, top_db=20):
    intervals = librosa.effects.split(y, top_db=top_db)
    if len(intervals) == 0:
        return y
    non_silent_audio = np.concatenate([y[start:end] for start, end in intervals])
    return non_silent_audio

# ---------------------------
# 2) ë¬´ìŒ ìƒì„± í•¨ìˆ˜
# ---------------------------
def generate_silence(duration_sec, sr):
    """ë¬´ìŒ ë°ì´í„° ìƒì„±"""
    length = int(sr * duration_sec)
    # ì™„ì „í•œ ë¬´ìŒì´ ì•„ë‹Œ ë§¤ìš° ì‘ì€ ë…¸ì´ì¦ˆ ì¶”ê°€ (í˜„ì‹¤ì ì¸ ë¬´ìŒ)
    silence = np.random.normal(0, 0.001, length).astype(np.float32)
    return silence

# ---------------------------
# 3) ì €ìŒëŸ‰ ë°°ê²½ ì†ŒìŒ ìƒì„± í•¨ìˆ˜
# ---------------------------
def generate_background_noise(duration_sec, sr):
    """ì €ìŒëŸ‰ ë°°ê²½ ì†ŒìŒ ìƒì„± (ì—ì–´ì»¨, ë¯¸ì„¸í•œ ì†ŒìŒ ë“±)"""
    length = int(sr * duration_sec)
    # ì €ì£¼íŒŒ ë…¸ì´ì¦ˆ ìƒì„±
    noise = np.random.normal(0, 0.01, length).astype(np.float32)
    # ì €ì—­ í†µê³¼ í•„í„° íš¨ê³¼ (ê°„ë‹¨í•œ ì´ë™í‰ê· )
    window_size = min(50, length)  # lengthë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ê²Œ ì œí•œ
    if window_size > 0:
        noise_filtered = np.convolve(noise, np.ones(window_size)/window_size, mode='same')
    else:
        noise_filtered = noise
    
    # ì •í™•í•œ ê¸¸ì´ë¡œ ì¡°ì •
    if len(noise_filtered) != length:
        if len(noise_filtered) > length:
            noise_filtered = noise_filtered[:length]
        else:
            noise_filtered = np.pad(noise_filtered, (0, length - len(noise_filtered)), mode='constant')
    
    return noise_filtered

# ---------------------------
# 4) ê³µì¥ ì†Œë¦¬ì™€ ìœ„í—˜ ì†Œë¦¬ í•©ì„± í•¨ìˆ˜
# ---------------------------
def mix_factory_and_event(factory_audio, event_audio, sr, desired_length=10.0):
    event_len = len(event_audio)
    factory_len = len(factory_audio)
    target_len = int(sr * desired_length)
    
    if factory_len < target_len:
        factory_audio = np.pad(factory_audio, (0, target_len - factory_len))
    else:
        factory_audio = factory_audio[:target_len]
    
    if event_len > target_len:
        event_audio = event_audio[:target_len] 
        event_len = target_len
    
    if event_len == 0:
        return factory_audio, 0, 0
        
    max_start = max(0, target_len - event_len)
    insert_pos = random.randint(0, max_start) if max_start > 0 else 0
    
    rms_factory = np.sqrt(np.mean(factory_audio**2))
    rms_event = np.sqrt(np.mean(event_audio**2))
    if rms_event > 0:
        # ìœ„í—˜ ì†Œë¦¬ì˜ ë³¼ë¥¨ì„ ëœë¤í•˜ê²Œ ì¡°ì • (ë” í˜„ì‹¤ì )
        volume_factor = random.uniform(0.3, 0.8)
        event_audio = event_audio * (rms_factory / rms_event) * volume_factor
    
    mixed_audio = factory_audio.copy()
    mixed_audio[insert_pos:insert_pos+event_len] += event_audio
    
    max_val = np.max(np.abs(mixed_audio))
    if max_val > 1.0:
        mixed_audio = mixed_audio / max_val
    
    return mixed_audio, insert_pos / sr, (insert_pos + event_len) / sr

# ---------------------------
# 5) YAMNet ì„ë² ë”© ì¶”ì¶œ í•¨ìˆ˜
# ---------------------------
def extract_yamnet_embeddings(audio, sr, yamnet_model):
    waveform = tf.convert_to_tensor(audio, dtype=tf.float32)
    waveform = tf.squeeze(waveform)
    yamnet_fn = yamnet_model.signatures['serving_default']
    yamnet_output = yamnet_fn(waveform=waveform)
    embeddings = yamnet_output['output_1'].numpy()  # (frames, 1024)
    return embeddings

# ---------------------------
# 6) ë¼ë²¨ ìƒì„± í•¨ìˆ˜ (5-í´ë˜ìŠ¤)
# ---------------------------
def generate_labels(start_sec, end_sec, class_id, total_duration=10.0, frame_length=0.48):
    """
    class_id: 0=ë¬´ìŒ, 1=ì •ìƒ(ê³µì¥ì†Œë¦¬), 2=í™”ì¬, 3=ê°€ìŠ¤ëˆ„ì¶œ, 4=ë¹„ëª…
    """
    num_frames = int(total_duration / frame_length)
    labels = np.zeros(num_frames, dtype=int)  # ê¸°ë³¸ê°’: ë¬´ìŒ(0)
    
    if class_id > 0:  # ë¬´ìŒì´ ì•„ë‹Œ ê²½ìš°
        if start_sec is not None and end_sec is not None and class_id > 1:
            # ìœ„í—˜ ì†Œë¦¬ê°€ ìˆëŠ” ê²½ìš° (í™”ì¬, ê°€ìŠ¤, ë¹„ëª…)
            start_frame = int(start_sec / frame_length)
            end_frame = int(end_sec / frame_length) + 1
            labels[start_frame:end_frame] = class_id
            # ë‚˜ë¨¸ì§€ êµ¬ê°„ì€ ì •ìƒ(ê³µì¥ì†Œë¦¬)ë¡œ ì„¤ì •
            labels[:start_frame] = 1
            labels[end_frame:] = 1
        else:
            # ì „ì²´ê°€ í•´ë‹¹ í´ë˜ìŠ¤ (ë¬´ìŒ ë˜ëŠ” ì •ìƒ)
            labels[:] = class_id
    
    return labels

# ---------------------------
# 7) LSTM ëª¨ë¸ ì •ì˜ í•¨ìˆ˜ (5-í´ë˜ìŠ¤)
# ---------------------------
def create_lstm_model(input_shape, num_classes=5):
    model = Sequential([
        LSTM(128, input_shape=input_shape, return_sequences=True),
        Dropout(0.4),
        LSTM(64, return_sequences=True),
        Dropout(0.4),
        LSTM(32, return_sequences=True),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dense(num_classes, activation='softmax')
    ])
    
    optimizer = Adam(learning_rate=0.001)
    
    model.compile(optimizer=optimizer,
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# ---------------------------
# 9) ë¬´ìŒì—ì„œ ê³µì¥ì†Œë¦¬ë¡œ ì „í™˜ë˜ëŠ” ë°ì´í„° ìƒì„± í•¨ìˆ˜
# ---------------------------
def create_silence_to_factory_transition(factory_audio, sr, total_duration=10.0):
    """
    ë¬´ìŒ ìƒíƒœì—ì„œ ê³µì¥ ì†Œë¦¬ê°€ ì‹œì‘ë˜ëŠ” ì „í™˜ ë°ì´í„° ìƒì„±
    
    Args:
        factory_audio: ê³µì¥ ì†Œë¦¬ ì˜¤ë””ì˜¤
        sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸
        total_duration: ì´ ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
    
    Returns:
        tuple: (ì „í™˜_ì˜¤ë””ì˜¤, ê³µì¥ì†Œë¦¬_ì‹œì‘ì‹œê°„)
    """
    target_len = int(sr * total_duration)
    
    # ê³µì¥ ì†Œë¦¬ ì¤€ë¹„
    if len(factory_audio) > target_len:
        start_pos = random.randint(0, len(factory_audio) - target_len)
        factory_audio = factory_audio[start_pos:start_pos + target_len]
    else:
        factory_audio = np.pad(factory_audio, (0, max(0, target_len - len(factory_audio))))
    
    # ì „í™˜ ì‹œì  ê²°ì • (ì „ì²´ ê¸¸ì´ì˜ 20%~80% ì§€ì ì—ì„œ ì‹œì‘)
    transition_start_ratio = random.uniform(0.2, 0.8)
    transition_frame = int(target_len * transition_start_ratio)
    
    # ë¬´ìŒ êµ¬ê°„ê³¼ ê³µì¥ì†Œë¦¬ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
    mixed_audio = np.zeros(target_len, dtype=np.float32)
    
    # ì•ë¶€ë¶„ì€ ë¬´ìŒ (ë§¤ìš° ì‘ì€ ë°°ê²½ ë…¸ì´ì¦ˆ)
    silence_part = generate_background_noise(transition_frame / sr, sr)
    # ì •í™•í•œ ê¸¸ì´ ë³´ì¥
    if len(silence_part) != transition_frame:
        if len(silence_part) > transition_frame:
            silence_part = silence_part[:transition_frame]
        else:
            silence_part = np.pad(silence_part, (0, transition_frame - len(silence_part)), mode='constant')
    mixed_audio[:transition_frame] = silence_part
    
    # ë’·ë¶€ë¶„ì€ ê³µì¥ ì†Œë¦¬
    factory_part = factory_audio[transition_frame:]
    remaining_samples = target_len - transition_frame
    # ì •í™•í•œ ê¸¸ì´ ë³´ì¥
    if len(factory_part) != remaining_samples:
        if len(factory_part) > remaining_samples:
            factory_part = factory_part[:remaining_samples]
        else:
            factory_part = np.pad(factory_part, (0, remaining_samples - len(factory_part)), mode='constant')
    mixed_audio[transition_frame:] = factory_part
    
    # ì „í™˜ ì§€ì ì—ì„œ ë¶€ë“œëŸ¬ìš´ fade-in íš¨ê³¼ (ë” í˜„ì‹¤ì )
    fade_duration = int(sr * 0.5)  # 0.5ì´ˆ í˜ì´ë“œì¸
    if transition_frame + fade_duration < target_len:
        fade_samples = min(fade_duration, len(factory_part))
        if fade_samples > 0:
            fade_curve = np.linspace(0, 1, fade_samples)
            mixed_audio[transition_frame:transition_frame + fade_samples] *= fade_curve
    
    # ë³¼ë¥¨ ì •ê·œí™”
    max_val = np.max(np.abs(mixed_audio))
    if max_val > 1.0:
        mixed_audio = mixed_audio / max_val
    
    transition_start_sec = transition_frame / sr
    
    return mixed_audio, transition_start_sec

# ---------------------------
# 10) ë¬´ìŒâ†’ê³µì¥ì†Œë¦¬ ì „í™˜ ë¼ë²¨ ìƒì„± í•¨ìˆ˜
# ---------------------------
def generate_transition_labels(transition_start_sec, total_duration=10.0, frame_length=0.48):
    """
    ë¬´ìŒâ†’ê³µì¥ì†Œë¦¬ ì „í™˜ì— ëŒ€í•œ í”„ë ˆì„ë³„ ë¼ë²¨ ìƒì„±
    
    Args:
        transition_start_sec: ê³µì¥ì†Œë¦¬ ì‹œì‘ ì‹œê°„ (ì´ˆ)
        total_duration: ì´ ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
        frame_length: í”„ë ˆì„ ê¸¸ì´ (ì´ˆ)
    
    Returns:
        numpy.array: í”„ë ˆì„ë³„ í´ë˜ìŠ¤ ë¼ë²¨ (0=ë¬´ìŒ, 1=ì •ìƒ)
    """
    num_frames = int(total_duration / frame_length)
    labels = np.zeros(num_frames, dtype=int)  # ê¸°ë³¸ê°’: ë¬´ìŒ(0)
    
    # ì „í™˜ ì‹œì  ê³„ì‚°
    transition_frame = int(transition_start_sec / frame_length)
    
    # ì „í™˜ ì´í›„ëŠ” ì •ìƒ(ê³µì¥ì†Œë¦¬)ë¡œ ì„¤ì •
    labels[transition_frame:] = 1
    
    return labels

# ---------------------------
# 11) ë¬´ìŒì—ì„œ ìœ„í—˜ì†Œë¦¬ë¡œ ì „í™˜ë˜ëŠ” ë°ì´í„° ìƒì„± í•¨ìˆ˜
# ---------------------------
def create_silence_to_danger_transition(event_audio, sr, class_id, total_duration=10.0):
    """
    ë¬´ìŒ ìƒíƒœì—ì„œ ìœ„í—˜ ì†Œë¦¬ê°€ ì‹œì‘ë˜ëŠ” ì „í™˜ ë°ì´í„° ìƒì„±
    
    Args:
        event_audio: ìœ„í—˜ ì†Œë¦¬ ì˜¤ë””ì˜¤
        sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸
        class_id: ìœ„í—˜ì†Œë¦¬ í´ë˜ìŠ¤ ID (2=í™”ì¬, 3=ê°€ìŠ¤, 4=ë¹„ëª…)
        total_duration: ì´ ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
    
    Returns:
        tuple: (ì „í™˜_ì˜¤ë””ì˜¤, ìœ„í—˜ì†Œë¦¬_ì‹œì‘ì‹œê°„, ìœ„í—˜ì†Œë¦¬_ëì‹œê°„)
    """
    target_len = int(sr * total_duration)
    
    # ìœ„í—˜ ì†Œë¦¬ ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ë‚´ê¸°)
    max_event_len = int(sr * 6.0)  # ìµœëŒ€ 6ì´ˆ
    if len(event_audio) > max_event_len:
        start_pos = random.randint(0, len(event_audio) - max_event_len)
        event_audio = event_audio[start_pos:start_pos + max_event_len]
    
    event_len = len(event_audio)
    
    # ì „í™˜ ì‹œì  ê²°ì • (ì „ì²´ ê¸¸ì´ì˜ 20%~60% ì§€ì ì—ì„œ ì‹œì‘, ìœ„í—˜ì†Œë¦¬ë¥¼ ìœ„í•´ ë” ì¼ì°)
    transition_start_ratio = random.uniform(0.2, 0.6)
    transition_frame = int(target_len * transition_start_ratio)
    
    # ìœ„í—˜ì†Œë¦¬ê°€ ëë‚˜ëŠ” ì§€ì  ê³„ì‚°
    event_end_frame = min(transition_frame + event_len, target_len)
    actual_event_len = event_end_frame - transition_frame
    
    # ì „ì²´ ì˜¤ë””ì˜¤ ìƒì„±
    mixed_audio = np.zeros(target_len, dtype=np.float32)
    
    # ì•ë¶€ë¶„ì€ ë¬´ìŒ (ë§¤ìš° ì‘ì€ ë°°ê²½ ë…¸ì´ì¦ˆ)
    silence_part = generate_background_noise(transition_frame / sr, sr)
    # ì •í™•í•œ ê¸¸ì´ ë³´ì¥
    if len(silence_part) != transition_frame:
        if len(silence_part) > transition_frame:
            silence_part = silence_part[:transition_frame]
        else:
            silence_part = np.pad(silence_part, (0, transition_frame - len(silence_part)), mode='constant')
    mixed_audio[:transition_frame] = silence_part
    
    # ì¤‘ê°„ ë¶€ë¶„ì€ ìœ„í—˜ ì†Œë¦¬
    if actual_event_len > 0:
        event_part = event_audio[:actual_event_len]
        
        # ìœ„í—˜ì†Œë¦¬ ë³¼ë¥¨ ì¡°ì • (ë” í˜„ì‹¤ì ì¸ ë³¼ë¥¨)
        event_rms = np.sqrt(np.mean(event_part**2))
        if event_rms > 0:
            # ìœ„í—˜ì†Œë¦¬ëŠ” ë°°ê²½ë³´ë‹¤ ì¶©ë¶„íˆ í¬ê²Œ (í•˜ì§€ë§Œ í´ë¦¬í•‘ ë°©ì§€)
            target_rms = random.uniform(0.1, 0.3)
            volume_factor = target_rms / event_rms
            event_part = event_part * volume_factor
        
        mixed_audio[transition_frame:event_end_frame] = event_part
        
        # ì „í™˜ ì§€ì ì—ì„œ ë¶€ë“œëŸ¬ìš´ fade-in íš¨ê³¼
        fade_duration = int(sr * 0.3)  # 0.3ì´ˆ í˜ì´ë“œì¸ (ìœ„í—˜ì†Œë¦¬ëŠ” ë¹ ë¥´ê²Œ)
        if transition_frame + fade_duration < event_end_frame:
            fade_samples = min(fade_duration, actual_event_len)
            fade_curve = np.linspace(0, 1, fade_samples)
            mixed_audio[transition_frame:transition_frame + fade_samples] *= fade_curve
    
    # ë’·ë¶€ë¶„ì€ ë‹¤ì‹œ ë¬´ìŒ (ìœ„í—˜ì†Œë¦¬ í›„ ì¡°ìš©í•´ì§)
    if event_end_frame < target_len:
        remaining_samples = target_len - event_end_frame
        remaining_silence = generate_background_noise((target_len - event_end_frame) / sr, sr)
        # ì •í™•í•œ ê¸¸ì´ ë³´ì¥
        if len(remaining_silence) != remaining_samples:
            if len(remaining_silence) > remaining_samples:
                remaining_silence = remaining_silence[:remaining_samples]
            else:
                remaining_silence = np.pad(remaining_silence, (0, remaining_samples - len(remaining_silence)), mode='constant')
        mixed_audio[event_end_frame:] = remaining_silence
    
    # ë³¼ë¥¨ ì •ê·œí™”
    max_val = np.max(np.abs(mixed_audio))
    if max_val > 1.0:
        mixed_audio = mixed_audio / max_val
    
    transition_start_sec = transition_frame / sr
    transition_end_sec = event_end_frame / sr
    
    return mixed_audio, transition_start_sec, transition_end_sec

# ---------------------------
# 12) ë¬´ìŒâ†’ìœ„í—˜ì†Œë¦¬ ì „í™˜ ë¼ë²¨ ìƒì„± í•¨ìˆ˜
# ---------------------------
def generate_danger_transition_labels(transition_start_sec, transition_end_sec, class_id, total_duration=10.0, frame_length=0.48):
    """
    ë¬´ìŒâ†’ìœ„í—˜ì†Œë¦¬ ì „í™˜ì— ëŒ€í•œ í”„ë ˆì„ë³„ ë¼ë²¨ ìƒì„±
    
    Args:
        transition_start_sec: ìœ„í—˜ì†Œë¦¬ ì‹œì‘ ì‹œê°„ (ì´ˆ)
        transition_end_sec: ìœ„í—˜ì†Œë¦¬ ë ì‹œê°„ (ì´ˆ)
        class_id: ìœ„í—˜ì†Œë¦¬ í´ë˜ìŠ¤ ID (2=í™”ì¬, 3=ê°€ìŠ¤, 4=ë¹„ëª…)
        total_duration: ì´ ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
        frame_length: í”„ë ˆì„ ê¸¸ì´ (ì´ˆ)
    
    Returns:
        numpy.array: í”„ë ˆì„ë³„ í´ë˜ìŠ¤ ë¼ë²¨ (0=ë¬´ìŒ, class_id=ìœ„í—˜ì†Œë¦¬)
    """
    num_frames = int(total_duration / frame_length)
    labels = np.zeros(num_frames, dtype=int)  # ê¸°ë³¸ê°’: ë¬´ìŒ(0)
    
    # ì „í™˜ ì‹œì  ê³„ì‚°
    start_frame = int(transition_start_sec / frame_length)
    end_frame = int(transition_end_sec / frame_length)
    
    # ìœ„í—˜ì†Œë¦¬ êµ¬ê°„ì€ í•´ë‹¹ í´ë˜ìŠ¤ë¡œ ì„¤ì •
    labels[start_frame:end_frame] = class_id
    
    # ë‚˜ë¨¸ì§€ëŠ” ë¬´ìŒ(0)ìœ¼ë¡œ ìœ ì§€
    
    return labels

def create_factory_to_factory_transition(factory_audio1, factory_audio2, sr, total_duration=10.0):
    """
    ê³µì¥ì†Œë¦¬ì—ì„œ ë‹¤ë¥¸ ê³µì¥ì†Œë¦¬ë¡œ ì „í™˜ë˜ëŠ” ì˜¤ë””ì˜¤ ìƒì„±
    
    Args:
        factory_audio1: ì²« ë²ˆì§¸ ê³µì¥ì†Œë¦¬ ì˜¤ë””ì˜¤
        factory_audio2: ë‘ ë²ˆì§¸ ê³µì¥ì†Œë¦¬ ì˜¤ë””ì˜¤
        sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸
        total_duration: ì´ ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
    
    Returns:
        tuple: (ì „í™˜ ì˜¤ë””ì˜¤, ì „í™˜ ì‹œì (ì´ˆ))
    """
    total_samples = int(total_duration * sr)
    
    # ì „í™˜ ì‹œì ì„ 30~70% ì‚¬ì´ì—ì„œ ëœë¤ ì„ íƒ
    transition_ratio = random.uniform(0.3, 0.7)
    transition_start_sec = total_duration * transition_ratio
    transition_samples = int(transition_start_sec * sr)
    
    # í˜ì´ë“œ ì „í™˜ ê¸¸ì´ (0.1ì´ˆ)
    fade_duration = 0.1
    fade_samples = int(fade_duration * sr)
    
    # ê²°ê³¼ ì˜¤ë””ì˜¤ ì´ˆê¸°í™”
    result_audio = np.zeros(total_samples)
    
    # ì²« ë²ˆì§¸ ê³µì¥ì†Œë¦¬ë¡œ ì‹œì‘ ë¶€ë¶„ ì±„ìš°ê¸°
    if len(factory_audio1) > 0:
        factory1_repeated = np.tile(factory_audio1, (transition_samples // len(factory_audio1)) + 1)
        result_audio[:transition_samples] = factory1_repeated[:transition_samples]
    
    # ë‘ ë²ˆì§¸ ê³µì¥ì†Œë¦¬ë¡œ ë‚˜ë¨¸ì§€ ë¶€ë¶„ ì±„ìš°ê¸°
    if len(factory_audio2) > 0:
        remaining_samples = total_samples - transition_samples
        factory2_repeated = np.tile(factory_audio2, (remaining_samples // len(factory_audio2)) + 1)
        second_part = factory2_repeated[:remaining_samples]
        
        # ì •í™•í•œ ê¸¸ì´ ë³´ì¥
        if len(second_part) != remaining_samples:
            if len(second_part) > remaining_samples:
                second_part = second_part[:remaining_samples]
            else:
                second_part = np.pad(second_part, (0, remaining_samples - len(second_part)), mode='constant')
        
        # í˜ì´ë“œ ì „í™˜ ì ìš©
        if transition_samples + fade_samples <= total_samples:
            # ì²« ë²ˆì§¸ ì†Œë¦¬ í˜ì´ë“œ ì•„ì›ƒ
            fade_start = max(0, transition_samples - fade_samples)
            fade_end = transition_samples
            fade_length = fade_end - fade_start
            if fade_length > 0:
                fade_out = np.linspace(1, 0, fade_length)
                result_audio[fade_start:fade_end] *= fade_out
            
            # ë‘ ë²ˆì§¸ ì†Œë¦¬ í˜ì´ë“œ ì¸
            second_fade_start = transition_samples
            second_fade_end = min(total_samples, transition_samples + fade_samples)
            second_fade_length = second_fade_end - second_fade_start
            if second_fade_length > 0 and second_fade_length <= len(second_part):
                fade_in = np.linspace(0, 1, second_fade_length)
                second_part[:second_fade_length] *= fade_in
        
        # ìµœì¢… ê²°í•© ì „ ê¸¸ì´ ì¬í™•ì¸
        end_index = min(len(result_audio), transition_samples + len(second_part))
        copy_length = end_index - transition_samples
        if copy_length > 0:
            result_audio[transition_samples:end_index] = second_part[:copy_length]
    
    return result_audio, transition_start_sec

def create_silence_variation_transition(sr, total_duration=10.0):
    """
    ë‹¤ì–‘í•œ ë¬´ìŒ/ë°°ê²½ì†ŒìŒ ë³€í™” ì˜¤ë””ì˜¤ ìƒì„±
    
    Args:
        sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸
        total_duration: ì´ ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
    
    Returns:
        tuple: (ë³€í™” ì˜¤ë””ì˜¤, ë³€í™” ì‹œì (ì´ˆ))
    """
    total_samples = int(total_duration * sr)
    
    # ë³€í™” ì‹œì ì„ 20~80% ì‚¬ì´ì—ì„œ ëœë¤ ì„ íƒ
    transition_ratio = random.uniform(0.2, 0.8)
    transition_start_sec = total_duration * transition_ratio
    transition_samples = int(transition_start_sec * sr)
    
    # ê²°ê³¼ ì˜¤ë””ì˜¤ ì´ˆê¸°í™”
    result_audio = np.zeros(total_samples)
    
    # ì²« ë²ˆì§¸ ë¬´ìŒ/ë°°ê²½ì†ŒìŒ íƒ€ì…
    silence_type1 = random.choice(['pure_silence', 'low_noise', 'white_noise'])
    if silence_type1 == 'pure_silence':
        first_part = np.zeros(transition_samples)
    elif silence_type1 == 'low_noise':
        first_part = generate_background_noise(transition_start_sec, sr)
        # ì •í™•í•œ ê¸¸ì´ ë³´ì¥
        if len(first_part) != transition_samples:
            if len(first_part) > transition_samples:
                first_part = first_part[:transition_samples]
            else:
                first_part = np.pad(first_part, (0, transition_samples - len(first_part)), mode='constant')
    else:  # white_noise
        first_part = np.random.normal(0, 0.001, transition_samples)
    
    # ë‘ ë²ˆì§¸ ë¬´ìŒ/ë°°ê²½ì†ŒìŒ íƒ€ì… (ì²« ë²ˆì§¸ì™€ ë‹¤ë¥´ê²Œ)
    remaining_duration = total_duration - transition_start_sec
    remaining_samples = total_samples - transition_samples
    
    silence_types = ['pure_silence', 'low_noise', 'white_noise']
    silence_types.remove(silence_type1)  # ì²« ë²ˆì§¸ì™€ ë‹¤ë¥¸ íƒ€ì… ì„ íƒ
    silence_type2 = random.choice(silence_types)
    
    if silence_type2 == 'pure_silence':
        second_part = np.zeros(remaining_samples)
    elif silence_type2 == 'low_noise':
        second_part = generate_background_noise(remaining_duration, sr)
        # ì •í™•í•œ ê¸¸ì´ ë³´ì¥
        if len(second_part) != remaining_samples:
            if len(second_part) > remaining_samples:
                second_part = second_part[:remaining_samples]
            else:
                second_part = np.pad(second_part, (0, remaining_samples - len(second_part)), mode='constant')
    else:  # white_noise
        second_part = np.random.normal(0, 0.001, remaining_samples)
    
    # í˜ì´ë“œ ì „í™˜ ì ìš© (0.2ì´ˆ)
    fade_duration = 0.2
    fade_samples = int(fade_duration * sr)
    
    if transition_samples >= fade_samples and remaining_samples >= fade_samples:
        # ì²« ë²ˆì§¸ ë¶€ë¶„ í˜ì´ë“œ ì•„ì›ƒ
        fade_start = transition_samples - fade_samples
        fade_out = np.linspace(1, 0, fade_samples)
        if len(first_part) >= fade_samples:
            first_part[fade_start:] *= fade_out
        
        # ë‘ ë²ˆì§¸ ë¶€ë¶„ í˜ì´ë“œ ì¸
        fade_in = np.linspace(0, 1, fade_samples)
        if len(second_part) >= fade_samples:
            second_part[:fade_samples] *= fade_in
    
    # ê²°í•© (ê¸¸ì´ ì¬í™•ì¸)
    copy_length1 = min(len(first_part), transition_samples)
    if copy_length1 > 0:
        result_audio[:copy_length1] = first_part[:copy_length1]
    
    copy_start = transition_samples
    copy_end = min(len(result_audio), transition_samples + len(second_part))
    copy_length2 = copy_end - copy_start
    if copy_length2 > 0:
        result_audio[copy_start:copy_end] = second_part[:copy_length2]
    
    return result_audio, transition_start_sec

def generate_factory_transition_labels(transition_start_sec, total_duration=10.0, frame_length=0.48):
    """
    ê³µì¥ì†Œë¦¬â†’ë‹¤ë¥¸ê³µì¥ì†Œë¦¬ ì „í™˜ì— ëŒ€í•œ í”„ë ˆì„ë³„ ë¼ë²¨ ìƒì„±
    (ì „ì²´ êµ¬ê°„ì´ ëª¨ë‘ ì •ìƒ ê³µì¥ì†Œë¦¬ì´ë¯€ë¡œ ëª¨ë“  í”„ë ˆì„ì´ í´ë˜ìŠ¤ 1)
    
    Args:
        transition_start_sec: ì „í™˜ ì‹œì‘ ì‹œì  (ì´ˆ)
        total_duration: ì´ ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
        frame_length: í”„ë ˆì„ ê¸¸ì´ (ì´ˆ)
    
    Returns:
        numpy.array: í”„ë ˆì„ë³„ í´ë˜ìŠ¤ ë¼ë²¨ (ëª¨ë‘ 1=ì •ìƒê³µì¥ì†Œë¦¬)
    """
    num_frames = int(total_duration / frame_length)
    labels = np.ones(num_frames, dtype=int)  # ëª¨ë“  í”„ë ˆì„ì´ ì •ìƒ ê³µì¥ì†Œë¦¬ (1)
    return labels

def generate_silence_variation_labels(transition_start_sec, total_duration=10.0, frame_length=0.48):
    """
    ë¬´ìŒ ë³€í™”ì— ëŒ€í•œ í”„ë ˆì„ë³„ ë¼ë²¨ ìƒì„±
    (ì „ì²´ êµ¬ê°„ì´ ëª¨ë‘ ë¬´ìŒì´ë¯€ë¡œ ëª¨ë“  í”„ë ˆì„ì´ í´ë˜ìŠ¤ 0)
    
    Args:
        transition_start_sec: ë³€í™” ì‹œì‘ ì‹œì  (ì´ˆ)
        total_duration: ì´ ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
        frame_length: í”„ë ˆì„ ê¸¸ì´ (ì´ˆ)
    
    Returns:
        numpy.array: í”„ë ˆì„ë³„ í´ë˜ìŠ¤ ë¼ë²¨ (ëª¨ë‘ 0=ë¬´ìŒ)
    """
    num_frames = int(total_duration / frame_length)
    labels = np.zeros(num_frames, dtype=int)  # ëª¨ë“  í”„ë ˆì„ì´ ë¬´ìŒ (0)
    return labels

# ---------------------------
# 13) ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
# ---------------------------
def main():
    sr = 16000
    frame_length = 0.48
    total_duration = 10.0
    num_classes = 5  # ë¬´ìŒ(0), ì •ìƒ(1), í™”ì¬(2), ê°€ìŠ¤ëˆ„ì¶œ(3), ë¹„ëª…(4)
    
    # ë²„ì „ë³„ ê²°ê³¼ë¬¼ í´ë” ìƒì„±
    output_folder = create_version_folder(VERSION)
    
    # í´ë˜ìŠ¤ ë§¤í•‘
    class_mapping = {
        'fire': 2,
        'gas': 3, 
        'scream': 4
    }
    
    # 1) ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    mixture_folder = 'mixture'
    envsound_folder = 'envsound'
    
    # 2) ê³µì¥ ì†Œë¦¬ íŒŒì¼ë“¤ ë¶ˆëŸ¬ì˜¤ê¸°
    factory_paths = glob.glob(os.path.join(mixture_folder, '*.wav'))
    print(f"ê³µì¥ ì†Œë¦¬ íŒŒì¼ ìˆ˜: {len(factory_paths)}")
    
    # 3) ìœ„í—˜ ì†Œë¦¬ íŒŒì¼ë“¤ ë¶ˆëŸ¬ì˜¤ê¸° (í´ë˜ìŠ¤ë³„ë¡œ ë¶„ë¦¬)
    event_folders = ['fire', 'gas', 'scream']
    event_data = {}
    
    for folder in event_folders:
        folder_path = os.path.join(envsound_folder, folder)
        wav_files = glob.glob(os.path.join(folder_path, '*.wav'))
        mp3_files = glob.glob(os.path.join(folder_path, '*.mp3'))
        event_data[folder] = wav_files + mp3_files
        print(f"{folder}: {len(event_data[folder])}ê°œ íŒŒì¼")
    
    # ê°€ì¤‘ì¹˜ ì„¤ì • (ìë™ ë˜ëŠ” ìˆ˜ë™)
    if AUTO_WEIGHT_CALCULATION and ENABLE_SMART_SEGMENTATION:
        print(f"\nğŸ”„ ìŠ¤ë§ˆíŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ê¸°ë°˜ ìë™ ê°€ì¤‘ì¹˜ ê³„ì‚° ëª¨ë“œ")
        smart_weights = calculate_smart_weights(envsound_folder, target_samples_per_class=TARGET_SAMPLES_PER_CLASS)
        DANGER_WEIGHTS = {class_name: info['selection_ratio'] for class_name, info in smart_weights.items()}
    elif AUTO_WEIGHT_CALCULATION:
        print(f"\nğŸ”„ ê¸°ì¡´ ìë™ ê°€ì¤‘ì¹˜ ê³„ì‚° ëª¨ë“œ")
        DANGER_WEIGHTS = calculate_auto_weights(envsound_folder, target_samples_per_class=250)
    else:
        print(f"\nâš™ï¸ ìˆ˜ë™ ê°€ì¤‘ì¹˜ ì„¤ì • ëª¨ë“œ")
        DANGER_WEIGHTS = MANUAL_DANGER_WEIGHTS.copy()
        print("ì„¤ì •ëœ ê°€ì¤‘ì¹˜:")
        for class_name, weight in DANGER_WEIGHTS.items():
            print(f"  {class_name}: {weight}")
    
    print(f"\nìµœì¢… ì‚¬ìš©ë  ê°€ì¤‘ì¹˜: {DANGER_WEIGHTS}")
    
    # YAMNet ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œ
    print("YAMNet ëª¨ë¸ ë¡œë”© ì¤‘...")
    yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'
    yamnet_model = hub.load(yamnet_model_handle)
    print("YAMNet ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    X_data = []
    y_data = []
    data_info = []  # ê° ìƒ˜í”Œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥
    
    # 4) ë¬´ìŒ ë°ì´í„° ìƒì„± (ì¦ê°€)
    print(f"\në¬´ìŒ ë°ì´í„° ìƒì„± ì¤‘... (ì´ {SILENCE_SAMPLES}ê°œ ìƒ˜í”Œ)")
    
    for i in range(SILENCE_SAMPLES):
        try:
            if (i + 1) % 30 == 0:  # ì§„í–‰ë¥  í‘œì‹œ ê°„ê²© ì¡°ì •
                print(f"  ë¬´ìŒ ë°ì´í„° ì§„í–‰ë¥ : {i+1}/{SILENCE_SAMPLES}")
            
            # ì™„ì „ ë¬´ìŒê³¼ ì €ìŒëŸ‰ ë°°ê²½ ì†ŒìŒì„ ì„ì–´ì„œ ìƒì„±
            if i < SILENCE_SAMPLES // 2:
                audio = generate_silence(total_duration, sr)
                silence_type = "ì™„ì „ë¬´ìŒ"
            else:
                audio = generate_background_noise(total_duration, sr)
                silence_type = "ë°°ê²½ì†ŒìŒ"
            
            embeddings = extract_yamnet_embeddings(audio, sr, yamnet_model)
            labels = generate_labels(None, None, 0, total_duration=total_duration, frame_length=frame_length)  # ë¬´ìŒ í´ë˜ìŠ¤
            
            X_data.append(embeddings)
            y_data.append(labels)
            data_info.append({
                'class': 'silence',
                'class_id': 0,
                'type': silence_type,
                'factory_file': None,
                'event_file': None,
                'sample_index': i
            })
            
        except Exception as e:
            print(f"  ë¬´ìŒ ë°ì´í„° ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            continue
    
    # 5) ì •ìƒ ë°ì´í„° ìƒì„± (ê³µì¥ ì†Œë¦¬ë§Œ) - ì¦ê°€
    print(f"\nì •ìƒ ë°ì´í„° ìƒì„± ì¤‘... (ì´ {NORMAL_SAMPLES}ê°œ ìƒ˜í”Œ)")
    
    for i in range(NORMAL_SAMPLES):
        try:
            if (i + 1) % 30 == 0:  # ì§„í–‰ë¥  í‘œì‹œ ê°„ê²© ì¡°ì •
                print(f"  ì •ìƒ ë°ì´í„° ì§„í–‰ë¥ : {i+1}/{NORMAL_SAMPLES}")
                
            factory_path = random.choice(factory_paths)
            factory_audio, _ = librosa.load(factory_path, sr=sr)
            
            target_len = int(sr * total_duration)
            if len(factory_audio) > target_len:
                start_pos = random.randint(0, len(factory_audio) - target_len)
                factory_audio = factory_audio[start_pos:start_pos + target_len]
            else:
                factory_audio = np.pad(factory_audio, (0, max(0, target_len - len(factory_audio))))
            
            embeddings = extract_yamnet_embeddings(factory_audio, sr, yamnet_model)
            labels = generate_labels(None, None, 1, total_duration=total_duration, frame_length=frame_length)  # ì •ìƒ í´ë˜ìŠ¤
            
            X_data.append(embeddings)
            y_data.append(labels)
            data_info.append({
                'class': 'normal',
                'class_id': 1,
                'type': 'ê³µì¥ì†ŒìŒ',
                'factory_file': os.path.basename(factory_path),
                'event_file': None,
                'sample_index': i
            })
            
        except Exception as e:
            print(f"  ì •ìƒ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            continue

    # 6) ë¬´ìŒâ†’ê³µì¥ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒì„± (ìƒˆë¡œ ì¶”ê°€)
    print(f"\në¬´ìŒâ†’ê³µì¥ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒì„± ì¤‘... (ì´ {TRANSITION_SAMPLES}ê°œ ìƒ˜í”Œ)")
    
    for i in range(TRANSITION_SAMPLES):
        try:
            if (i + 1) % 20 == 0:  # ì§„í–‰ë¥  í‘œì‹œ ê°„ê²© ì¡°ì •
                print(f"  ì „í™˜ ë°ì´í„° ì§„í–‰ë¥ : {i+1}/{TRANSITION_SAMPLES}")
                
            factory_path = random.choice(factory_paths)
            factory_audio, _ = librosa.load(factory_path, sr=sr)
            
            # ë¬´ìŒâ†’ê³µì¥ì†Œë¦¬ ì „í™˜ ì˜¤ë””ì˜¤ ìƒì„±
            transition_audio, transition_start_sec = create_silence_to_factory_transition(
                factory_audio, sr, total_duration=total_duration
            )
            
            embeddings = extract_yamnet_embeddings(transition_audio, sr, yamnet_model)
            labels = generate_transition_labels(transition_start_sec, total_duration=total_duration, frame_length=frame_length)
            
            X_data.append(embeddings)
            y_data.append(labels)
            data_info.append({
                'class': 'transition',
                'class_id': 1,  # ìµœì¢…ì ìœ¼ë¡œ ì •ìƒ(ê³µì¥ì†Œë¦¬)ë¡œ ë¶„ë¥˜
                'type': 'ë¬´ìŒâ†’ê³µì¥ì†Œë¦¬ ì „í™˜',
                'factory_file': os.path.basename(factory_path),
                'event_file': None,
                'transition_start_sec': transition_start_sec,
                'sample_index': i
            })
            
        except Exception as e:
            print(f"  ì „í™˜ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            continue

    # 7) ë¬´ìŒâ†’ìœ„í—˜ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒì„± (ìƒˆë¡œ ì¶”ê°€)
    print(f"\në¬´ìŒâ†’ìœ„í—˜ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒì„± ì¤‘... (ì´ {DANGER_TRANSITION_SAMPLES}ê°œ ìƒ˜í”Œ)")
    
    # í´ë˜ìŠ¤ë³„ë¡œ ê· ë“±í•˜ê²Œ ë¶„ë°°
    samples_per_danger_class = DANGER_TRANSITION_SAMPLES // len(event_folders)
    remaining_samples = DANGER_TRANSITION_SAMPLES % len(event_folders)
    
    current_sample_count = 0
    
    for class_idx, class_name in enumerate(event_folders):
        if class_name not in event_data or len(event_data[class_name]) == 0:
            print(f"  âš ï¸ {class_name} í´ë˜ìŠ¤ íŒŒì¼ì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
            
        class_id = class_mapping[class_name]
        
        # ë§ˆì§€ë§‰ í´ë˜ìŠ¤ì— ë‚¨ì€ ìƒ˜í”Œ ì¶”ê°€
        class_samples = samples_per_danger_class
        if class_idx < remaining_samples:
            class_samples += 1
            
        print(f"  {class_name} í´ë˜ìŠ¤ ì „í™˜ ë°ì´í„°: {class_samples}ê°œ ìƒ˜í”Œ")
        
        for i in range(class_samples):
            try:
                current_sample_count += 1
                if current_sample_count % 10 == 0:
                    print(f"    ìœ„í—˜ ì „í™˜ ë°ì´í„° ì§„í–‰ë¥ : {current_sample_count}/{DANGER_TRANSITION_SAMPLES}")
                
                # ëœë¤í•˜ê²Œ ìœ„í—˜ì†Œë¦¬ íŒŒì¼ ì„ íƒ
                event_path = random.choice(event_data[class_name])
                event_audio, _ = librosa.load(event_path, sr=sr)
                event_audio_ns = remove_silence(event_audio, sr, top_db=20)
                
                # ë¬´ìŒâ†’ìœ„í—˜ì†Œë¦¬ ì „í™˜ ì˜¤ë””ì˜¤ ìƒì„±
                transition_audio, transition_start_sec, transition_end_sec = create_silence_to_danger_transition(
                    event_audio_ns, sr, class_id, total_duration=total_duration
                )
                
                embeddings = extract_yamnet_embeddings(transition_audio, sr, yamnet_model)
                labels = generate_danger_transition_labels(
                    transition_start_sec, transition_end_sec, class_id, 
                    total_duration=total_duration, frame_length=frame_length
                )
                
                X_data.append(embeddings)
                y_data.append(labels)
                data_info.append({
                    'class': f'danger_transition_{class_name}',
                    'class_id': class_id,
                    'type': f'ë¬´ìŒâ†’{class_name} ì „í™˜',
                    'factory_file': None,
                    'event_file': os.path.basename(event_path),
                    'transition_start_sec': transition_start_sec,
                    'transition_end_sec': transition_end_sec,
                    'sample_index': i
                })
                
            except Exception as e:
                print(f"    ìœ„í—˜ ì „í™˜ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
    
    # 8) ìœ„í—˜ ì†Œë¦¬ ë°ì´í„° ìƒì„± - ìŠ¤ë§ˆíŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì ìš©
    if ENABLE_SMART_SEGMENTATION:
        print(f"\nğŸ¯ ìŠ¤ë§ˆíŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ì‚¬ìš©í•œ ìœ„í—˜ ì†Œë¦¬ ë°ì´í„° ìƒì„±")
        print("=" * 60)
        
        for class_name, event_paths in event_data.items():
            class_id = class_mapping[class_name]
            selection_ratio = DANGER_WEIGHTS[class_name]
            
            print(f"\n{class_name.upper()} í´ë˜ìŠ¤ ë°ì´í„° ìƒì„± ì¤‘... (í´ë˜ìŠ¤ ID: {class_id})")
            print(f"ì„ íƒ ë¹„ìœ¨: {selection_ratio:.3f}")
            
            class_segments = []
            
            # ê° íŒŒì¼ì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
            for idx, event_path in enumerate(event_paths):
                try:
                    print(f"  ğŸ“„ [{idx+1}/{len(event_paths)}] {os.path.basename(event_path)} ë¶„ì„ ì¤‘...")
                    
                    # ì˜¤ë””ì˜¤ ë¡œë“œ
                    event_audio, _ = librosa.load(event_path, sr=sr)
                    
                    # ìŠ¤ë§ˆíŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ìœ¼ë¡œ ì˜ë¯¸ìˆëŠ” êµ¬ê°„ ì¶”ì¶œ
                    segments = extract_sound_segments(
                        event_audio, sr,
                        min_duration=MIN_SEGMENT_DURATION,
                        max_duration=MAX_SEGMENT_DURATION,
                        threshold=SILENCE_THRESHOLD,
                        min_sound_ratio=MIN_SOUND_RATIO
                    )
                    
                    print(f"    â†’ {len(segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œë¨")
                    
                    # ì¶”ì¶œëœ ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    for seg_idx, segment in enumerate(segments):
                        class_segments.append({
                            'audio': segment,
                            'file_path': event_path,
                            'file_name': os.path.basename(event_path),
                            'segment_index': seg_idx,
                            'duration': len(segment) / sr
                        })
                        
                except Exception as e:
                    print(f"    âŒ ì˜¤ë¥˜: {e}")
                    continue
            
            # ëª©í‘œ ìƒ˜í”Œ ìˆ˜ì— ë§ê²Œ ì„¸ê·¸ë¨¼íŠ¸ ì„ íƒ
            total_segments = len(class_segments)
            target_samples = int(total_segments * selection_ratio)
            
            print(f"\nğŸ“Š {class_name.upper()} ì„¸ê·¸ë¨¼íŠ¸ ì„ íƒ:")
            print(f"  â€¢ ì´ ì¶”ì¶œëœ ì„¸ê·¸ë¨¼íŠ¸: {total_segments}ê°œ")
            print(f"  â€¢ ì„ íƒí•  ìƒ˜í”Œ: {target_samples}ê°œ")
            
            if target_samples > 0 and total_segments > 0:
                # ëœë¤í•˜ê²Œ ì„¸ê·¸ë¨¼íŠ¸ ì„ íƒ
                selected_segments = random.sample(class_segments, min(target_samples, total_segments))
                
                for sample_idx, segment_info in enumerate(selected_segments):
                    try:
                        if (sample_idx + 1) % 20 == 0:
                            print(f"    ì²˜ë¦¬ ì§„í–‰ë¥ : {sample_idx+1}/{len(selected_segments)}")
                        
                        segment_audio = segment_info['audio']
                        
                        # ì„¸ê·¸ë¨¼íŠ¸ë¥¼ 10ì´ˆë¡œ ì¡°ì • (íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°)
                        target_length = int(total_duration * sr)
                        
                        if len(segment_audio) > target_length:
                            # ëœë¤ ì‹œì‘ì ì—ì„œ 10ì´ˆ ì¶”ì¶œ
                            start_idx = random.randint(0, len(segment_audio) - target_length)
                            segment_audio = segment_audio[start_idx:start_idx + target_length]
                        else:
                            # ì•ë’¤ë¡œ ë¬´ìŒ íŒ¨ë”© (ì¤‘ì•™ ì •ë ¬)
                            padding_total = target_length - len(segment_audio)
                            padding_start = padding_total // 2
                            padding_end = padding_total - padding_start
                            segment_audio = np.pad(segment_audio, (padding_start, padding_end), mode='constant')
                        
                        # ê³µì¥ ì†Œë¦¬ì™€ ë¯¹ì‹±
                        factory_path = random.choice(factory_paths)
                        factory_audio, _ = librosa.load(factory_path, sr=sr)
                        
                        mixed_audio, start_sec, end_sec = mix_factory_and_event(
                            factory_audio, segment_audio, sr, desired_length=total_duration
                        )
                        
                        # YAMNet ì„ë² ë”© ì¶”ì¶œ
                        embeddings = extract_yamnet_embeddings(mixed_audio, sr, yamnet_model)
                        labels = generate_labels(start_sec, end_sec, class_id, 
                                               total_duration=total_duration, frame_length=frame_length)
                        
                        X_data.append(embeddings)
                        y_data.append(labels)
                        data_info.append({
                            'class': class_name,
                            'class_id': class_id,
                            'type': 'ìŠ¤ë§ˆíŠ¸ì„¸ê·¸ë¨¼íŠ¸_ìœ„í—˜ì†ŒìŒ',
                            'factory_file': os.path.basename(factory_path),
                            'event_file': segment_info['file_name'],
                            'segment_index': segment_info['segment_index'],
                            'segment_duration': segment_info['duration'],
                            'event_start_sec': start_sec,
                            'event_end_sec': end_sec,
                            'sample_index': sample_idx
                        })
                        
                    except Exception as e:
                        print(f"    ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
                
                print(f"  âœ… {len(selected_segments)}ê°œ ìƒ˜í”Œ ìƒì„± ì™„ë£Œ")
            else:
                print(f"  âš ï¸ ìƒì„±í•  ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # 9) v1.3 ìƒˆë¡œ ì¶”ê°€: ê³µì¥ì†Œë¦¬â†’ë‹¤ë¥¸ê³µì¥ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒì„±
    print(f"\nê³µì¥ì†Œë¦¬â†’ë‹¤ë¥¸ê³µì¥ì†Œë¦¬ ì „í™˜ ë°ì´í„° ìƒì„± ì¤‘... (ì´ {FACTORY_TRANSITION_SAMPLES}ê°œ ìƒ˜í”Œ)")
    if len(factory_paths) >= 2:
        for i in range(FACTORY_TRANSITION_SAMPLES):
            try:
                # ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ê³µì¥ì†Œë¦¬ íŒŒì¼ ì„ íƒ
                factory_files = random.sample(factory_paths, 2)
                factory_audio1, _ = librosa.load(factory_files[0], sr=sr)
                factory_audio2, _ = librosa.load(factory_files[1], sr=sr)
                
                # ì „í™˜ ì˜¤ë””ì˜¤ ìƒì„±
                transition_audio, transition_start_sec = create_factory_to_factory_transition(
                    factory_audio1, factory_audio2, sr, total_duration
                )
                
                # YAMNet ì„ë² ë”© ì¶”ì¶œ
                embeddings = extract_yamnet_embeddings(transition_audio, sr, yamnet_model)
                labels = generate_factory_transition_labels(transition_start_sec, total_duration, frame_length)
                
                X_data.append(embeddings)
                y_data.append(labels)
                data_info.append({
                    'class': 'factory_transition',
                    'class_id': 1,  # ì •ìƒ ê³µì¥ì†Œë¦¬
                    'type': 'ê³µì¥ì†Œë¦¬ì „í™˜',
                    'factory_file1': os.path.basename(factory_files[0]),
                    'factory_file2': os.path.basename(factory_files[1]),
                    'transition_start_sec': transition_start_sec,
                    'sample_index': i
                })
                
                if (i + 1) % 10 == 0:
                    print(f"    ê³µì¥ ì „í™˜ ë°ì´í„° ì§„í–‰ë¥ : {i+1}/{FACTORY_TRANSITION_SAMPLES}")
                    
            except Exception as e:
                print(f"    ê³µì¥ ì „í™˜ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
    else:
        print("    ê²½ê³ : ê³µì¥ì†Œë¦¬ íŒŒì¼ì´ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. ê³µì¥ì†Œë¦¬ ì „í™˜ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # 10) v1.3 ìƒˆë¡œ ì¶”ê°€: ë¬´ìŒ ë³€í™” ë°ì´í„° ìƒì„±
    print(f"\në¬´ìŒ ë³€í™” ë°ì´í„° ìƒì„± ì¤‘... (ì´ {SILENCE_VARIATION_SAMPLES}ê°œ ìƒ˜í”Œ)")
    for i in range(SILENCE_VARIATION_SAMPLES):
        try:
            # ë‹¤ì–‘í•œ ë¬´ìŒ/ë°°ê²½ì†ŒìŒ ë³€í™” ì˜¤ë””ì˜¤ ìƒì„±
            variation_audio, transition_start_sec = create_silence_variation_transition(sr, total_duration)
            
            # YAMNet ì„ë² ë”© ì¶”ì¶œ
            embeddings = extract_yamnet_embeddings(variation_audio, sr, yamnet_model)
            labels = generate_silence_variation_labels(transition_start_sec, total_duration, frame_length)
            
            X_data.append(embeddings)
            y_data.append(labels)
            data_info.append({
                'class': 'silence_variation',
                'class_id': 0,  # ë¬´ìŒ
                'type': 'ë¬´ìŒë³€í™”',
                'transition_start_sec': transition_start_sec,
                'sample_index': i
            })
            
            if (i + 1) % 10 == 0:
                print(f"    ë¬´ìŒ ë³€í™” ë°ì´í„° ì§„í–‰ë¥ : {i+1}/{SILENCE_VARIATION_SAMPLES}")
                
        except Exception as e:
            print(f"    ë¬´ìŒ ë³€í™” ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            continue

    # 11) ë°ì´í„° ë°°ì—´í™” ë° ì „ì²˜ë¦¬
    print(f"\nì´ ë°ì´í„° ìƒ˜í”Œ ìˆ˜: {len(X_data)}")
    
    if len(X_data) == 0:
        print("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    try:
        # ë™ì¼í•œ ê¸¸ì´ë¡œ ë§ì¶”ê¸° (íŒ¨ë”©)
        max_len = max([x.shape[0] for x in X_data])
        print(f"ìµœëŒ€ í”„ë ˆì„ ê¸¸ì´: {max_len}")
        
        X_data_padded = []
        y_data_padded = []
        
        for i in range(len(X_data)):
            x = X_data[i]
            y = y_data[i]
            
            # ë°ì´í„° í˜•íƒœ ê²€ì¦
            if len(x.shape) != 2 or x.shape[1] != 1024:
                print(f"ê²½ê³ : ìƒ˜í”Œ {i}ì˜ X ë°ì´í„° í˜•íƒœê°€ ì´ìƒí•©ë‹ˆë‹¤: {x.shape}")
                continue
                
            if len(y) != x.shape[0]:
                print(f"ê²½ê³ : ìƒ˜í”Œ {i}ì˜ Xì™€ y ê¸¸ì´ê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤: X={x.shape[0]}, y={len(y)}")
                if len(y) < x.shape[0]:
                    y = np.pad(y, (0, x.shape[0] - len(y)), mode='constant')
                else:
                    y = y[:x.shape[0]]
            
            if x.shape[0] < max_len:
                x_padded = np.pad(x, ((0, max_len - x.shape[0]), (0, 0)), mode='constant')
                y_padded = np.pad(y, (0, max_len - len(y)), mode='constant')
            else:
                x_padded = x[:max_len]
                y_padded = y[:max_len]
            
            X_data_padded.append(x_padded)
            y_data_padded.append(y_padded)
        
        print(f"íŒ¨ë”© í›„ ìœ íš¨í•œ ìƒ˜í”Œ ìˆ˜: {len(X_data_padded)}")
        
        X_data = np.array(X_data_padded, dtype=np.float32)
        y_data = np.array(y_data_padded, dtype=np.int32)
        
        print(f"ë°ì´í„° í˜•íƒœ - X: {X_data.shape}, y: {y_data.shape}")
        
        # One-hot ì¸ì½”ë”© (5-í´ë˜ìŠ¤)
        y_data_oh = to_categorical(y_data, num_classes=num_classes)
        print(f"One-hot ì¸ì½”ë”© í›„ y í˜•íƒœ: {y_data_oh.shape}")
        
        # í´ë˜ìŠ¤ë³„ ë°ì´í„° ë¶„í¬ í™•ì¸
        unique, counts = np.unique(y_data, return_counts=True)
        print("í´ë˜ìŠ¤ë³„ í”„ë ˆì„ ìˆ˜:")
        class_names = ['ë¬´ìŒ', 'ì •ìƒ(ê³µì¥)', 'í™”ì¬', 'ê°€ìŠ¤ëˆ„ì¶œ', 'ë¹„ëª…']
        for cls, count in zip(unique, counts):
            if cls < len(class_names):
                print(f"  {class_names[cls]}: {count}ê°œ í”„ë ˆì„")
        
        # í´ë˜ìŠ¤ë³„ ë¹„ìœ¨ í™•ì¸
        total_frames = np.sum(counts)
        print("\ní´ë˜ìŠ¤ë³„ ë¹„ìœ¨:")
        for cls, count in zip(unique, counts):
            if cls < len(class_names):
                ratio = count / total_frames * 100
                print(f"  {class_names[cls]}: {ratio:.1f}%")
        
        # 10) í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬ (80% / 10% / 10%)
        X_temp, X_test, y_temp, y_test, info_temp, info_test = train_test_split(
            X_data, y_data_oh, data_info, test_size=0.1, random_state=42, stratify=[info['class_id'] for info in data_info]
        )
        
        X_train, X_val, y_train, y_val, info_train, info_val = train_test_split(
            X_temp, y_temp, info_temp, test_size=0.111, random_state=42, stratify=[info['class_id'] for info in info_temp]  # 0.111 * 0.9 â‰ˆ 0.1 (ì „ì²´ì˜ 10%)
        )
        
        print(f"\ní›ˆë ¨ ë°ì´í„°: {X_train.shape}, ë ˆì´ë¸”: {y_train.shape}")
        print(f"ê²€ì¦ ë°ì´í„°: {X_val.shape}, ë ˆì´ë¸”: {y_val.shape}")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}, ë ˆì´ë¸”: {y_test.shape}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë³„ë„ í´ë”ì— ì €ì¥
        test_folder = f"test_{VERSION}"
        if not os.path.exists(test_folder):
            os.makedirs(test_folder)
            print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ë°ì´í„° í´ë” ìƒì„±: {test_folder}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥
        test_data_path = os.path.join(test_folder, f"test_data_{VERSION}.npz")
        np.savez_compressed(test_data_path, 
                          X_test=X_test, 
                          y_test=y_test, 
                          y_test_labels=np.argmax(y_test, axis=2))  # ì›í•« -> ë¼ë²¨ ë³€í™˜
        print(f"ğŸ’¾ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥: {test_data_path}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë³´ ì €ì¥
        test_info_path = os.path.join(test_folder, f"test_info_{VERSION}.json")
        with open(test_info_path, 'w', encoding='utf-8') as f:
            json.dump(info_test, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ í…ŒìŠ¤íŠ¸ ì •ë³´ ì €ì¥: {test_info_path}")
        
        # ë°ì´í„°ì…‹ ì •ë³´ ì €ì¥
        dataset_info = {
            'train': info_train,
            'validation': info_val,
            'test': info_test
        }
        
        dataset_file_path = os.path.join(output_folder, f'dataset_info_{VERSION}.json')
        with open(dataset_file_path, 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, ensure_ascii=False, indent=2)
        print(f"ë°ì´í„°ì…‹ ì •ë³´ ì €ì¥ ì™„ë£Œ: {dataset_file_path}")
        
        # ê° ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ë³„ ë¶„í¬ í™•ì¸
        def print_dataset_distribution(y_data, dataset_name, class_names):
            y_labels = np.argmax(y_data, axis=2).flatten()  # one-hotì„ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
            unique, counts = np.unique(y_labels, return_counts=True)
            total = np.sum(counts)
            print(f"\n{dataset_name} ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ë¶„í¬:")
            for cls, count in zip(unique, counts):
                if cls < len(class_names):
                    ratio = count / total * 100
                    print(f"  {class_names[cls]}: {count:,}ê°œ í”„ë ˆì„ ({ratio:.1f}%)")
        
        print_dataset_distribution(y_train, "í›ˆë ¨", class_names)
        print_dataset_distribution(y_val, "ê²€ì¦", class_names)
        print_dataset_distribution(y_test, "í…ŒìŠ¤íŠ¸", class_names)
        
        # 11) LSTM ëª¨ë¸ ìƒì„±
        input_shape = X_train.shape[1:]
        model = create_lstm_model(input_shape, num_classes)
        model.summary()
        
        # ì½œë°± ì„¤ì •
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=5,
                restore_best_weights=True
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=3,
                min_lr=0.00001
            )
        ]
        
        # 12) í•™ìŠµ
        print(f"\nëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        history = model.fit(
            X_train, y_train, 
            epochs=20,
            batch_size=8, 
            validation_data=(X_val, y_val),
            callbacks=callbacks,
            verbose=1
        )
        
        # 13) ëª¨ë¸ í‰ê°€
        print("\n=== ê²€ì¦ ë°ì´í„° í‰ê°€ ===")
        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)
        print(f"ê²€ì¦ ì†ì‹¤: {val_loss:.4f}")
        print(f"ê²€ì¦ ì •í™•ë„: {val_acc:.4f}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€
        print("\n=== í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ ===")
        test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
        print(f"í…ŒìŠ¤íŠ¸ ì†ì‹¤: {test_loss:.4f}")
        print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}")
        
        # ìƒì„¸í•œ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ë¶„ì„
        y_test_pred = model.predict(X_test, verbose=0)
        y_test_pred_classes = np.argmax(y_test_pred, axis=2)
        y_test_true_classes = np.argmax(y_test, axis=2)
        
        # í”„ë ˆì„ ë‹¨ìœ„ í‰ê°€ë¥¼ ìœ„í•´ flatten
        y_test_pred_flat = y_test_pred_classes.flatten()
        y_test_true_flat = y_test_true_classes.flatten()
        
        print("\n=== ìƒì„¸ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ (í”„ë ˆì„ ë‹¨ìœ„) ===")
        print("ë¶„ë¥˜ ë³´ê³ ì„œ:")
        print(classification_report(y_test_true_flat, y_test_pred_flat, 
                                  target_names=class_names, 
                                  zero_division=0))
        
        print("\ní˜¼ë™ í–‰ë ¬:")
        cm = confusion_matrix(y_test_true_flat, y_test_pred_flat)
        print("ì‹¤ì œ\\ì˜ˆì¸¡", end="")
        for name in class_names:
            print(f"\t{name[:6]}", end="")
        print()
        for i, name in enumerate(class_names):
            print(f"{name[:8]}", end="")
            for j in range(len(class_names)):
                print(f"\t{cm[i][j]}", end="")
            print()
        
        # 14) ëª¨ë¸ ì €ì¥
        model_file_path = os.path.join(output_folder, f'yamnet_lstm_model_{VERSION}.h5')
        model.save(model_file_path)
        print(f"\n5í´ë˜ìŠ¤ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_file_path}")
        
        # í´ë˜ìŠ¤ ë§¤í•‘ ì •ë³´ ì €ì¥
        model_info = {
            'class_mapping': {**class_mapping, 'silence': 0, 'normal': 1},
            'class_names': class_names,
            'num_classes': num_classes,
            'version': VERSION,
            'model_file': f'yamnet_lstm_model_{VERSION}.h5'
        }
        
        # pickleì„ ì‚¬ìš©í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ ì €ì¥
        model_info_path = os.path.join(output_folder, f'model_info_{VERSION}.pkl')
        with open(model_info_path, 'wb') as f:
            pickle.dump(model_info, f)
        print(f"ëª¨ë¸ ì •ë³´ ì €ì¥ ì™„ë£Œ: {model_info_path}")
        
        # 15) ëª¨ë¸ ì„±ëŠ¥ ë° ì •ë³´ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
        from datetime import datetime
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        performance_log = f"""
========================================
5í´ë˜ìŠ¤ LSTM ëª¨ë¸ í•™ìŠµ ë³´ê³ ì„œ ({VERSION})
========================================
í•™ìŠµ ì™„ë£Œ ì‹œê°„: {current_time}
ëª¨ë¸ ë²„ì „: {VERSION}
ê²°ê³¼ë¬¼ í´ë”: {output_folder}

=== ëª¨ë¸ êµ¬ì„± ===
- ëª¨ë¸ íƒ€ì…: YAMNet + LSTM
- í´ë˜ìŠ¤ ìˆ˜: {num_classes}ê°œ
- ì…ë ¥ í˜•íƒœ: {input_shape}
- ìŒì„± ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜: {sr} Hz
- í”„ë ˆì„ ê¸¸ì´: {frame_length}ì´ˆ
- ì´ ì˜¤ë””ì˜¤ ê¸¸ì´: {total_duration}ì´ˆ

=== í´ë˜ìŠ¤ ì •ë³´ ===
í´ë˜ìŠ¤ ë§¤í•‘:
  - 0: ë¬´ìŒ (silence)
  - 1: ì •ìƒ (ê³µì¥ì†ŒìŒ)
  - 2: í™”ì¬ (fire)
  - 3: ê°€ìŠ¤ëˆ„ì¶œ (gas)
  - 4: ë¹„ëª… (scream)

=== ë°ì´í„° ìƒì„± ì„¤ì • ===
ë¬´ìŒ ë°ì´í„°: {SILENCE_SAMPLES}ê°œ ìƒ˜í”Œ
ì •ìƒ ë°ì´í„°: {NORMAL_SAMPLES}ê°œ ìƒ˜í”Œ
ì „í™˜ ë°ì´í„°: {TRANSITION_SAMPLES}ê°œ ìƒ˜í”Œ (ë¬´ìŒâ†’ê³µì¥ì†Œë¦¬)
ìœ„í—˜ ì „í™˜ ë°ì´í„°: {DANGER_TRANSITION_SAMPLES}ê°œ ìƒ˜í”Œ (ë¬´ìŒâ†’ìœ„í—˜ì†Œë¦¬)
ê°€ì¤‘ì¹˜ ê³„ì‚° ëª¨ë“œ: {'ìë™' if AUTO_WEIGHT_CALCULATION else 'ìˆ˜ë™'}
ìœ„í—˜ ì†ŒìŒ ê°€ì¤‘ì¹˜:
  - í™”ì¬: {DANGER_WEIGHTS.get('fire', 0)} (íŒŒì¼ë‹¹ ìƒ˜í”Œ ìˆ˜)
  - ê°€ìŠ¤ëˆ„ì¶œ: {DANGER_WEIGHTS.get('gas', 0)} (íŒŒì¼ë‹¹ ìƒ˜í”Œ ìˆ˜)
  - ë¹„ëª…: {DANGER_WEIGHTS.get('scream', 0)} (íŒŒì¼ë‹¹ ìƒ˜í”Œ ìˆ˜)

=== ë°ì´í„° ë¶„í¬ ===
ì´ ìƒ˜í”Œ ìˆ˜: {len(X_data)}ê°œ
í›ˆë ¨ ë°ì´í„°: {X_train.shape[0]}ê°œ ({X_train.shape[0]/len(X_data)*100:.1f}%)
ê²€ì¦ ë°ì´í„°: {X_val.shape[0]}ê°œ ({X_val.shape[0]/len(X_data)*100:.1f}%)
í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]}ê°œ ({X_test.shape[0]/len(X_data)*100:.1f}%)

í´ë˜ìŠ¤ë³„ í”„ë ˆì„ ìˆ˜:
"""
        # í´ë˜ìŠ¤ë³„ ë¶„í¬ ì •ë³´ ì¶”ê°€
        unique, counts = np.unique(y_data, return_counts=True)
        total_frames = np.sum(counts)
        for cls, count in zip(unique, counts):
            if cls < len(class_names):
                ratio = count / total_frames * 100
                performance_log += f"  - {class_names[cls]}: {count:,}ê°œ í”„ë ˆì„ ({ratio:.1f}%)\n"
        
        performance_log += f"""
=== ëª¨ë¸ ì„±ëŠ¥ ===
ê²€ì¦ ì†ì‹¤: {val_loss:.6f}
ê²€ì¦ ì •í™•ë„: {val_acc:.6f} ({val_acc*100:.2f}%)

í…ŒìŠ¤íŠ¸ ì†ì‹¤: {test_loss:.6f}
í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.6f} ({test_acc*100:.2f}%)

=== ìƒì„¸ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ë¶„ì„ ===
{classification_report(y_test_true_flat, y_test_pred_flat, target_names=class_names, zero_division=0)}

=== ë°ì´í„°ì…‹ íŒŒì¼ ì •ë³´ ===
- dataset_info_{VERSION}.json: í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ì‚¬ìš©ëœ ì˜¤ë””ì˜¤ íŒŒì¼ ì •ë³´

=== í•™ìŠµ ì„¤ì • ===
ì—í¬í¬ ìˆ˜: 20
ë°°ì¹˜ í¬ê¸°: 8
ìµœì í™” ì•Œê³ ë¦¬ì¦˜: Adam (learning_rate=0.001)
ì½œë°±:
  - EarlyStopping (patience=5)
  - ReduceLROnPlateau (patience=3, factor=0.5)

=== ì €ì¥ëœ íŒŒì¼ ===
- ëª¨ë¸ íŒŒì¼: yamnet_lstm_model_{VERSION}.h5
- ëª¨ë¸ ì •ë³´: model_info_{VERSION}.pkl
- ì„±ëŠ¥ ë³´ê³ ì„œ: model_performance_{VERSION}.txt
- ë°ì´í„°ì…‹ ì •ë³´: dataset_info_{VERSION}.json

========================================
"""
        
        # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
        performance_report_path = os.path.join(output_folder, f'model_performance_{VERSION}.txt')
        with open(performance_report_path, 'w', encoding='utf-8') as f:
            f.write(performance_log)
        
        print(f"ëª¨ë¸ ì„±ëŠ¥ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {performance_report_path}")
        
        # ìš”ì•½ ì •ë³´ íŒŒì¼ ìƒì„±
        summary_info = {
            'version': VERSION,
            'timestamp': current_time,
            'folder': output_folder,
            'files': {
                'model': f'yamnet_lstm_model_{VERSION}.h5',
                'model_info': f'model_info_{VERSION}.pkl',
                'performance_report': f'model_performance_{VERSION}.txt',
                'dataset_info': f'dataset_info_{VERSION}.json'
            },
            'performance': {
                'validation_accuracy': float(val_acc),
                'validation_loss': float(val_loss),
                'test_accuracy': float(test_acc),
                'test_loss': float(test_loss)
            },
            'data_summary': {
                'total_samples': len(X_data),
                'train_samples': X_train.shape[0],
                'validation_samples': X_val.shape[0],
                'test_samples': X_test.shape[0],
                'silence_samples': SILENCE_SAMPLES,
                'normal_samples': NORMAL_SAMPLES,
                'transition_samples': TRANSITION_SAMPLES,
                'danger_transition_samples': DANGER_TRANSITION_SAMPLES,
                'auto_weight_calculation': AUTO_WEIGHT_CALCULATION,
                'danger_weights': DANGER_WEIGHTS
            }
        }
        
        summary_path = os.path.join(output_folder, f'summary_{VERSION}.json')
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary_info, f, ensure_ascii=False, indent=2)
        
        print(f"ìš”ì•½ ì •ë³´ ì €ì¥ ì™„ë£Œ: {summary_path}")
        
        # ê²°ê³¼ë¬¼ í´ë” ì •ë³´ ì¶œë ¥
        print(f"\nğŸ‰ ëª¨ë“  ê²°ê³¼ë¬¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“ ê²°ê³¼ë¬¼ í´ë”: {output_folder}")
        print(f"ğŸ“‹ í¬í•¨ëœ íŒŒì¼:")
        print(f"  - yamnet_lstm_model_{VERSION}.h5 (í•™ìŠµëœ ëª¨ë¸)")
        print(f"  - model_info_{VERSION}.pkl (ëª¨ë¸ ì •ë³´)")
        print(f"  - model_performance_{VERSION}.txt (ìƒì„¸ ì„±ëŠ¥ ë³´ê³ ì„œ)")
        print(f"  - dataset_info_{VERSION}.json (ë°ì´í„°ì…‹ ì •ë³´)")
        print(f"  - summary_{VERSION}.json (ìš”ì•½ ì •ë³´)")
        
    except Exception as e:
        print(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
