"""
YAMNet + LSTM í›ˆë ¨ ì„¤ì • íŒŒì¼
============================

ëª¨ë“  í›ˆë ¨ ê´€ë ¨ ì„¤ì •ì„ ì´ íŒŒì¼ì—ì„œ ê´€ë¦¬í•©ë‹ˆë‹¤.
ê° ë‹¨ê³„ë³„ë¡œ í•„ìš”í•œ ì„¤ì •ë“¤ì„ ì •ì˜í•˜ê³ , ë°ì´í„° ìƒì„±ë¶€í„° ëª¨ë¸ í›ˆë ¨, í‰ê°€ê¹Œì§€ì˜ 
ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì¤‘ì•™ì—ì„œ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import os
from datetime import datetime
import json
import numpy as np

# ================================
# ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
# ================================
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
ENVSOUND_DIR = os.path.join(BASE_DIR, 'envsound')
MIXTURE_DIR = os.path.join(BASE_DIR, 'mixture')

# í˜¸í™˜ì„±ì„ ìœ„í•œ AUDIO_DATA_DIR (evaluation.pyì—ì„œ ì‚¬ìš©)
AUDIO_DATA_DIR = ENVSOUND_DIR

# ================================
# í‰ê°€ìš© ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì„¤ì •
# ================================
EVALUATION_AUDIO_PATHS = {
    'silence': None,  # ë¬´ìŒì€ ìŠ¤í‚µ
    'factory': MIXTURE_DIR,  # ê³µì¥ì†Œë¦¬ëŠ” mixture í´ë”ì—ì„œ
    'fire': os.path.join(ENVSOUND_DIR, 'fire'),
    'gas': os.path.join(ENVSOUND_DIR, 'gas'), 
    'scream': os.path.join(ENVSOUND_DIR, 'scream'),
    'spark': os.path.join(ENVSOUND_DIR, 'spark'),
}

# ê²°ê³¼ ì €ì¥ ê²½ë¡œ - ë²„ì „ë³„ ì²´ê³„ì  ê´€ë¦¬
RESULTS_BASE_DIR = os.path.join(BASE_DIR, 'results')

# ë²„ì „ë³„ í´ë” êµ¬ì¡°
def get_version_dir():
    """í˜„ì¬ ëª¨ë¸ ë²„ì „ì— ë”°ë¥¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ"""
    try:
        version = MODEL_CONFIG['version']
        return os.path.join(RESULTS_BASE_DIR, f"version_{version}")
    except NameError:
        # MODEL_CONFIGê°€ ì•„ì§ ì •ì˜ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        return os.path.join(RESULTS_BASE_DIR, 'version_default')

def get_experiment_dir():
    """ì‹¤í—˜ë³„ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ë²„ì „ + íƒ€ì„ìŠ¤íƒ¬í”„)"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_name = f"exp_{timestamp}"
    return os.path.join(get_version_dir(), experiment_name)

# ëª¨ë“  ì„¤ì •ì´ ì •ì˜ëœ í›„ ë™ì  ê²½ë¡œ ì´ˆê¸°í™”
def initialize_paths():
    """ëª¨ë“  ì„¤ì •ì´ ì •ì˜ëœ í›„ ê²½ë¡œë“¤ì„ ì´ˆê¸°í™”"""
    global VERSION_DIR, TRAINING_RESULTS_DIR, EVALUATION_RESULTS_DIR
    global MODEL_SAVE_DIR, REPORT_SAVE_DIR, DATASET_SAVE_DIR
    
    VERSION_DIR = get_version_dir()
    TRAINING_RESULTS_DIR = os.path.join(VERSION_DIR, 'training')
    EVALUATION_RESULTS_DIR = os.path.join(VERSION_DIR, 'evaluation')
    MODEL_SAVE_DIR = os.path.join(VERSION_DIR, 'models')
    REPORT_SAVE_DIR = os.path.join(VERSION_DIR, 'reports')
    DATASET_SAVE_DIR = os.path.join(VERSION_DIR, 'datasets')

# ================================
# ëª¨ë¸ ë° í›ˆë ¨ ì„¤ì •
# ================================
MODEL_CONFIG = {
    'version': 'v2.26',                    # ëª¨ë¸ ë²„ì „
    'audio_duration': 5.0,                # ì˜¤ë””ì˜¤ ì…ë ¥ ê¸¸ì´ (ì´ˆ) [5.0 ~ 10.0]
    'sample_rate': 16000,                 # ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜
}

TRAINING_CONFIG = {
    'epochs': 50,                         # í›ˆë ¨ ì—í¬í¬ ìˆ˜
    'batch_size': 8,                     # ë°°ì¹˜ í¬ê¸°
    'learning_rate': 0.001,               # í•™ìŠµë¥ 
    'validation_split': 0.2,              # ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (ë ˆê±°ì‹œ - train/valë§Œ ì‚¬ìš©ì‹œ)
    'random_seed': 42,                    # ëœë¤ ì‹œë“œ
    'normalize_input': True,              # ì…ë ¥ ì •ê·œí™”
    'use_class_weights': True,            # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì‚¬ìš©
    
    # 3-way ë°ì´í„° ë¶„í•  ì„¤ì • (train/validation/test)
    'data_split': {
        'train_ratio': 0.7,               # í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨ (70%)
        'validation_ratio': 0.15,         # ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (15%)  
        'test_ratio': 0.15,               # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ (15%)
        'stratify': True,                 # í´ë˜ìŠ¤ë³„ ê· ë“± ë¶„í• 
        'shuffle': True,                  # ë°ì´í„° ì…”í”Œ
    },
    'dense_units': 256,                   # Dense ë ˆì´ì–´ ìœ ë‹› ìˆ˜
    'lstm_units': 128,                    # LSTM ë ˆì´ì–´ ìœ ë‹› ìˆ˜
    'dropout_rate': 0.6,                  # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
    'save_checkpoints': True,             # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    'early_stopping': {
        'enabled': True,
        'patience': 5
    },
    'learning_rate_schedule': {
        'enabled': True,
        'factor': 0.5,
        'patience': 5,
        'min_lr': 1e-7
    }
}

# ================================
# ì‚¬ìš©í•  ìœ„í—˜ ì†Œë¦¬ í´ë˜ìŠ¤ ì„¤ì •
# ================================
DANGER_CLASSES = {
    'fire': True,        # í™”ì¬ ì†Œë¦¬ ì‚¬ìš© ì—¬ë¶€
    'gas': True,         # ê°€ìŠ¤ëˆ„ì¶œ ì†Œë¦¬ ì‚¬ìš© ì—¬ë¶€
    'scream': True,      # ë¹„ëª… ì†Œë¦¬ ì‚¬ìš© ì—¬ë¶€
}

# í´ë˜ìŠ¤ ì •ë³´ ìë™ ìƒì„±
BASE_CLASSES = ['silence', 'factory']
ACTIVE_DANGER_CLASSES = [name for name, enabled in DANGER_CLASSES.items() if enabled]
ALL_CLASSES = BASE_CLASSES + ACTIVE_DANGER_CLASSES
NUM_CLASSES = len(ALL_CLASSES)

CLASS_NAMES = {
    'silence': 'ë¬´ìŒ',
    'factory': 'ì •ìƒ(ê³µì¥)',
    'fire': 'í™”ì¬',
    'gas': 'ê°€ìŠ¤ëˆ„ì¶œ',
    'scream': 'ë¹„ëª…'
}

# ì˜ì–´ í´ë˜ìŠ¤ ì´ë¦„ (ì°¨íŠ¸ ë° ê·¸ë˜í”„ìš©)
CLASS_NAMES_EN = {
    'silence': 'Silence',
    'factory': 'Factory',
    'fire': 'Fire',
    'gas': 'Gas Leak',
    'scream': 'Scream'
}

# ================================
# ë°ì´í„° ì¦ê°• ì„¤ì •
# ================================
AUGMENTATION_CONFIG = {
    'silence': {
        'enabled': True,
        'methods': ['noise_variation', 'volume_change'],
        'noise_types': ['white', 'pink', 'brown'],        # ë…¸ì´ì¦ˆ ì¢…ë¥˜
        'volume_range': (0.1, 0.8),                       # ë³¼ë¥¨ ë²”ìœ„
    },
    'factory': {
        'enabled': True,
        'methods': ['volume_change','noise_add'],
        'volume_range': (0.7, 1.3),                       # ë³¼ë¥¨ ë²”ìœ„
        'reverb_decay': (0.1, 0.5),                       # ë¦¬ë²„ë¸Œ ê°ì‡  ì‹œê°„
        'room_size': (0.1, 0.9),                           # ë£¸ í¬ê¸°
        'speed_range': (0.9, 1.1),                        # ì†ë„ ë³€í™” ë²”ìœ„
        'noise_level': (0.01, 0.05),  
    },
    'fire': {
        'enabled': True,
        'methods': ['factory_mix', 'volume_change','noise_add'],
        'snr_range': (10, 30),                             # SNR ë²”ìœ„ (dB)
        'volume_range': (0.8, 1.2),                       # ë³¼ë¥¨ ë²”ìœ„
        'noise_level': (0.01, 0.05),                      # ë…¸ì´ì¦ˆ ë ˆë²¨
    },
    'gas': {
        'enabled': True,
        'methods': ['factory_mix', 'volume_change','noise_add'],
        'snr_range': (10, 25),                             # SNR ë²”ìœ„ (dB)
        'volume_range': (0.8, 1.2),                       # ë³¼ë¥¨ ë²”ìœ„
        'noise_level': (0.01, 0.05),                      # ë…¸ì´ì¦ˆ ë ˆë²¨
    },
    'scream': {
        'enabled': True,
        'methods': ['factory_mix', 'volume_change', 'reverb','noise_add'],
        'snr_range': (10, 30),                            # SNR ë²”ìœ„ (dB)
        'volume_range': (0.7, 1.3),                       # ë³¼ë¥¨ ë²”ìœ„
        'room_size': (0.1, 0.9),                          # ë£¸ í¬ê¸°
        'reverb_decay': (0.1, 0.3),                       # ë¦¬ë²„ë¸Œ ê°ì‡  ì‹œê°„
        'noise_level': (0.01, 0.05), 
    }
}

# ================================
# ì „í™˜ ë°ì´í„° ì„¤ì •
# ================================
TRANSITION_CONFIG = {
    'enabled': True,
    'fade_duration': 0.2,                                 # í˜ì´ë“œ ì „í™˜ ì‹œê°„ (ì´ˆ)
    
    # ì „í™˜ íƒ€ì…ë³„ ì„¤ì •
    'types': {
        'silence_to_silence': {
            'enabled': True,
            'description': 'ë¬´ìŒ â†’ ë‹¤ë¥¸ ë¬´ìŒ (ë°°ê²½ë…¸ì´ì¦ˆ ë³€í™”)',
            'transition_point_range': (0.2, 0.8),         # ì „í™˜ ì‹œì  ë²”ìœ„ (ë¹„ìœ¨)
            'weight': 1.0,                                 # ìƒì„± ê°€ì¤‘ì¹˜
        },
        'silence_to_factory': {
            'enabled': True,
            'description': 'ë¬´ìŒ â†’ ê³µì¥ì†Œë¦¬ (ê¸°ê³„ ì‹œì‘)',
            'transition_point_range': (0.2, 0.7),         # ì „í™˜ ì‹œì  ë²”ìœ„
            'weight': 1.5,                                 # ìƒì„± ê°€ì¤‘ì¹˜
        },
        'silence_to_danger': {
            'enabled': True,
            'description': 'ë¬´ìŒ â†’ ìœ„í—˜ì†Œë¦¬ (ê¸´ê¸‰ìƒí™©)',
            'transition_point_range': (0.1, 0.6),         # ì „í™˜ ì‹œì  ë²”ìœ„
            'weight': 2.0,                                 # ìƒì„± ê°€ì¤‘ì¹˜
        },
        'factory_to_factory': {
            'enabled': True,
            'description': 'ê³µì¥ì†Œë¦¬ â†’ ë‹¤ë¥¸ ê³µì¥ì†Œë¦¬ (ê¸°ê³„ ë³€ê²½)',
            'transition_point_range': (0.3, 0.7),         # ì „í™˜ ì‹œì  ë²”ìœ„
            'weight': 1.0,                                 # ìƒì„± ê°€ì¤‘ì¹˜
        },
        'factory_to_danger': {
            'enabled': True,
            'description': 'ê³µì¥ì†Œë¦¬ â†’ ìœ„í—˜ì†Œë¦¬ (ì‚¬ê³  ë°œìƒ)',
            'transition_point_range': (0.2, 0.8),         # ì „í™˜ ì‹œì  ë²”ìœ„
            'danger_volume_ratio': (0.8, 1.5),            # ìœ„í—˜ì†Œë¦¬ ë³¼ë¥¨ ë¹„ìœ¨
            'weight': 1.0,                                 # ìƒì„± ê°€ì¤‘ì¹˜ (ê°€ì¥ ì¤‘ìš”)
        }
    }
}

# ================================
# ë°ì´í„° ìƒì„± ëª©í‘œ ì„¤ì •
# ================================
DATA_GENERATION_CONFIG = {
    'target_frames_per_class': 1000,                      # í´ë˜ìŠ¤ë‹¹ ëª©í‘œ í”„ë ˆì„ ìˆ˜ (ìˆ˜ì •: 2000 â†’ 1000)
    'min_frames_for_augmentation': 500,                   # ë°ì´í„° ì¦ê°• ì‹œì‘ ì„ê³„ê°’
    'transition_data_ratio': 0.2,                         # ì „í™˜ ë°ì´í„° ë¹„ìœ¨ (ì „ì²´ì˜ 20%)
    'auto_balance': True,                                  # ìë™ í´ë˜ìŠ¤ ê· í˜• ì¡°ì •
    'allow_user_input': True,                             # ì‚¬ìš©ì ì…ë ¥ í—ˆìš© (ìˆ˜ì •: True â†’ False)
}

# ================================
# ì¶œë ¥ ë° ë¡œê¹… ì„¤ì •
# ================================
OUTPUT_CONFIG = {
    'verbose': True,                                       # ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
    'save_dataset_info': True,                            # ë°ì´í„°ì…‹ ì •ë³´ ì €ì¥
    'save_model_summary': True,                           # ëª¨ë¸ ìš”ì•½ ì €ì¥
    'save_training_history': True,                        # í›ˆë ¨ íˆìŠ¤í† ë¦¬ ì €ì¥
    'save_confusion_matrix': True,                        # í˜¼ë™í–‰ë ¬ ì €ì¥
    'save_classification_report': True,                   # ë¶„ë¥˜ ë³´ê³ ì„œ ì €ì¥
    'plot_training_curves': True,                         # í›ˆë ¨ ê³¡ì„  ê·¸ë˜í”„ ì €ì¥
    'plot_confusion_matrix': True,                        # í˜¼ë™í–‰ë ¬ ì‹œê°í™” ì €ì¥
}

# ================================
# ê³„ì‚°ëœ ì„¤ì •ê°’ë“¤ (ìë™ ìƒì„±)
# ================================
def get_audio_frames_count(duration=None):
    """ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë”°ë¥¸ YAMNet í”„ë ˆì„ ìˆ˜ ê³„ì‚°"""
    if duration is None:
        duration = MODEL_CONFIG['audio_duration']
    # YAMNetì€ 0.48ì´ˆë§ˆë‹¤ í•˜ë‚˜ì˜ í”„ë ˆì„ ìƒì„±
    return int(duration / 0.48)

def get_transition_frame_counts():
    """ì „í™˜ ë°ì´í„°ë¡œ ìƒì„±ë  í”„ë ˆì„ ìˆ˜ ê³„ì‚°"""
    if not TRANSITION_CONFIG['enabled']:
        return {}
    
    total_frames = get_audio_frames_count()
    transition_counts = {}
    
    for trans_type, config in TRANSITION_CONFIG['types'].items():
        if config['enabled']:
            # ì „í™˜ ë°ì´í„°ëŠ” ì „ì²´ ê¸¸ì´ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ full frame count
            transition_counts[trans_type] = total_frames * config['weight']
    
    return transition_counts

def get_recommended_samples():
    """ê¶Œì¥ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°"""
    target_frames = DATA_GENERATION_CONFIG['target_frames_per_class']
    frames_per_audio = get_audio_frames_count()
    
    # ê¸°ë³¸ ë°ì´í„° í•„ìš” ìƒ˜í”Œ ìˆ˜
    base_samples_needed = target_frames // frames_per_audio
    
    # ì „í™˜ ë°ì´í„°ë¡œ ì–»ì„ ìˆ˜ ìˆëŠ” í”„ë ˆì„ ìˆ˜ ê³ ë ¤
    transition_frames = get_transition_frame_counts()
    total_transition_frames = sum(transition_frames.values()) if transition_frames else 0
    
    # ì „í™˜ ë°ì´í„° ë¹„ìœ¨ ê³ ë ¤
    transition_ratio = DATA_GENERATION_CONFIG['transition_data_ratio']
    transition_contribution = total_transition_frames * transition_ratio
    
    # ì‹¤ì œ í•„ìš”í•œ ê¸°ë³¸ ìƒ˜í”Œ ìˆ˜
    actual_samples_needed = max(1, int((target_frames - transition_contribution) // frames_per_audio))
    
    return {
        'frames_per_audio': frames_per_audio,
        'target_frames_per_class': target_frames,
        'base_samples_needed': base_samples_needed,
        'transition_contribution': transition_contribution,
        'recommended_samples': actual_samples_needed,
        'total_classes': NUM_CLASSES,
        'active_classes': ALL_CLASSES
    }

def create_output_directories():
    """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± - ë²„ì „ë³„ ì²´ê³„ì  êµ¬ì¡°"""
    directories = [
        RESULTS_BASE_DIR,
        VERSION_DIR,
        TRAINING_RESULTS_DIR,
        EVALUATION_RESULTS_DIR,
        MODEL_SAVE_DIR,
        REPORT_SAVE_DIR,
        DATASET_SAVE_DIR
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
    
    # ì‹¤í—˜ ê¸°ë¡ íŒŒì¼ ìƒì„±
    create_experiment_log()

def create_experiment_log():
    """ì‹¤í—˜ ë¡œê·¸ íŒŒì¼ ìƒì„± ë° ì—…ë°ì´íŠ¸"""
    log_file = os.path.join(VERSION_DIR, 'experiment_log.txt')
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # ì‹¤í—˜ ì •ë³´ ìˆ˜ì§‘
    experiment_info = f"""
========================================
ì‹¤í—˜ ì‹œì‘: {timestamp}
ëª¨ë¸ ë²„ì „: {MODEL_CONFIG['version']}
ì˜¤ë””ì˜¤ ê¸¸ì´: {MODEL_CONFIG['audio_duration']}ì´ˆ
í›ˆë ¨ ì—í¬í¬: {TRAINING_CONFIG['epochs']}
ë°°ì¹˜ í¬ê¸°: {TRAINING_CONFIG['batch_size']}
í•™ìŠµë¥ : {TRAINING_CONFIG['learning_rate']}
ëª©í‘œ í”„ë ˆì„/í´ë˜ìŠ¤: {DATA_GENERATION_CONFIG['target_frames_per_class']}
í™œì„± í´ë˜ìŠ¤: {', '.join(ALL_CLASSES)}
========================================
"""
    
    # ë¡œê·¸ íŒŒì¼ì— ì¶”ê°€
    with open(log_file, 'a', encoding='utf-8') as f:
        f.write(experiment_info)

def get_experiment_save_path(file_type='model', custom_name=None):
    """ì‹¤í—˜ë³„ ì €ì¥ ê²½ë¡œ ìƒì„±"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    version = MODEL_CONFIG['version']
    
    if custom_name:
        base_name = custom_name
    else:
        base_name = f"yamnet_lstm_{version}_{timestamp}"
    
    if file_type == 'model':
        return os.path.join(MODEL_SAVE_DIR, f"{base_name}.h5")
    elif file_type == 'dataset':
        return os.path.join(DATASET_SAVE_DIR, f"dataset_{base_name}.npz")
    elif file_type == 'dataset_info':
        return os.path.join(DATASET_SAVE_DIR, f"dataset_info_{base_name}.json")
    elif file_type == 'training_report':
        return os.path.join(REPORT_SAVE_DIR, f"training_report_{base_name}.json")
    elif file_type == 'evaluation_report':
        return os.path.join(REPORT_SAVE_DIR, f"evaluation_report_{base_name}.md")
    elif file_type == 'experiment_summary':
        return os.path.join(VERSION_DIR, f"experiment_summary_{timestamp}.json")
    else:
        return os.path.join(REPORT_SAVE_DIR, f"{file_type}_{base_name}")

def save_experiment_summary(results_dict):
    """ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ì €ì¥"""
    summary_path = get_experiment_save_path('experiment_summary')
    
    summary = {
        'experiment_info': {
            'timestamp': datetime.now().isoformat(),
            'version': MODEL_CONFIG['version'],
            'config': {
                'model_config': MODEL_CONFIG,
                'training_config': TRAINING_CONFIG,
                'data_generation_config': DATA_GENERATION_CONFIG
            }
        },
        'results': results_dict
    }
    
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“‹ ì‹¤í—˜ ìš”ì•½ ì €ì¥: {summary_path}")
    return summary_path

def create_training_report(results, model_config=None, best_epoch=1, dataset_stats=None, class_weights=None, user_samples=None):
    """ìƒì„¸ í›ˆë ¨ ë³´ê³ ì„œ ìƒì„± (í…ìŠ¤íŠ¸ + ì‹œê°í™”)"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    version = MODEL_CONFIG['version']
    
    # í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ê²½ë¡œ
    report_path = os.path.join(TRAINING_RESULTS_DIR, f"training_report_{version}_{timestamp}.txt")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("ğŸµ YAMNet + LSTM ëª¨ë¸ í›ˆë ¨ ë³´ê³ ì„œ\n")
        f.write("=" * 80 + "\n\n")
        
        # 1. ê¸°ë³¸ ì •ë³´
        f.write("ğŸ“‹ ì‹¤í—˜ ì •ë³´\n")
        f.write("-" * 40 + "\n")
        f.write(f"ì‹¤í—˜ ì¼ì‹œ: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}\n")
        f.write(f"ëª¨ë¸ ë²„ì „: {MODEL_CONFIG['version']}\n")
        f.write(f"ì˜¤ë””ì˜¤ ê¸¸ì´: {MODEL_CONFIG['audio_duration']}ì´ˆ\n")
        f.write(f"ìƒ˜í”Œë§ ë ˆì´íŠ¸: {MODEL_CONFIG['sample_rate']:,} Hz\n\n")
        
        # 2. ë°ì´í„°ì…‹ êµ¬ì„± ì •ë³´
        f.write("ğŸ“Š ë°ì´í„°ì…‹ êµ¬ì„±\n")
        f.write("-" * 40 + "\n")
        
        if dataset_stats and 'final_stats' in dataset_stats:
            stats = dataset_stats['final_stats']
            f.write(f"ì´ ì‹œí€€ìŠ¤ ìˆ˜: {stats['total_sequences']:,}ê°œ\n")
            f.write(f"ì‹œí€€ìŠ¤ í˜•íƒœ: {stats['sequence_shape']}\n")
            f.write(f"íŒ¨ë”© ê¸¸ì´: {stats['target_length']}ê°œ í”„ë ˆì„\n\n")
            
            # ì‹œí€€ìŠ¤ ê¸¸ì´ í†µê³„
            if 'sequence_length_stats' in stats:
                length_stats = stats['sequence_length_stats']
                f.write("ì‹œí€€ìŠ¤ ê¸¸ì´ í†µê³„ (íŒ¨ë”© ì „):\n")
                f.write(f"  ìµœì†Œ: {length_stats['min_length']}í”„ë ˆì„\n")
                f.write(f"  ìµœëŒ€: {length_stats['max_length']}í”„ë ˆì„\n")
                f.write(f"  í‰ê· : {length_stats['mean_length']:.1f}í”„ë ˆì„\n")
                f.write(f"  íŒ¨ë”©: {length_stats['padded_length']}í”„ë ˆì„\n\n")
            
            # í´ë˜ìŠ¤ë³„ ë¶„í¬
            f.write("í´ë˜ìŠ¤ë³„ ì‹œí€€ìŠ¤ ë¶„í¬:\n")
            total_sequences = stats['total_sequences']
            for class_name, count in stats['class_distribution'].items():
                percentage = (count / total_sequences) * 100
                frame_total = stats.get('class_frame_totals', {}).get(class_name, 0)
                f.write(f"  {class_name:8}: {count:4,}ê°œ ({percentage:5.1f}%) - {frame_total:,} ì‹¤ì œí”„ë ˆì„\n")
            
            f.write("\n")
        
        # 3. ì‚¬ìš©ì ì„¤ì • ìƒ˜í”Œ ìˆ˜ (ìˆëŠ” ê²½ìš°)
        if user_samples:
            f.write("ğŸ¯ ì‚¬ìš©ì ì„¤ì • ìƒ˜í”Œ ìˆ˜\n")
            f.write("-" * 40 + "\n")
            total_user_samples = sum(user_samples.values())
            for class_name, sample_count in user_samples.items():
                percentage = (sample_count / total_user_samples) * 100
                f.write(f"  {class_name:8}: {sample_count:4,}ê°œ ({percentage:5.1f}%)\n")
            f.write(f"  ì´í•©:      {total_user_samples:4,}ê°œ\n\n")
        
        # 4. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì •ë³´
        if class_weights:
            f.write("âš–ï¸ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜\n")
            f.write("-" * 40 + "\n")
            for class_idx, weight in class_weights.items():
                class_name = ALL_CLASSES[int(class_idx)]
                f.write(f"  {class_name:8}: {weight:.3f}\n")
            f.write("\n")
        
        # 5. í›ˆë ¨ ì„¤ì •
        f.write("ğŸ§  í›ˆë ¨ ì„¤ì •\n")
        f.write("-" * 40 + "\n")
        f.write(f"ì—í¬í¬ ìˆ˜: {TRAINING_CONFIG['epochs']}\n")
        f.write(f"ë°°ì¹˜ í¬ê¸°: {TRAINING_CONFIG['batch_size']}\n")
        f.write(f"í•™ìŠµë¥ : {TRAINING_CONFIG['learning_rate']}\n")
        f.write(f"LSTM ìœ ë‹›: {TRAINING_CONFIG['lstm_units']}\n")
        f.write(f"ë“œë¡­ì•„ì›ƒ: {TRAINING_CONFIG['dropout_rate']}\n")
        f.write(f"í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {'ì‚¬ìš©' if TRAINING_CONFIG['use_class_weights'] else 'ë¯¸ì‚¬ìš©'}\n")
        f.write(f"ì¡°ê¸° ì¢…ë£Œ: {'ì‚¬ìš©' if TRAINING_CONFIG['early_stopping']['enabled'] else 'ë¯¸ì‚¬ìš©'}\n")
        f.write(f"í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§: {'ì‚¬ìš©' if TRAINING_CONFIG['learning_rate_schedule']['enabled'] else 'ë¯¸ì‚¬ìš©'}\n\n")
        
        # 6. ë°ì´í„° ë¶„í•  ì •ë³´
        split_config = TRAINING_CONFIG['data_split']
        f.write("ğŸ“Š ë°ì´í„° ë¶„í•  ì„¤ì •\n")
        f.write("-" * 40 + "\n")
        f.write(f"í›ˆë ¨: {split_config['train_ratio']:.1%}\n")
        f.write(f"ê²€ì¦: {split_config['validation_ratio']:.1%}\n")
        f.write(f"í…ŒìŠ¤íŠ¸: {split_config['test_ratio']:.1%}\n")
        f.write(f"ê³„ì¸µ ë¶„í• : {'ì‚¬ìš©' if split_config['stratify'] else 'ë¯¸ì‚¬ìš©'}\n")
        f.write(f"ë°ì´í„° ì…”í”Œ: {'ì‚¬ìš©' if split_config['shuffle'] else 'ë¯¸ì‚¬ìš©'}\n\n")
        
        # 7. ë°ì´í„° ì¦ê°• ì„¤ì •
        f.write("ğŸ¨ ë°ì´í„° ì¦ê°• ì„¤ì •\n")
        f.write("-" * 40 + "\n")
        for class_name in ALL_CLASSES:
            if class_name in AUGMENTATION_CONFIG:
                config = AUGMENTATION_CONFIG[class_name]
                enabled = config.get('enabled', False)
                f.write(f"  {class_name:8}: {'ì‚¬ìš©' if enabled else 'ë¯¸ì‚¬ìš©'}")
                if enabled and 'methods' in config:
                    f.write(f" - {', '.join(config['methods'])}")
                f.write("\n")
        f.write("\n")
        
        # 8. ì „í™˜ ë°ì´í„° ì„¤ì •
        f.write("ğŸ”„ ì „í™˜ ë°ì´í„° ì„¤ì •\n")
        f.write("-" * 40 + "\n")
        f.write(f"ì „í™˜ ë°ì´í„°: {'ì‚¬ìš©' if TRANSITION_CONFIG['enabled'] else 'ë¯¸ì‚¬ìš©'}\n")
        if TRANSITION_CONFIG['enabled']:
            f.write(f"í˜ì´ë“œ ì‹œê°„: {TRANSITION_CONFIG['fade_duration']}ì´ˆ\n")
            f.write("ì „í™˜ íƒ€ì…ë³„ ì„¤ì •:\n")
            for trans_type, config in TRANSITION_CONFIG['types'].items():
                enabled = config.get('enabled', False)
                weight = config.get('weight', 1.0)
                f.write(f"  {trans_type:20}: {'ì‚¬ìš©' if enabled else 'ë¯¸ì‚¬ìš©'} (ê°€ì¤‘ì¹˜: {weight})\n")
        f.write("\n")
        
        # 9. í›ˆë ¨ ê²°ê³¼
        if 'evaluation' in results:
            eval_results = results['evaluation']
            f.write("ğŸ¯ í›ˆë ¨ ê²°ê³¼\n")
            f.write("-" * 40 + "\n")
            f.write(f"ê²€ì¦ ì •í™•ë„: {eval_results.get('accuracy', 0):.4f} ({eval_results.get('accuracy', 0)*100:.2f}%)\n")
            f.write(f"ê²€ì¦ ì†ì‹¤: {eval_results.get('loss', 0):.4f}\n")
            
            if 'precision' in eval_results:
                f.write(f"ì •ë°€ë„: {eval_results['precision']:.4f}\n")
            if 'recall' in eval_results:
                f.write(f"ì¬í˜„ìœ¨: {eval_results['recall']:.4f}\n")
            
            # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥
            if 'class_report' in results:
                f.write("\ní´ë˜ìŠ¤ë³„ ì„±ëŠ¥:\n")
                class_report = results['class_report']
                for class_name in ALL_CLASSES:
                    if class_name in class_report:
                        report = class_report[class_name]
                        f.write(f"  {class_name:8}: P={report['precision']:.3f}, R={report['recall']:.3f}, F1={report['f1-score']:.3f}\n")
            f.write("\n")
        
        # 10. ëª¨ë¸ íŒŒì¼ ì •ë³´
        if 'model_paths' in results:
            f.write("ğŸ’¾ ìƒì„±ëœ íŒŒì¼\n")
            f.write("-" * 40 + "\n")
            for path in results['model_paths']:
                f.write(f"ëª¨ë¸: {os.path.basename(path)}\n")
        
        f.write("\n")
        f.write("=" * 80 + "\n")
        f.write("ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ\n")
        f.write("=" * 80 + "\n")
    
    print(f"ğŸ“‹ í›ˆë ¨ ë³´ê³ ì„œ ì €ì¥: {report_path}")
    return {'training_report': report_path}

def create_dataset_visualization(dataset_info, save_dir=None):
    """ë°ì´í„°ì…‹ êµ¬ì„± ì‹œê°í™”"""
    import matplotlib.pyplot as plt
    import numpy as np
    
    if save_dir is None:
        save_dir = TRAINING_RESULTS_DIR
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # í•œê¸€ í°íŠ¸ ì„¤ì •
    plt.rcParams['font.family'] = ['Malgun Gothic', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    if 'final_stats' not in dataset_info:
        return None
    
    stats = dataset_info['final_stats']
    
    # 1. í´ë˜ìŠ¤ë³„ ì‹œí€€ìŠ¤ ë¶„í¬ íŒŒì´ ì°¨íŠ¸
    if 'class_distribution' in stats:
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # íŒŒì´ ì°¨íŠ¸
        class_names = list(stats['class_distribution'].keys())
        class_counts = list(stats['class_distribution'].values())
        
        colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc']
        
        wedges, texts, autotexts = ax1.pie(class_counts, labels=class_names, autopct='%1.1f%%',
                                          colors=colors[:len(class_names)], startangle=90)
        ax1.set_title('í´ë˜ìŠ¤ë³„ ì‹œí€€ìŠ¤ ë¶„í¬', fontsize=14, fontweight='bold')
        
        # ë§‰ëŒ€ ê·¸ë˜í”„
        bars = ax2.bar(class_names, class_counts, color=colors[:len(class_names)])
        ax2.set_title('í´ë˜ìŠ¤ë³„ ì‹œí€€ìŠ¤ ìˆ˜', fontsize=14, fontweight='bold')
        ax2.set_ylabel('ì‹œí€€ìŠ¤ ìˆ˜')
        ax2.tick_params(axis='x', rotation=45)
        
        # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
        for bar, count in zip(bars, class_counts):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + max(class_counts)*0.01,
                    f'{count:,}', ha='center', va='bottom')
        
        plt.tight_layout()
        
        # ì €ì¥
        pie_chart_path = os.path.join(save_dir, f'dataset_distribution_{timestamp}.png')
        plt.savefig(pie_chart_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š ë°ì´í„°ì…‹ ë¶„í¬ ì°¨íŠ¸ ì €ì¥: {pie_chart_path}")
    
    # 2. í´ë˜ìŠ¤ë³„ ì‹¤ì œ í”„ë ˆì„ ìˆ˜ ë¹„êµ
    if 'class_frame_totals' in stats:
        fig, ax = plt.subplots(figsize=(12, 6))
        
        frame_totals = stats['class_frame_totals']
        class_names = list(frame_totals.keys())
        frame_counts = list(frame_totals.values())
        
        bars = ax.bar(class_names, frame_counts, color=colors[:len(class_names)])
        ax.set_title('í´ë˜ìŠ¤ë³„ ì‹¤ì œ í”„ë ˆì„ ìˆ˜ (íŒ¨ë”© ì œì™¸)', fontsize=14, fontweight='bold')
        ax.set_ylabel('í”„ë ˆì„ ìˆ˜')
        ax.tick_params(axis='x', rotation=45)
        
        # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
        for bar, count in zip(bars, frame_counts):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + max(frame_counts)*0.01,
                   f'{count:,}', ha='center', va='bottom')
        
        plt.tight_layout()
        
        # ì €ì¥
        frame_chart_path = os.path.join(save_dir, f'frame_distribution_{timestamp}.png')
        plt.savefig(frame_chart_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š í”„ë ˆì„ ë¶„í¬ ì°¨íŠ¸ ì €ì¥: {frame_chart_path}")
    
    # 3. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì‹œê°í™”
    if 'class_weights' in dataset_info:
        fig, ax = plt.subplots(figsize=(10, 6))
        
        weights = dataset_info['class_weights']
        class_indices = [int(k) for k in weights.keys()]
        class_names = [ALL_CLASSES[i] for i in class_indices]
        weight_values = [float(weights[str(i)]) for i in class_indices]
        
        bars = ax.bar(class_names, weight_values, color=colors[:len(class_names)])
        ax.set_title('í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜', fontsize=14, fontweight='bold')
        ax.set_ylabel('ê°€ì¤‘ì¹˜')
        ax.tick_params(axis='x', rotation=45)
        ax.axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='ê¸°ì¤€ì„  (1.0)')
        
        # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
        for bar, weight in zip(bars, weight_values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + max(weight_values)*0.01,
                   f'{weight:.3f}', ha='center', va='bottom')
        
        ax.legend()
        plt.tight_layout()
        
        # ì €ì¥
        weight_chart_path = os.path.join(save_dir, f'class_weights_{timestamp}.png')
        plt.savefig(weight_chart_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì°¨íŠ¸ ì €ì¥: {weight_chart_path}")
    
    return True

def get_model_save_path():
    """ëª¨ë¸ ì €ì¥ ê²½ë¡œ ìƒì„±"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_name = f"yamnet_lstm_{MODEL_CONFIG['version']}_{timestamp}"
    return os.path.join(MODEL_SAVE_DIR, f"{model_name}.h5")

def get_report_save_path():
    """ë³´ê³ ì„œ ì €ì¥ ê²½ë¡œ ìƒì„±"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_name = f"training_report_{MODEL_CONFIG['version']}_{timestamp}"
    return os.path.join(REPORT_SAVE_DIR, f"{report_name}.txt")

def get_dataset_save_path():
    """ë°ì´í„°ì…‹ ì €ì¥ ê²½ë¡œ ìƒì„±"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    dataset_name = f"dataset_info_{MODEL_CONFIG['version']}_{timestamp}"
    return os.path.join(DATASET_SAVE_DIR, f"{dataset_name}.json")

# ================================
# ì„¤ì • ê²€ì¦ í•¨ìˆ˜
# ================================
def validate_config():
    """ì„¤ì •ê°’ ê²€ì¦"""
    errors = []
    
    # ì˜¤ë””ì˜¤ ê¸¸ì´ ê²€ì¦
    if not (5.0 <= MODEL_CONFIG['audio_duration'] <= 10.0):
        errors.append("audio_durationì€ 5.0~10.0 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")
    
    # ìœ„í—˜ í´ë˜ìŠ¤ ìµœì†Œ í•˜ë‚˜ í™œì„±í™” ê²€ì¦
    if not any(DANGER_CLASSES.values()):
        errors.append("ìµœì†Œ í•˜ë‚˜ì˜ ìœ„í—˜ í´ë˜ìŠ¤ëŠ” í™œì„±í™”ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
    
    # ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€ ê²€ì¦
    if not os.path.exists(ENVSOUND_DIR):
        errors.append(f"envsound í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {ENVSOUND_DIR}")
    
    if not os.path.exists(MIXTURE_DIR):
        errors.append(f"mixture í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {MIXTURE_DIR}")
    
    # ì „í™˜ ë°ì´í„° ê°€ì¤‘ì¹˜ ê²€ì¦
    if TRANSITION_CONFIG['enabled']:
        total_weight = sum(config['weight'] for config in TRANSITION_CONFIG['types'].values() if config['enabled'])
        if total_weight == 0:
            errors.append("í™œì„±í™”ëœ ì „í™˜ íƒ€ì…ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ë°ì´í„° ë¶„í•  ë¹„ìœ¨ ê²€ì¦
    split_config = TRAINING_CONFIG['data_split']
    total_ratio = split_config['train_ratio'] + split_config['validation_ratio'] + split_config['test_ratio']
    if abs(total_ratio - 1.0) > 0.001:
        errors.append(f"ë°ì´í„° ë¶„í•  ë¹„ìœ¨ì˜ í•©ì´ 1.0ì´ ì•„ë‹™ë‹ˆë‹¤: {total_ratio:.3f}")
    
    if split_config['train_ratio'] <= 0 or split_config['validation_ratio'] <= 0 or split_config['test_ratio'] <= 0:
        errors.append("ëª¨ë“  ë°ì´í„° ë¶„í•  ë¹„ìœ¨ì€ 0ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤.")
    
    return errors

def split_dataset_3way(X, y, random_seed=None):
    """ë°ì´í„°ë¥¼ train/validation/testë¡œ 3-way ë¶„í•  (ì¤‘ë³µ ì—†ìŒ)"""
    from sklearn.model_selection import train_test_split
    
    if random_seed is None:
        random_seed = TRAINING_CONFIG['random_seed']
    
    split_config = TRAINING_CONFIG['data_split']
    train_ratio = split_config['train_ratio']
    val_ratio = split_config['validation_ratio'] 
    test_ratio = split_config['test_ratio']
    stratify = split_config['stratify']
    shuffle = split_config['shuffle']
    
    print(f"ğŸ“Š ë°ì´í„° 3-way ë¶„í•  (Train:{train_ratio:.1%} / Val:{val_ratio:.1%} / Test:{test_ratio:.1%})")
    print(f"  - ì´ ë°ì´í„°: {len(X):,}ê°œ")
    
    # 1ë‹¨ê³„: Train + Validation vs Test ë¶„í• 
    train_val_ratio = train_ratio + val_ratio  # ë‚¨ì€ ë¹„ìœ¨
    test_size_step1 = test_ratio
    
    stratify_step1 = y if stratify else None
    
    X_train_val, X_test, y_train_val, y_test = train_test_split(
        X, y,
        test_size=test_size_step1,
        random_state=random_seed,
        stratify=stratify_step1,
        shuffle=shuffle
    )
    
    # 2ë‹¨ê³„: Train vs Validation ë¶„í• 
    val_size_step2 = val_ratio / train_val_ratio  # train_val ë‚´ì—ì„œì˜ validation ë¹„ìœ¨
    
    stratify_step2 = y_train_val if stratify else None
    
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_val, y_train_val,
        test_size=val_size_step2,
        random_state=random_seed + 1,  # ë‹¤ë¥¸ ì‹œë“œ ì‚¬ìš©
        stratify=stratify_step2,
        shuffle=shuffle
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"  - Train: {len(X_train):,}ê°œ ({len(X_train)/len(X):.1%})")
    print(f"  - Validation: {len(X_val):,}ê°œ ({len(X_val)/len(X):.1%})")
    print(f"  - Test: {len(X_test):,}ê°œ ({len(X_test)/len(X):.1%})")
    
    # í´ë˜ìŠ¤ë³„ ë¶„í¬ í™•ì¸
    if hasattr(y, '__iter__'):
        print(f"\nğŸ“ˆ í´ë˜ìŠ¤ë³„ ë¶„í¬ í™•ì¸:")
        unique_classes = np.unique(y)
        for class_idx in unique_classes:
            train_count = np.sum(y_train == class_idx)
            val_count = np.sum(y_val == class_idx)
            test_count = np.sum(y_test == class_idx)
            total_count = train_count + val_count + test_count
            
            class_name = ALL_CLASSES[class_idx] if class_idx < len(ALL_CLASSES) else f"Class_{class_idx}"
            print(f"  - {class_name}: Train {train_count}, Val {val_count}, Test {test_count} (ì´ {total_count})")
    
    return X_train, X_val, X_test, y_train, y_val, y_test

def split_dataset_3way_with_lengths(X, y, lengths, random_seed=None):
    """ë°ì´í„°ë¥¼ train/validation/testë¡œ 3-way ë¶„í•  (ê¸¸ì´ ì •ë³´ í¬í•¨)"""
    from sklearn.model_selection import train_test_split
    
    if random_seed is None:
        random_seed = TRAINING_CONFIG['random_seed']
    
    split_config = TRAINING_CONFIG['data_split']
    train_ratio = split_config['train_ratio']
    val_ratio = split_config['validation_ratio'] 
    test_ratio = split_config['test_ratio']
    stratify = split_config['stratify']
    shuffle = split_config['shuffle']
    
    print(f"ğŸ“Š ë°ì´í„° 3-way ë¶„í•  (Train:{train_ratio:.1%} / Val:{val_ratio:.1%} / Test:{test_ratio:.1%})")
    print(f"  - ì´ ë°ì´í„°: {len(X):,}ê°œ")
    
    # 1ë‹¨ê³„: Train + Validation vs Test ë¶„í• 
    train_val_ratio = train_ratio + val_ratio  # ë‚¨ì€ ë¹„ìœ¨
    test_size_step1 = test_ratio
    
    stratify_step1 = y if stratify else None
    
    X_train_val, X_test, y_train_val, y_test, lengths_train_val, lengths_test = train_test_split(
        X, y, lengths,
        test_size=test_size_step1,
        random_state=random_seed,
        stratify=stratify_step1,
        shuffle=shuffle
    )
    
    # 2ë‹¨ê³„: Train vs Validation ë¶„í• 
    val_size_step2 = val_ratio / train_val_ratio  # train_val ë‚´ì—ì„œì˜ validation ë¹„ìœ¨
    
    stratify_step2 = y_train_val if stratify else None
    
    X_train, X_val, y_train, y_val, lengths_train, lengths_val = train_test_split(
        X_train_val, y_train_val, lengths_train_val,
        test_size=val_size_step2,
        random_state=random_seed + 1,  # ë‹¤ë¥¸ ì‹œë“œ ì‚¬ìš©
        stratify=stratify_step2,
        shuffle=shuffle
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"  - Train: {len(X_train):,}ê°œ ({len(X_train)/len(X):.1%})")
    print(f"  - Validation: {len(X_val):,}ê°œ ({len(X_val)/len(X):.1%})")
    print(f"  - Test: {len(X_test):,}ê°œ ({len(X_test)/len(X):.1%})")
    
    # í´ë˜ìŠ¤ë³„ ë¶„í¬ í™•ì¸
    if hasattr(y, '__iter__'):
        print(f"\nğŸ“ˆ í´ë˜ìŠ¤ë³„ ë¶„í¬ í™•ì¸:")
        unique_classes = np.unique(y)
        for class_idx in unique_classes:
            train_count = np.sum(y_train == class_idx)
            val_count = np.sum(y_val == class_idx)
            test_count = np.sum(y_test == class_idx)
            total_count = train_count + val_count + test_count
            
            class_name = ALL_CLASSES[class_idx] if class_idx < len(ALL_CLASSES) else f"Class_{class_idx}"
            print(f"  - {class_name}: Train {train_count}, Val {val_count}, Test {test_count} (ì´ {total_count})")
    
    return X_train, X_val, X_test, y_train, y_val, y_test, lengths_train, lengths_val, lengths_test

def save_dataset_splits(X_train, X_val, X_test, y_train, y_val, y_test, base_name=None):
    """ë¶„í• ëœ ë°ì´í„°ì…‹ì„ ê°œë³„ íŒŒì¼ë¡œ ì €ì¥"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if base_name is None:
        base_name = f"split_dataset_{MODEL_CONFIG['version']}_{timestamp}"
    
    # ê° ë¶„í• ì„ ê°œë³„ íŒŒì¼ë¡œ ì €ì¥
    train_path = os.path.join(DATASET_SAVE_DIR, f"{base_name}_train.npz")
    val_path = os.path.join(DATASET_SAVE_DIR, f"{base_name}_val.npz")
    test_path = os.path.join(DATASET_SAVE_DIR, f"{base_name}_test.npz")
    
    np.savez_compressed(train_path, X=X_train, y=y_train)
    np.savez_compressed(val_path, X=X_val, y=y_val)
    np.savez_compressed(test_path, X=X_test, y=y_test)
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        'split_info': {
            'train_ratio': TRAINING_CONFIG['data_split']['train_ratio'],
            'validation_ratio': TRAINING_CONFIG['data_split']['validation_ratio'],
            'test_ratio': TRAINING_CONFIG['data_split']['test_ratio'],
            'stratify': TRAINING_CONFIG['data_split']['stratify'],
            'random_seed': TRAINING_CONFIG['random_seed']
        },
        'data_counts': {
            'train': len(X_train),
            'validation': len(X_val),
            'test': len(X_test),
            'total': len(X_train) + len(X_val) + len(X_test)
        },
        'files': {
            'train': train_path,
            'validation': val_path,
            'test': test_path
        },
        'creation_time': datetime.now().isoformat(),
        'model_version': MODEL_CONFIG['version']
    }
    
    metadata_path = os.path.join(DATASET_SAVE_DIR, f"{base_name}_metadata.json")
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ ë¶„í• ëœ ë°ì´í„°ì…‹ ì €ì¥:")
    print(f"  - Train: {train_path}")
    print(f"  - Validation: {val_path}")
    print(f"  - Test: {test_path}")
    print(f"  - Metadata: {metadata_path}")
    
    return {
        'train': train_path,
        'validation': val_path,
        'test': test_path,
        'metadata': metadata_path
    }

def save_dataset_splits_with_lengths(X_train, X_val, X_test, y_train, y_val, y_test, 
                                   lengths_train, lengths_val, lengths_test, base_name=None):
    """ë¶„í• ëœ ë°ì´í„°ì…‹ì„ ê°œë³„ íŒŒì¼ë¡œ ì €ì¥ (ê¸¸ì´ ì •ë³´ í¬í•¨)"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if base_name is None:
        base_name = f"split_dataset_{MODEL_CONFIG['version']}_{timestamp}"
    
    # ê° ë¶„í• ì„ ê°œë³„ íŒŒì¼ë¡œ ì €ì¥ (ê¸¸ì´ ì •ë³´ í¬í•¨)
    train_path = os.path.join(DATASET_SAVE_DIR, f"{base_name}_train.npz")
    val_path = os.path.join(DATASET_SAVE_DIR, f"{base_name}_val.npz")
    test_path = os.path.join(DATASET_SAVE_DIR, f"{base_name}_test.npz")
    
    np.savez_compressed(train_path, X=X_train, y=y_train, lengths=lengths_train)
    np.savez_compressed(val_path, X=X_val, y=y_val, lengths=lengths_val)
    np.savez_compressed(test_path, X=X_test, y=y_test, lengths=lengths_test)
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥ (ê¸¸ì´ ì •ë³´ í¬í•¨)
    metadata = {
        'split_info': {
            'train_ratio': TRAINING_CONFIG['data_split']['train_ratio'],
            'validation_ratio': TRAINING_CONFIG['data_split']['validation_ratio'],
            'test_ratio': TRAINING_CONFIG['data_split']['test_ratio'],
            'stratify': TRAINING_CONFIG['data_split']['stratify'],
            'random_seed': TRAINING_CONFIG['random_seed']
        },
        'data_counts': {
            'train': len(X_train),
            'validation': len(X_val),
            'test': len(X_test),
            'total': len(X_train) + len(X_val) + len(X_test)
        },
        'sequence_length_stats': {
            'train': {
                'min': int(np.min(lengths_train)),
                'max': int(np.max(lengths_train)),
                'mean': float(np.mean(lengths_train)),
                'std': float(np.std(lengths_train))
            },
            'validation': {
                'min': int(np.min(lengths_val)),
                'max': int(np.max(lengths_val)),
                'mean': float(np.mean(lengths_val)),
                'std': float(np.std(lengths_val))
            },
            'test': {
                'min': int(np.min(lengths_test)),
                'max': int(np.max(lengths_test)),
                'mean': float(np.mean(lengths_test)),
                'std': float(np.std(lengths_test))
            }
        },
        'files': {
            'train': train_path,
            'validation': val_path,
            'test': test_path
        },
        'creation_time': datetime.now().isoformat(),
        'model_version': MODEL_CONFIG['version'],
        'includes_sequence_lengths': True
    }
    
    metadata_path = os.path.join(DATASET_SAVE_DIR, f"{base_name}_metadata.json")
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ ë¶„í• ëœ ë°ì´í„°ì…‹ ì €ì¥ (ê¸¸ì´ ì •ë³´ í¬í•¨):")
    print(f"  - Train: {train_path}")
    print(f"  - Validation: {val_path}")
    print(f"  - Test: {test_path}")
    print(f"  - Metadata: {metadata_path}")
    
    return {
        'train': train_path,
        'validation': val_path,
        'test': test_path,
        'metadata': metadata_path
    }

def print_config_summary():
    """ì„¤ì • ìš”ì•½ ì¶œë ¥"""
    # ê²½ë¡œ ì´ˆê¸°í™” í™•ì¸
    if 'MODEL_SAVE_DIR' not in globals():
        initialize_paths()
    
    print("=" * 60)
    print("ğŸ”§ YAMNet + LSTM í›ˆë ¨ ì„¤ì • ìš”ì•½")
    print("=" * 60)
    
    print(f"ğŸ“± ëª¨ë¸ ë²„ì „: {MODEL_CONFIG['version']}")
    print(f"ğŸµ ì˜¤ë””ì˜¤ ê¸¸ì´: {MODEL_CONFIG['audio_duration']}ì´ˆ")
    print(f"ğŸ“Š ì´ í´ë˜ìŠ¤ ìˆ˜: {NUM_CLASSES}ê°œ")
    print(f"ğŸ¯ í™œì„± í´ë˜ìŠ¤: {', '.join(ALL_CLASSES)}")
    
    print(f"\nğŸ“ˆ í›ˆë ¨ ì„¤ì •:")
    print(f"  - ì—í¬í¬: {TRAINING_CONFIG['epochs']}")
    print(f"  - ë°°ì¹˜ í¬ê¸°: {TRAINING_CONFIG['batch_size']}")
    print(f"  - í•™ìŠµë¥ : {TRAINING_CONFIG['learning_rate']}")
    
    # ë°ì´í„° ë¶„í•  ë¹„ìœ¨ ì •ë³´ ì¶”ê°€
    split_config = TRAINING_CONFIG['data_split']
    print(f"\nğŸ“Š ë°ì´í„° ë¶„í•  ë¹„ìœ¨:")
    print(f"  - Train: {split_config['train_ratio']:.1%}")
    print(f"  - Validation: {split_config['validation_ratio']:.1%}")
    print(f"  - Test: {split_config['test_ratio']:.1%}")
    print(f"  - í´ë˜ìŠ¤ë³„ ê· ë“±ë¶„í• : {'âœ…' if split_config['stratify'] else 'âŒ'}")
    print(f"  - ë°ì´í„° ì…”í”Œ: {'âœ…' if split_config['shuffle'] else 'âŒ'}")
    
    print(f"\nğŸ”„ ë°ì´í„° ì¦ê°• í™œì„±í™”:")
    for class_name, config in AUGMENTATION_CONFIG.items():
        if class_name in ALL_CLASSES and config['enabled']:
            methods = ', '.join(config['methods'])
            print(f"  - {class_name}: {methods}")
    
    print(f"\nâ†”ï¸ ì „í™˜ ë°ì´í„° ì„¤ì •:")
    if TRANSITION_CONFIG['enabled']:
        for trans_type, config in TRANSITION_CONFIG['types'].items():
            if config['enabled']:
                print(f"  - {config['description']}: ê°€ì¤‘ì¹˜ {config['weight']}")
    else:
        print("  - ë¹„í™œì„±í™”")
    
    print(f"\nğŸ’¾ ì¶œë ¥ ê²½ë¡œ:")
    print(f"  - ëª¨ë¸: {MODEL_SAVE_DIR}")
    print(f"  - ë³´ê³ ì„œ: {REPORT_SAVE_DIR}")
    print(f"  - ë°ì´í„°ì…‹: {DATASET_SAVE_DIR}")
    
    print(f"\nğŸµ í‰ê°€ìš© ì˜¤ë””ì˜¤ ê²½ë¡œ:")
    for class_name, path in EVALUATION_AUDIO_PATHS.items():
        if path is not None:
            status = "âœ…" if os.path.exists(path) else "âŒ"
            print(f"  - {class_name}: {status} {path}")
        else:
            print(f"  - {class_name}: â­ï¸ ìŠ¤í‚µ")
    
    print("=" * 60)

def save_config_info(user_samples=None):
    """ì„¤ì • ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    version = MODEL_CONFIG['version']
    
    config_path = os.path.join(TRAINING_RESULTS_DIR, f"config_{version}_{timestamp}.txt")
    
    with open(config_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("âš™ï¸ YAMNet + LSTM ëª¨ë¸ ì„¤ì • ì •ë³´\n")
        f.write("=" * 80 + "\n\n")
        
        # 1. ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
        f.write("ğŸ“± ëª¨ë¸ ê¸°ë³¸ ì„¤ì •\n")
        f.write("-" * 40 + "\n")
        f.write(f"ëª¨ë¸ ë²„ì „: {MODEL_CONFIG['version']}\n")
        f.write(f"ì˜¤ë””ì˜¤ ê¸¸ì´: {MODEL_CONFIG['audio_duration']}ì´ˆ\n")
        f.write(f"ìƒ˜í”Œë§ ë ˆì´íŠ¸: {MODEL_CONFIG['sample_rate']:,} Hz\n")
        f.write(f"YAMNet í”„ë ˆì„ ìˆ˜: {get_audio_frames_count()}ê°œ\n")
        f.write(f"YAMNet íŠ¹ì„± ìˆ˜: 1024ì°¨ì›\n\n")
        
        # 2. í´ë˜ìŠ¤ ì„¤ì •
        f.write("ğŸ¯ í´ë˜ìŠ¤ ì„¤ì •\n")
        f.write("-" * 40 + "\n")
        f.write(f"ì´ í´ë˜ìŠ¤ ìˆ˜: {NUM_CLASSES}ê°œ\n")
        f.write("í™œì„± í´ë˜ìŠ¤:\n")
        for i, class_name in enumerate(ALL_CLASSES):
            korean_name = CLASS_NAMES.get(class_name, class_name)
            f.write(f"  {i}. {class_name} ({korean_name})\n")
        
        f.write("\nìœ„í—˜ í´ë˜ìŠ¤ í™œì„±í™”:\n")
        for danger_class, enabled in DANGER_CLASSES.items():
            f.write(f"  {danger_class}: {'âœ…' if enabled else 'âŒ'}\n")
        f.write("\n")
        
        # 3. í›ˆë ¨ ì„¤ì •
        f.write("ğŸ§  í›ˆë ¨ ì„¤ì •\n")
        f.write("-" * 40 + "\n")
        f.write(f"ì—í¬í¬ ìˆ˜: {TRAINING_CONFIG['epochs']}\n")
        f.write(f"ë°°ì¹˜ í¬ê¸°: {TRAINING_CONFIG['batch_size']}\n")
        f.write(f"í•™ìŠµë¥ : {TRAINING_CONFIG['learning_rate']}\n")
        f.write(f"ëœë¤ ì‹œë“œ: {TRAINING_CONFIG['random_seed']}\n")
        f.write(f"ì…ë ¥ ì •ê·œí™”: {'âœ…' if TRAINING_CONFIG['normalize_input'] else 'âŒ'}\n")
        f.write(f"í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì‚¬ìš©: {'âœ…' if TRAINING_CONFIG['use_class_weights'] else 'âŒ'}\n\n")
        
        # 4. ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¤ì •
        f.write("ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜\n")
        f.write("-" * 40 + "\n")
        f.write(f"LSTM ìœ ë‹› ìˆ˜: {TRAINING_CONFIG['lstm_units']}\n")
        f.write(f"Dense ìœ ë‹› ìˆ˜: {TRAINING_CONFIG['dense_units']}\n")
        f.write(f"ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨: {TRAINING_CONFIG['dropout_rate']}\n")
        f.write("LSTM êµ¬ì¡°:\n")
        f.write(f"  1ì¸µ: {TRAINING_CONFIG['lstm_units']}ìœ ë‹› (return_sequences=True)\n")
        f.write(f"  2ì¸µ: {TRAINING_CONFIG['lstm_units']//2}ìœ ë‹› (return_sequences=True)\n")
        f.write(f"  3ì¸µ: {TRAINING_CONFIG['lstm_units']//4}ìœ ë‹› (return_sequences=False)\n")
        f.write("Dense êµ¬ì¡°:\n")
        f.write(f"  1ì¸µ: {TRAINING_CONFIG['dense_units']}ìœ ë‹› (ReLU)\n")
        f.write(f"  2ì¸µ: {TRAINING_CONFIG['dense_units']//2}ìœ ë‹› (ReLU)\n")
        f.write(f"  ì¶œë ¥: {NUM_CLASSES}ìœ ë‹› (Softmax)\n\n")
        
        # 5. ë°ì´í„° ë¶„í•  ì„¤ì •
        split_config = TRAINING_CONFIG['data_split']
        f.write("ğŸ“Š ë°ì´í„° ë¶„í•  ì„¤ì •\n")
        f.write("-" * 40 + "\n")
        f.write(f"í›ˆë ¨ ë¹„ìœ¨: {split_config['train_ratio']:.1%}\n")
        f.write(f"ê²€ì¦ ë¹„ìœ¨: {split_config['validation_ratio']:.1%}\n")
        f.write(f"í…ŒìŠ¤íŠ¸ ë¹„ìœ¨: {split_config['test_ratio']:.1%}\n")
        f.write(f"ê³„ì¸µ ë¶„í• : {'âœ…' if split_config['stratify'] else 'âŒ'}\n")
        f.write(f"ë°ì´í„° ì…”í”Œ: {'âœ…' if split_config['shuffle'] else 'âŒ'}\n\n")
        
        # 6. ì½œë°± ì„¤ì •
        f.write("ğŸ“ ì½œë°± ì„¤ì •\n")
        f.write("-" * 40 + "\n")
        f.write(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {'âœ…' if TRAINING_CONFIG['save_checkpoints'] else 'âŒ'}\n")
        
        if TRAINING_CONFIG['early_stopping']['enabled']:
            f.write(f"ì¡°ê¸° ì¢…ë£Œ: âœ… (patience: {TRAINING_CONFIG['early_stopping']['patience']})\n")
        else:
            f.write("ì¡°ê¸° ì¢…ë£Œ: âŒ\n")
        
        if TRAINING_CONFIG['learning_rate_schedule']['enabled']:
            lr_config = TRAINING_CONFIG['learning_rate_schedule']
            f.write(f"í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§: âœ…\n")
            f.write(f"  ê°ì†Œ ë¹„ìœ¨: {lr_config['factor']}\n")
            f.write(f"  ëŒ€ê¸° ì—í¬í¬: {lr_config['patience']}\n")
            f.write(f"  ìµœì†Œ í•™ìŠµë¥ : {lr_config['min_lr']}\n")
        else:
            f.write("í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§: âŒ\n")
        f.write("\n")
        
        # 7. ë°ì´í„° ìƒì„± ì„¤ì •
        data_config = DATA_GENERATION_CONFIG
        f.write("ğŸ­ ë°ì´í„° ìƒì„± ì„¤ì •\n")
        f.write("-" * 40 + "\n")
        f.write(f"í´ë˜ìŠ¤ë‹¹ ëª©í‘œ í”„ë ˆì„: {data_config['target_frames_per_class']:,}ê°œ\n")
        f.write(f"ì¦ê°• ì‹œì‘ ì„ê³„ê°’: {data_config['min_frames_for_augmentation']:,}ê°œ\n")
        f.write(f"ì „í™˜ ë°ì´í„° ë¹„ìœ¨: {data_config['transition_data_ratio']:.1%}\n")
        f.write(f"ìë™ ê· í˜• ì¡°ì •: {'âœ…' if data_config['auto_balance'] else 'âŒ'}\n")
        f.write(f"ì‚¬ìš©ì ì…ë ¥ í—ˆìš©: {'âœ…' if data_config['allow_user_input'] else 'âŒ'}\n\n")
        
        # 8. ì‚¬ìš©ì ì…ë ¥ ìƒ˜í”Œ ìˆ˜
        if user_samples:
            f.write("ğŸ¯ ì‚¬ìš©ì ì„¤ì • ìƒ˜í”Œ ìˆ˜\n")
            f.write("-" * 40 + "\n")
            total_user_samples = sum(user_samples.values())
            for class_name, sample_count in user_samples.items():
                percentage = (sample_count / total_user_samples) * 100
                f.write(f"  {class_name:8}: {sample_count:4,}ê°œ ({percentage:5.1f}%)\n")
            f.write(f"  ì´í•©:      {total_user_samples:4,}ê°œ\n\n")
        
        # 9. ë°ì´í„° ì¦ê°• ì„¤ì •
        f.write("ğŸ¨ ë°ì´í„° ì¦ê°• ì„¤ì •\n")
        f.write("-" * 40 + "\n")
        for class_name in ALL_CLASSES:
            if class_name in AUGMENTATION_CONFIG:
                config = AUGMENTATION_CONFIG[class_name]
                enabled = config.get('enabled', False)
                f.write(f"{class_name:8}: {'âœ…' if enabled else 'âŒ'}")
                
                if enabled:
                    methods = config.get('methods', [])
                    f.write(f" - ë°©ë²•: {', '.join(methods)}")
                    
                    if 'volume_range' in config:
                        vol_range = config['volume_range']
                        f.write(f", ë³¼ë¥¨: {vol_range[0]}-{vol_range[1]}")
                    
                    if 'snr_range' in config:
                        snr_range = config['snr_range']
                        f.write(f", SNR: {snr_range[0]}-{snr_range[1]}dB")
                
                f.write("\n")
        f.write("\n")
        
        # 10. ì „í™˜ ë°ì´í„° ì„¤ì •
        f.write("ğŸ”„ ì „í™˜ ë°ì´í„° ì„¤ì •\n")
        f.write("-" * 40 + "\n")
        f.write(f"ì „í™˜ ë°ì´í„° ì‚¬ìš©: {'âœ…' if TRANSITION_CONFIG['enabled'] else 'âŒ'}\n")
        
        if TRANSITION_CONFIG['enabled']:
            f.write(f"í˜ì´ë“œ ì‹œê°„: {TRANSITION_CONFIG['fade_duration']}ì´ˆ\n")
            f.write("ì „í™˜ íƒ€ì…ë³„ ì„¤ì •:\n")
            
            for trans_type, config in TRANSITION_CONFIG['types'].items():
                enabled = config.get('enabled', False)
                weight = config.get('weight', 1.0)
                description = config.get('description', trans_type)
                
                f.write(f"  {trans_type:20}: {'âœ…' if enabled else 'âŒ'}")
                if enabled:
                    f.write(f" (ê°€ì¤‘ì¹˜: {weight:.1f}) - {description}")
                f.write("\n")
        f.write("\n")
        
        # 11. ê²½ë¡œ ì„¤ì •
        f.write("ğŸ“ ê²½ë¡œ ì„¤ì •\n")
        f.write("-" * 40 + "\n")
        f.write(f"í™˜ê²½ìŒ ë””ë ‰í† ë¦¬: {ENVSOUND_DIR}\n")
        f.write(f"ê³µì¥ìŒ ë””ë ‰í† ë¦¬: {MIXTURE_DIR}\n")
        f.write(f"ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬: {VERSION_DIR}\n")
        f.write(f"ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬: {MODEL_SAVE_DIR}\n")
        f.write(f"ë°ì´í„°ì…‹ ì €ì¥ ë””ë ‰í† ë¦¬: {DATASET_SAVE_DIR}\n\n")
        
        # 12. ì‹œìŠ¤í…œ ì •ë³´
        f.write("ğŸ’» ì‹œìŠ¤í…œ ì •ë³´\n")
        f.write("-" * 40 + "\n")
        f.write(f"ì„¤ì • ìƒì„± ì‹œê°„: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„ %Sì´ˆ')}\n")
        
        try:
            import sys
            f.write(f"Python ë²„ì „: {sys.version.split()[0]}\n")
        except:
            f.write("Python ë²„ì „: í™•ì¸ ë¶ˆê°€\n")
        
        try:
            import tensorflow as tf
            f.write(f"TensorFlow ë²„ì „: {tf.__version__}\n")
        except:
            f.write("TensorFlow ë²„ì „: í™•ì¸ ë¶ˆê°€\n")
        
        f.write("\n")
        f.write("=" * 80 + "\n")
        f.write("ì„¤ì • ì •ë³´ ì €ì¥ ì™„ë£Œ\n")
        f.write("=" * 80 + "\n")
    
    print(f"âš™ï¸ ì„¤ì • ì •ë³´ ì €ì¥: {config_path}")
    return config_path

if __name__ == "__main__":
    # ì„¤ì • ê²€ì¦
    errors = validate_config()
    if errors:
        print("âŒ ì„¤ì • ì˜¤ë¥˜:")
        for error in errors:
            print(f"  - {error}")
    else:
        print("âœ… ì„¤ì • ê²€ì¦ ì™„ë£Œ")
        
    # ì„¤ì • ìš”ì•½ ì¶œë ¥
    print_config_summary()
    
    # ê¶Œì¥ ìƒ˜í”Œ ìˆ˜ ì¶œë ¥
    recommendations = get_recommended_samples()
    print(f"\nğŸ“Š ê¶Œì¥ ë°ì´í„° êµ¬ì„±:")
    print(f"  - ì˜¤ë””ì˜¤ë‹¹ í”„ë ˆì„ ìˆ˜: {recommendations['frames_per_audio']}")
    print(f"  - í´ë˜ìŠ¤ë‹¹ ëª©í‘œ í”„ë ˆì„: {recommendations['target_frames_per_class']}")
    print(f"  - ê¶Œì¥ ê¸°ë³¸ ìƒ˜í”Œ ìˆ˜: {recommendations['recommended_samples']}ê°œ/í´ë˜ìŠ¤")

    print(f"  - ì „í™˜ ë°ì´í„° ê¸°ì—¬ë¶„: {recommendations['transition_contribution']:.0f} í”„ë ˆì„")

# ëª¨ë“  ì„¤ì •ì´ ì •ì˜ëœ í›„ ê²½ë¡œ ì´ˆê¸°í™”
initialize_paths()

def update_evaluation_path(class_name, new_path):
    """í‰ê°€ìš© ì˜¤ë””ì˜¤ ê²½ë¡œ ì—…ë°ì´íŠ¸"""
    global EVALUATION_AUDIO_PATHS
    if class_name in EVALUATION_AUDIO_PATHS:
        EVALUATION_AUDIO_PATHS[class_name] = new_path
        print(f"âœ… {class_name} í´ë˜ìŠ¤ ê²½ë¡œ ì—…ë°ì´íŠ¸: {new_path}")
    else:
        print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” í´ë˜ìŠ¤: {class_name}")

def get_evaluation_path(class_name):
    """íŠ¹ì • í´ë˜ìŠ¤ì˜ í‰ê°€ìš© ì˜¤ë””ì˜¤ ê²½ë¡œ ë°˜í™˜"""
    return EVALUATION_AUDIO_PATHS.get(class_name, os.path.join(ENVSOUND_DIR, class_name))

def print_evaluation_paths():
    """í‰ê°€ìš© ê²½ë¡œ í™•ì¸ ë° ì¶œë ¥"""
    print(f"\nğŸµ í‰ê°€ìš© ì˜¤ë””ì˜¤ ê²½ë¡œ í™•ì¸:")
    for class_name, path in EVALUATION_AUDIO_PATHS.items():
        if path is not None:
            exists = os.path.exists(path)
            status = "âœ…" if exists else "âŒ"
            if exists:
                file_count = len([f for f in os.listdir(path) 
                                if f.endswith(('.wav', '.mp3', '.flac'))]) if os.path.isdir(path) else 0
                print(f"  {status} {class_name}: {path} ({file_count}ê°œ íŒŒì¼)")
            else:
                print(f"  {status} {class_name}: {path} (ê²½ë¡œ ì—†ìŒ)")
        else:
            print(f"  â­ï¸ {class_name}: ìŠ¤í‚µ")
