"""
ë°ì´í„° ìƒì„± ëª¨ë“ˆ
===============

ì´ ëª¨ë“ˆì€ YAMNet + LSTM í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- ê° í´ë˜ìŠ¤ë³„ ê· ë“±í•œ í”„ë ˆì„ ìˆ˜ ìƒì„±
- ë°ì´í„° ì¦ê°•ì„ í†µí•œ ë°ì´í„° ë¶€ì¡± í•´ê²°
- ë‹¤ì–‘í•œ ì „í™˜ ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° ìƒì„±
- í”„ë ˆì„ ë ˆë²¨ ë¼ë²¨ë§
"""

import os
import json
import numpy as np
import librosa
import soundfile as sf
from scipy import signal
from sklearn.utils import shuffle
import tensorflow as tf
import tensorflow_hub as hub
from tqdm import tqdm
import matplotlib.pyplot as plt
from datetime import datetime
from config import *

class DataGenerator:
    def __init__(self):
        """ë°ì´í„° ìƒì„±ê¸° ì´ˆê¸°í™”"""
        self.yamnet_model = None
        self.audio_files = {}
        self.audio_durations = {}
        self.total_frames_available = {}
        self.load_yamnet()
        self.scan_audio_files()
        
    def load_yamnet(self):
        """YAMNet ëª¨ë¸ ë¡œë“œ"""
        print("ğŸ”„ YAMNet ëª¨ë¸ ë¡œë”© ì¤‘...")
        yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'
        self.yamnet_model = hub.load(yamnet_model_handle)
        print("âœ… YAMNet ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
    def scan_audio_files(self):
        """ì˜¤ë””ì˜¤ íŒŒì¼ ìŠ¤ìº” ë° í”„ë ˆì„ ìˆ˜ ê³„ì‚°"""
        print("ğŸ” ì˜¤ë””ì˜¤ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
        
        # ìœ„í—˜ ì†Œë¦¬ íŒŒì¼ ìŠ¤ìº”
        for class_name in ACTIVE_DANGER_CLASSES:
            class_dir = os.path.join(ENVSOUND_DIR, class_name)
            if not os.path.exists(class_dir):
                print(f"âš ï¸ ê²½ê³ : {class_dir} í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                continue
                
            files = [f for f in os.listdir(class_dir) if f.endswith(('.wav', '.mp3', '.flac'))]
            self.audio_files[class_name] = [os.path.join(class_dir, f) for f in files]
            
            # ê° íŒŒì¼ì˜ ê¸¸ì´ ê³„ì‚°
            total_duration = 0
            durations = []
            for file_path in self.audio_files[class_name]:
                try:
                    duration = librosa.get_duration(filename=file_path)
                    durations.append(duration)
                    total_duration += duration
                except Exception as e:
                    print(f"âš ï¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {file_path}: {e}")
                    
            self.audio_durations[class_name] = durations
            # YAMNet í”„ë ˆì„ ìˆ˜ ê³„ì‚° (0.48ì´ˆë‹¹ 1í”„ë ˆì„)
            self.total_frames_available[class_name] = int(total_duration / 0.48)
            
            print(f"  ğŸ“ {class_name}: {len(files)}ê°œ íŒŒì¼, {total_duration:.1f}ì´ˆ, {self.total_frames_available[class_name]}ê°œ í”„ë ˆì„")
        
        # ê³µì¥ ì†Œë¦¬ íŒŒì¼ ìŠ¤ìº”
        if os.path.exists(MIXTURE_DIR):
            files = [f for f in os.listdir(MIXTURE_DIR) if f.endswith(('.wav', '.mp3', '.flac'))]
            self.audio_files['factory'] = [os.path.join(MIXTURE_DIR, f) for f in files]
            
            total_duration = 0
            durations = []
            for file_path in self.audio_files['factory']:
                try:
                    duration = librosa.get_duration(filename=file_path)
                    durations.append(duration)
                    total_duration += duration
                except Exception as e:
                    print(f"âš ï¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {file_path}: {e}")
                    
            self.audio_durations['factory'] = durations
            self.total_frames_available['factory'] = int(total_duration / 0.48)
            
            print(f"  ğŸ“ factory: {len(files)}ê°œ íŒŒì¼, {total_duration:.1f}ì´ˆ, {self.total_frames_available['factory']}ê°œ í”„ë ˆì„")
        
        # ë¬´ìŒì€ ë¬´ì œí•œìœ¼ë¡œ ìƒì„± ê°€ëŠ¥
        self.total_frames_available['silence'] = 999999
        print(f"  ğŸ“ silence: ë¬´ì œí•œ ìƒì„± ê°€ëŠ¥")
        
    def calculate_optimal_samples(self):
        """í´ë˜ìŠ¤ë³„ ìµœì  ìƒ˜í”Œ ìˆ˜ ê³„ì‚° ë° ìƒì„¸ ë¶„ì„"""
        target_frames = DATA_GENERATION_CONFIG['target_frames_per_class']
        frames_per_audio = get_audio_frames_count()
        
        print("\nğŸ“Š í´ë˜ìŠ¤ë³„ ìƒì„¸ ë°ì´í„° ë¶„ì„:")
        print("=" * 80)
        print(f"ğŸ¯ ëª©í‘œ: í´ë˜ìŠ¤ë‹¹ {target_frames:,}ê°œ í”„ë ˆì„")
        print(f"ğŸ“ ì˜¤ë””ì˜¤ë‹¹ í”„ë ˆì„ ìˆ˜: {frames_per_audio}ê°œ")
        print("=" * 80)
        
        recommendations = {}
        
        for class_name in ALL_CLASSES:
            available_frames = self.total_frames_available.get(class_name, 0)
            base_samples_possible = available_frames // frames_per_audio if frames_per_audio > 0 else 0
            
            # í•„ìš”í•œ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
            needed_samples = target_frames // frames_per_audio if frames_per_audio > 0 else 0
            needed_frames_from_samples = needed_samples * frames_per_audio
            
            # ì „í™˜ ë°ì´í„° ê¸°ì—¬ë¶„ ê³„ì‚°
            transition_contribution = self._calculate_transition_contribution(class_name, frames_per_audio)
            
            # ì‹¤ì œ í•„ìš”í•œ ê¸°ë³¸ ìƒ˜í”Œ ìˆ˜ (ì „í™˜ ë°ì´í„° ê³ ë ¤)
            remaining_frames_needed = max(0, target_frames - transition_contribution)
            actual_needed_samples = remaining_frames_needed // frames_per_audio if frames_per_audio > 0 else 0
            
            # ì¦ê°• í•„ìš” ì—¬ë¶€ ë° í•„ìš”ëŸ‰ ê³„ì‚°
            shortage = max(0, actual_needed_samples - base_samples_possible)
            
            # ë¬´ìŒì€ ë¬´ì œí•œ, ë‹¤ë¥¸ í´ë˜ìŠ¤ëŠ” ì¦ê°•ìœ¼ë¡œ ë¶€ì¡±ë¶„ í•´ê²°
            if class_name == 'silence':
                augmentation_needed = 0  # ë¬´ìŒì€ ë¬´ì œí•œ ìƒì„± ê°€ëŠ¥
                can_achieve_target = True
            else:
                # ì¦ê°• í™œì„±í™” í™•ì¸
                aug_config = AUGMENTATION_CONFIG.get(class_name, {})
                if aug_config.get('enabled', False) and shortage > 0:
                    # ì¦ê°•ìœ¼ë¡œ ë¶€ì¡±ë¶„ í•´ê²° (ì œí•œ ì—†ìŒ)
                    augmentation_needed = shortage
                    can_achieve_target = True
                else:
                    # ì¦ê°• ë¹„í™œì„±í™” ë˜ëŠ” ë¶€ì¡±ë¶„ ì—†ìŒ
                    augmentation_needed = 0
                    can_achieve_target = base_samples_possible >= actual_needed_samples
            
            # ì´ ì˜ˆìƒ í”„ë ˆì„ ìˆ˜ ê³„ì‚° (ê¸°ë³¸ + ì¦ê°• + ì „í™˜)
            base_frames = min(base_samples_possible * frames_per_audio, remaining_frames_needed)
            augmented_frames = augmentation_needed * frames_per_audio
            total_estimated_frames = base_frames + augmented_frames + transition_contribution
            
            recommendations[class_name] = {
                'available_frames': available_frames,
                'target_frames': target_frames,
                'base_samples_possible': base_samples_possible,
                'needed_samples': needed_samples,
                'transition_contribution': transition_contribution,
                'actual_needed_samples': actual_needed_samples,
                'shortage_samples': shortage,
                'augmentation_needed': augmentation_needed,
                'estimated_total_frames': min(total_estimated_frames, target_frames),
                'balance_ratio': min(total_estimated_frames, target_frames) / target_frames if target_frames > 0 else 0,
                'need_augmentation': augmentation_needed > 0,
                'feasible': can_achieve_target  # ì¦ê°• ê°€ëŠ¥ ì—¬ë¶€ë¥¼ ê³ ë ¤í•œ ì‹¤í˜„ê°€ëŠ¥ì„±
            }
            
            # ìƒì„¸ ì •ë³´ ì¶œë ¥
            print(f"\nğŸ“ {class_name} ({CLASS_NAMES.get(class_name, class_name)}):")
            print(f"  ğŸ“Š ê°€ìš© í”„ë ˆì„: {available_frames:,}ê°œ")
            print(f"  ğŸ¯ ëª©í‘œ í”„ë ˆì„: {target_frames:,}ê°œ")
            print(f"  ğŸ“ ê¸°ë³¸ ìƒ˜í”Œ ê°€ëŠ¥: {base_samples_possible:,}ê°œ â†’ {base_frames:,}ê°œ í”„ë ˆì„")
            print(f"  ğŸ”„ ì „í™˜ ë°ì´í„° ê¸°ì—¬: {transition_contribution:.0f}ê°œ í”„ë ˆì„")
            print(f"  ğŸ“Š ì‹¤ì œ í•„ìš” ìƒ˜í”Œ: {actual_needed_samples:,}ê°œ")
            
            if augmentation_needed > 0:
                print(f"  ğŸ”§ ì¦ê°• í•„ìš”: {augmentation_needed:,}ê°œ ìƒ˜í”Œ â†’ {augmented_frames:,}ê°œ í”„ë ˆì„")
                print(f"  âš¡ ì¦ê°• ë°°ìœ¨: {augmentation_needed/base_samples_possible:.1f}ë°°" if base_samples_possible > 0 else "  âš¡ ì¦ê°• ë°°ìœ¨: ë¬´í•œ")
            else:
                print(f"  âœ… ì¦ê°• ë¶ˆí•„ìš”")
            
            print(f"  ğŸ“ˆ ì´ ì˜ˆìƒ í”„ë ˆì„: {min(total_estimated_frames, target_frames):,.0f}ê°œ")
            print(f"  âš–ï¸ ê· í˜•ë„: {min(total_estimated_frames, target_frames)/target_frames*100:.1f}%")
            
            # ìƒíƒœ í‘œì‹œ
            if recommendations[class_name]['feasible']:
                if augmentation_needed > 0:
                    print(f"  ğŸŸ¡ ìƒíƒœ: ì¦ê°• í•„ìš”í•˜ì§€ë§Œ ë‹¬ì„± ê°€ëŠ¥")
                else:
                    print(f"  ğŸŸ¢ ìƒíƒœ: ì¶©ë¶„í•œ ë°ì´í„° ë³´ìœ ")
            else:
                aug_config = AUGMENTATION_CONFIG.get(class_name, {})
                if not aug_config.get('enabled', False):
                    print(f"  ğŸ”´ ìƒíƒœ: ëª©í‘œ ë‹¬ì„± ë¶ˆê°€ (ì¦ê°• ë¹„í™œì„±í™”)")
                else:
                    print(f"  ğŸ”´ ìƒíƒœ: ëª©í‘œ ë‹¬ì„± ë¶ˆê°€ (ë°ì´í„° ë¶€ì¡±)")  # ì´ ê²½ìš°ëŠ” ì´ì œ ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•¨
            
        print("\n" + "=" * 80)
        
        # ì „ì²´ ê· í˜• ë¶„ì„
        balance_ratios = [rec['balance_ratio'] for rec in recommendations.values()]
        min_balance = min(balance_ratios)
        max_balance = max(balance_ratios)
        balance_difference = max_balance - min_balance
        
        print(f"ğŸ“Š ì „ì²´ ê· í˜• ë¶„ì„:")
        print(f"  - ìµœì†Œ ê· í˜•ë„: {min_balance*100:.1f}%")
        print(f"  - ìµœëŒ€ ê· í˜•ë„: {max_balance*100:.1f}%")
        print(f"  - ê· í˜• ì°¨ì´: {balance_difference*100:.1f}%p")
        
        if balance_difference < 0.05:  # 5% ì´ë‚´
            print(f"  âœ… ë§¤ìš° ê· ë“±í•œ ë¶„í¬ (ì°¨ì´ 5% ì´ë‚´)")
        elif balance_difference < 0.1:  # 10% ì´ë‚´
            print(f"  ğŸŸ¡ ì–‘í˜¸í•œ ë¶„í¬ (ì°¨ì´ 10% ì´ë‚´)")
        else:
            print(f"  ğŸ”´ ë¶ˆê· ë“±í•œ ë¶„í¬ (ì°¨ì´ 10% ì´ˆê³¼)")
        
        # ì‹¤í˜„ ê°€ëŠ¥ì„± í™•ì¸
        all_feasible = all(rec['feasible'] for rec in recommendations.values())
        
        if all_feasible:
            print(f"  âœ… ëª¨ë“  í´ë˜ìŠ¤ì˜ ëª©í‘œ í”„ë ˆì„ ìˆ˜ ë‹¬ì„± ê°€ëŠ¥")
        else:
            infeasible = [name for name, rec in recommendations.items() if not rec['feasible']]
            print(f"  âŒ ì¼ë¶€ í´ë˜ìŠ¤ì˜ ëª©í‘œ í”„ë ˆì„ ìˆ˜ ë‹¬ì„± ë¶ˆê°€ëŠ¥")
            print(f"     ë¬¸ì œ í´ë˜ìŠ¤: {', '.join(infeasible)}")
            
        print("=" * 80)
        
        return recommendations
    
    def _calculate_transition_contribution(self, class_name, frames_per_audio):
        """ì „í™˜ ë°ì´í„°ë¡œë¶€í„° í•´ë‹¹ í´ë˜ìŠ¤ê°€ ì–»ì„ ìˆ˜ ìˆëŠ” í”„ë ˆì„ ìˆ˜ ê³„ì‚°"""
        if not TRANSITION_CONFIG['enabled']:
            return 0
            
        contribution = 0
        transition_ratio = DATA_GENERATION_CONFIG['transition_data_ratio']
        
        # ë””ë²„ê·¸ ì¶œë ¥ì„ ìœ„í•œ ìƒì„¸ ê³„ì‚°
        details = []
        
        # ê° ì „í™˜ íƒ€ì…ì—ì„œ í•´ë‹¹ í´ë˜ìŠ¤ê°€ ì°¨ì§€í•˜ëŠ” í”„ë ˆì„ ê³„ì‚°
        for trans_type, config in TRANSITION_CONFIG['types'].items():
            if not config['enabled']:
                continue
                
            base_contribution = frames_per_audio * config['weight'] * transition_ratio
            
            if trans_type == 'silence_to_silence' and class_name == 'silence':
                contrib = base_contribution
                contribution += contrib
                details.append(f"    - {trans_type}: {contrib:.1f}í”„ë ˆì„ (ì „ì²´)")
                
            elif trans_type == 'silence_to_factory' and class_name in ['silence', 'factory']:
                # ì „í™˜ ë°ì´í„°ì—ì„œ ê° í´ë˜ìŠ¤ê°€ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨ ì¶”ì • (50:50)
                contrib = base_contribution * 0.5
                contribution += contrib
                details.append(f"    - {trans_type}: {contrib:.1f}í”„ë ˆì„ (50% ê¸°ì—¬)")
                
            elif trans_type == 'silence_to_danger' and (class_name == 'silence' or class_name in ACTIVE_DANGER_CLASSES):
                contrib = base_contribution * 0.5
                contribution += contrib
                details.append(f"    - {trans_type}: {contrib:.1f}í”„ë ˆì„ (50% ê¸°ì—¬)")
                
            elif trans_type == 'factory_to_factory' and class_name == 'factory':
                contrib = base_contribution
                contribution += contrib
                details.append(f"    - {trans_type}: {contrib:.1f}í”„ë ˆì„ (ì „ì²´)")
                
            elif trans_type == 'factory_to_danger' and (class_name == 'factory' or class_name in ACTIVE_DANGER_CLASSES):
                contrib = base_contribution * 0.5
                contribution += contrib
                details.append(f"    - {trans_type}: {contrib:.1f}í”„ë ˆì„ (50% ê¸°ì—¬)")
        
        # ìƒì„¸ ê³„ì‚° ì¶œë ¥ (ë””ë²„ê·¸ìš©, í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
        # if details:
        #     print(f"  ğŸ”„ {class_name} ì „í™˜ ë°ì´í„° ìƒì„¸:")
        #     for detail in details:
        #         print(detail)
        #     print(f"    ğŸ’¡ ì´ ê¸°ì—¬ë„: {contribution:.1f}í”„ë ˆì„")
                
        return int(contribution)
    
    def get_user_input_for_frames(self, recommendations):
        """ì‚¬ìš©ìë¡œë¶€í„° ì‹¤ì œ ìƒì„±í•  í”„ë ˆì„ ìˆ˜ ì…ë ¥ë°›ê¸°"""
        if not DATA_GENERATION_CONFIG['allow_user_input']:
            # ìë™ìœ¼ë¡œ ëª©í‘œê°’ ì‚¬ìš©
            target_frames = DATA_GENERATION_CONFIG['target_frames_per_class']
            return {name: target_frames for name in ALL_CLASSES}
        
        print(f"\nğŸ¯ ê° í´ë˜ìŠ¤ë³„ ìƒì„±í•  í”„ë ˆì„ ìˆ˜ë¥¼ ê²°ì •í•´ì£¼ì„¸ìš”:")
        print(f"ğŸ’¡ ê¶Œì¥: í´ë˜ìŠ¤ë‹¹ {DATA_GENERATION_CONFIG['target_frames_per_class']}ê°œ í”„ë ˆì„ (ê· ë“± ë¶„ë°°)")
        print(f"ğŸ“ ì°¸ê³ : ì˜¤ë””ì˜¤ë‹¹ í‰ê·  {get_audio_frames_count()}ê°œ í”„ë ˆì„")
        print("-" * 60)
        
        user_frames = {}
        
        for class_name in ALL_CLASSES:
            rec = recommendations[class_name]
            target_frames = DATA_GENERATION_CONFIG['target_frames_per_class']
            
            # ì¦ê°•ì„ ê³ ë ¤í•œ ìµœëŒ€ ê°€ëŠ¥ í”„ë ˆì„ ìˆ˜ ê³„ì‚°
            if class_name == 'silence':
                max_possible_frames = 999999  # ë¬´ìŒì€ ë¬´ì œí•œ
            else:
                # ì¦ê°•ì´ í™œì„±í™”ëœ ê²½ìš° ì´ë¡ ì ìœ¼ë¡œ ë¬´ì œí•œ
                aug_config = AUGMENTATION_CONFIG.get(class_name, {})
                if aug_config.get('enabled', False):
                    max_possible_frames = 999999  # ì¦ê°•ìœ¼ë¡œ ë¬´ì œí•œ ìƒì„± ê°€ëŠ¥
                else:
                    max_possible_frames = rec['available_frames']  # ì¦ê°• ë¹„í™œì„±í™”ì‹œë§Œ ì›ë³¸ ì œí•œ
            
            print(f"\nğŸ“ {class_name} ({CLASS_NAMES.get(class_name, class_name)}):")
            if max_possible_frames >= 999999:
                print(f"  - ê°€ìš© ìµœëŒ€ í”„ë ˆì„: ë¬´ì œí•œ (ì¦ê°• í™œì„±í™”)")
            else:
                print(f"  - ê°€ìš© ìµœëŒ€ í”„ë ˆì„: {max_possible_frames:,}ê°œ")
            print(f"  - ê¶Œì¥ í”„ë ˆì„ ìˆ˜: {target_frames:,}ê°œ")
            print(f"  - ì „í™˜ ë°ì´í„° ê¸°ì—¬: {rec['transition_contribution']:.0f}ê°œ í”„ë ˆì„")
            
            while True:
                try:
                    user_input = input(f"  ğŸ‘‰ ìƒì„±í•  í”„ë ˆì„ ìˆ˜ (ê¶Œì¥: {target_frames:,}): ").strip()
                    
                    if user_input == "":
                        frames = target_frames
                    else:
                        frames = int(user_input.replace(',', ''))
                    
                    if frames < 0:
                        print("     âŒ ìŒìˆ˜ëŠ” ì…ë ¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        continue
                    
                    # ë¬´ì œí•œ í´ë˜ìŠ¤ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì œí•œ ê²€ì‚¬
                    if max_possible_frames < 999999 and frames > max_possible_frames:
                        print(f"     âŒ ìµœëŒ€ {max_possible_frames:,}ê°œê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                        continue
                    
                    user_frames[class_name] = frames
                    print(f"     âœ… {frames:,}ê°œ í”„ë ˆì„ ì„¤ì •ë¨")
                    break
                    
                except ValueError:
                    print("     âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                except KeyboardInterrupt:
                    print("\n\nğŸ‘‹ ì‚¬ìš©ì ì¤‘ë‹¨")
                    return None
        
        # ìµœì¢… í™•ì¸
        print(f"\nğŸ“‹ ìµœì¢… ë°ì´í„° ìƒì„± ê³„íš:")
        total_frames = sum(user_frames.values())
        for class_name, frames in user_frames.items():
            percentage = (frames / total_frames) * 100 if total_frames > 0 else 0
            print(f"  - {class_name}: {frames:,}ê°œ í”„ë ˆì„ ({percentage:.1f}%)")
        
        print(f"\nì´ {total_frames:,}ê°œ í”„ë ˆì„ ìƒì„± ì˜ˆì •")
        
        # ê· ë“±ì„± í™•ì¸
        frame_values = list(user_frames.values())
        if len(set(frame_values)) == 1:
            print("âœ… ëª¨ë“  í´ë˜ìŠ¤ê°€ ë™ì¼í•œ í”„ë ˆì„ ìˆ˜ë¡œ ì„¤ì •ë¨ (ì™„ë²½í•œ ê· í˜•)")
        else:
            max_diff = max(frame_values) - min(frame_values)
            print(f"âš ï¸  í´ë˜ìŠ¤ê°„ í”„ë ˆì„ ìˆ˜ ì°¨ì´: {max_diff:,}ê°œ")
        
        confirm = input("\nê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if confirm not in ['y', 'yes']:
            print("ğŸ‘‹ ì¤‘ë‹¨ë¨")
            return None
            
        return user_frames
    
    def generate_silence_audio(self, duration):
        """ë¬´ìŒ ì˜¤ë””ì˜¤ ìƒì„± (ë‹¤ì–‘í•œ ë°°ê²½ë…¸ì´ì¦ˆ í¬í•¨)"""
        sr = MODEL_CONFIG['sample_rate']
        length = int(duration * sr)
        
        # ê¸°ë³¸ ì €ì¡ìŒ
        noise_level = np.random.uniform(0.001, 0.005)
        audio = np.random.normal(0, noise_level, length)
        
        # ë°°ê²½ ë…¸ì´ì¦ˆ ì¶”ê°€ (í™•ë¥ ì )
        if np.random.random() < 0.3:  # 30% í™•ë¥ ë¡œ ë°°ê²½ë…¸ì´ì¦ˆ ì¶”ê°€
            noise_type = np.random.choice(['white', 'pink', 'brown'])
            noise_level = np.random.uniform(0.005, 0.02)
            
            if noise_type == 'white':
                noise = np.random.normal(0, noise_level, length)
            elif noise_type == 'pink':
                # í•‘í¬ ë…¸ì´ì¦ˆ ìƒì„± (1/f íŠ¹ì„±)
                freqs = np.fft.fftfreq(length, 1/sr)
                freqs[0] = 1  # DC ì„±ë¶„ ë°©ì§€
                pink_filter = 1 / np.sqrt(np.abs(freqs))
                white_noise = np.random.normal(0, 1, length)
                pink_fft = np.fft.fft(white_noise) * pink_filter
                noise = np.real(np.fft.ifft(pink_fft)) * noise_level
            else:  # brown
                # ë¸Œë¼ìš´ ë…¸ì´ì¦ˆ ìƒì„± (1/f^2 íŠ¹ì„±)
                freqs = np.fft.fftfreq(length, 1/sr)
                freqs[0] = 1
                brown_filter = 1 / np.abs(freqs)
                white_noise = np.random.normal(0, 1, length)
                brown_fft = np.fft.fft(white_noise) * brown_filter
                noise = np.real(np.fft.ifft(brown_fft)) * noise_level
            
            audio += noise
        
        # í´ë¦¬í•‘ ë°©ì§€
        max_val = np.max(np.abs(audio))
        if max_val > 0.95:
            audio = audio * (0.95 / max_val)
            
        return audio.astype(np.float32)
    
    def load_audio_file(self, file_path):
        """ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ"""
        try:
            audio, sr = librosa.load(file_path, sr=MODEL_CONFIG['sample_rate'])
            return audio
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")
            return None
    
    def load_audio_segment(self, file_path, duration=None):
        """ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ ì§€ì •ëœ ê¸¸ì´ì˜ ì„¸ê·¸ë¨¼íŠ¸ ë¡œë“œ"""
        audio = self.load_audio_file(file_path)
        if audio is not None:
            return self.extract_audio_segment(audio, duration)
        return None
    
    def extract_audio_segment(self, audio, duration=None):
        """ì˜¤ë””ì˜¤ì—ì„œ ì§€ì •ëœ ê¸¸ì´ì˜ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ"""
        if duration is None:
            duration = MODEL_CONFIG['audio_duration']
            
        sr = MODEL_CONFIG['sample_rate']
        target_length = int(duration * sr)
        
        if len(audio) >= target_length:
            # ëœë¤ ìœ„ì¹˜ì—ì„œ ì¶”ì¶œ
            start_idx = np.random.randint(0, len(audio) - target_length + 1)
            return audio[start_idx:start_idx + target_length]
        else:
            # íŒ¨ë”© ë˜ëŠ” ë°˜ë³µ
            if len(audio) < target_length // 2:
                # ë„ˆë¬´ ì§§ìœ¼ë©´ ë°˜ë³µ
                repeat_count = (target_length // len(audio)) + 1
                audio = np.tile(audio, repeat_count)
            
            # ë¶€ì¡±í•œ ë¶€ë¶„ì€ ì œë¡œ íŒ¨ë”©
            padding = target_length - len(audio)
            if padding > 0:
                pad_left = padding // 2
                pad_right = padding - pad_left
                audio = np.pad(audio, (pad_left, pad_right), mode='constant')
            
            return audio[:target_length]
    
    def apply_augmentation(self, audio, class_name, method):
        """ë°ì´í„° ì¦ê°• ì ìš©"""
        config = AUGMENTATION_CONFIG.get(class_name, {})
        if not config.get('enabled', False):
            return audio
            
        sr = MODEL_CONFIG['sample_rate']
        augmented = audio.copy()
        
        if method == 'volume_change':
            vol_min, vol_max = config.get('volume_range', (0.7, 1.3))
            volume_factor = np.random.uniform(vol_min, vol_max)
            augmented = augmented * volume_factor
            
        elif method == 'noise_variation' and class_name == 'silence':
            # ë¬´ìŒ í´ë˜ìŠ¤ì˜ ë…¸ì´ì¦ˆ ë³€í™”
            noise_types = config.get('noise_types', ['white'])
            noise_type = np.random.choice(noise_types)
            noise_level = np.random.uniform(0.001, 0.01)
            
            if noise_type == 'white':
                noise = np.random.normal(0, noise_level, len(augmented))
            elif noise_type == 'pink':
                freqs = np.fft.fftfreq(len(augmented), 1/sr)
                freqs[0] = 1
                pink_filter = 1 / np.sqrt(np.abs(freqs))
                white_noise = np.random.normal(0, 1, len(augmented))
                pink_fft = np.fft.fft(white_noise) * pink_filter
                noise = np.real(np.fft.ifft(pink_fft)) * noise_level
            else:  # brown
                freqs = np.fft.fftfreq(len(augmented), 1/sr)
                freqs[0] = 1
                brown_filter = 1 / np.abs(freqs)
                white_noise = np.random.normal(0, 1, len(augmented))
                brown_fft = np.fft.fft(white_noise) * brown_filter
                noise = np.real(np.fft.ifft(brown_fft)) * noise_level
            
            augmented = noise  # ë¬´ìŒì˜ ê²½ìš° ë…¸ì´ì¦ˆë¡œ ëŒ€ì²´
            
        elif method == 'reverb':
            # ê°„ë‹¨í•œ ë¦¬ë²„ë¸Œ íš¨ê³¼
            decay = np.random.uniform(*config.get('reverb_decay', (0.1, 0.5)))
            delay_samples = int(0.05 * sr)  # 50ms ì§€ì—°
            
            reverb = np.zeros_like(augmented)
            for i in range(3):  # 3íšŒ ë°˜ë³µ
                delay = delay_samples * (i + 1)
                amplitude = decay ** (i + 1)
                if delay < len(augmented):
                    reverb[delay:] += augmented[:-delay] * amplitude
            
            augmented = augmented + reverb * 0.3
            
        elif method == 'room_effect':
            # ë£¸ íš¨ê³¼ (ê°„ë‹¨í•œ IIR í•„í„°)
            room_size = np.random.uniform(*config.get('room_size', (0.1, 0.9)))
            try:
                b, a = signal.butter(2, 0.1 + room_size * 0.4, btype='low')
                augmented = signal.filtfilt(b, a, augmented)
            except Exception:
                # í•„í„° ìƒì„± ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
                pass
            
        elif method == 'speed_change':
            # ì†ë„ ë³€í™” (ì‹œê°„ ìŠ¤íŠ¸ë ˆì¹­)
            speed_min, speed_max = config.get('speed_range', (0.9, 1.1))
            speed_factor = np.random.uniform(speed_min, speed_max)
            augmented = librosa.effects.time_stretch(augmented, rate=speed_factor)
            # ê¸¸ì´ ì¡°ì •
            augmented = self.extract_audio_segment(augmented, MODEL_CONFIG['audio_duration'])
            
        elif method == 'noise_add':
            # ë…¸ì´ì¦ˆ ì¶”ê°€
            noise_level = np.random.uniform(*config.get('noise_level', (0.01, 0.05)))
            noise = np.random.normal(0, noise_level, len(augmented))
            augmented = augmented + noise
            
        elif method == 'factory_mix':
            # ê³µì¥ì†Œë¦¬ì™€ í˜¼í•© (ìœ„í—˜ì†Œë¦¬ìš©)
            if 'factory' in self.audio_files and self.audio_files['factory']:
                factory_file = np.random.choice(self.audio_files['factory'])
                factory_audio = self.load_audio_file(factory_file)
                if factory_audio is not None:
                    factory_segment = self.extract_audio_segment(factory_audio, MODEL_CONFIG['audio_duration'])
                    
                    # SNR ê³„ì‚°
                    snr_min, snr_max = config.get('snr_range', (5, 20))
                    target_snr = np.random.uniform(snr_min, snr_max)
                    
                    # ì‹ í˜¸ì™€ ë…¸ì´ì¦ˆ(ê³µì¥ì†Œë¦¬) íŒŒì›Œ ê³„ì‚°
                    signal_power = np.mean(augmented ** 2)
                    noise_power = np.mean(factory_segment ** 2)
                    
                    if noise_power > 0:
                        # SNRì— ë§ëŠ” ìŠ¤ì¼€ì¼ë§ íŒ©í„° ê³„ì‚°
                        snr_linear = 10 ** (target_snr / 10)
                        noise_scale = np.sqrt(signal_power / (noise_power * snr_linear))
                        factory_segment = factory_segment * noise_scale
                        
                        augmented = augmented + factory_segment
        
        # í´ë¦¬í•‘ ë°©ì§€
        max_val = np.max(np.abs(augmented))
        if max_val > 0.95:
            augmented = augmented * (0.95 / max_val)
            
        return augmented
    
    def generate_transition_audio(self, trans_type, class1, class2=None):
        """ì „í™˜ ì˜¤ë””ì˜¤ ìƒì„±"""
        duration = MODEL_CONFIG['audio_duration']
        sr = MODEL_CONFIG['sample_rate']
        length = int(duration * sr)
        
        config = TRANSITION_CONFIG['types'][trans_type]
        trans_point_range = config['transition_point_range']
        transition_point = np.random.uniform(*trans_point_range)
        transition_frame = int(transition_point * length)
        
        fade_duration = TRANSITION_CONFIG['fade_duration']
        fade_samples = int(fade_duration * sr)
        
        # ê¸°ë³¸ ë¬´ìŒìœ¼ë¡œ ì´ˆê¸°í™”
        audio1 = self.generate_silence_audio(duration)
        audio2 = self.generate_silence_audio(duration)
        
        # ì˜¤ë””ì˜¤ ìƒì„±
        if trans_type == 'silence_to_silence':
            # ë‘ ì¢…ë¥˜ì˜ ë¬´ìŒ ìƒì„±
            audio1 = self.generate_silence_audio(duration)
            audio2 = self.generate_silence_audio(duration)
            
        elif trans_type == 'silence_to_factory':
            audio1 = self.generate_silence_audio(duration)
            if 'factory' in self.audio_files and self.audio_files['factory']:
                factory_file = np.random.choice(self.audio_files['factory'])
                factory_audio = self.load_audio_file(factory_file)
                audio2 = self.extract_audio_segment(factory_audio, duration)
            else:
                audio2 = self.generate_silence_audio(duration)
                
        elif trans_type == 'silence_to_danger':
            audio1 = self.generate_silence_audio(duration)
            danger_class = np.random.choice(ACTIVE_DANGER_CLASSES)
            if danger_class in self.audio_files and self.audio_files[danger_class]:
                danger_file = np.random.choice(self.audio_files[danger_class])
                danger_audio = self.load_audio_file(danger_file)
                audio2 = self.extract_audio_segment(danger_audio, duration)
            else:
                audio2 = self.generate_silence_audio(duration)
                
        elif trans_type == 'factory_to_factory':
            if 'factory' in self.audio_files and len(self.audio_files['factory']) >= 2:
                files = np.random.choice(self.audio_files['factory'], 2, replace=False)
                audio1 = self.extract_audio_segment(self.load_audio_file(files[0]), duration)
                audio2 = self.extract_audio_segment(self.load_audio_file(files[1]), duration)
            else:
                audio1 = self.generate_silence_audio(duration)
                audio2 = self.generate_silence_audio(duration)
                
        elif trans_type == 'factory_to_danger':
            if 'factory' in self.audio_files and self.audio_files['factory']:
                factory_file = np.random.choice(self.audio_files['factory'])
                factory_audio = self.load_audio_file(factory_file)
                audio1 = self.extract_audio_segment(factory_audio, duration)
            else:
                audio1 = self.generate_silence_audio(duration)
                
            danger_class = np.random.choice(ACTIVE_DANGER_CLASSES)
            if danger_class in self.audio_files and self.audio_files[danger_class]:
                danger_file = np.random.choice(self.audio_files[danger_class])
                danger_audio = self.load_audio_file(danger_file)
                danger_segment = self.extract_audio_segment(danger_audio, duration)
                
                # ìœ„í—˜ì†Œë¦¬ ë³¼ë¥¨ ì¡°ì ˆ
                vol_min, vol_max = config.get('danger_volume_ratio', (0.8, 1.5))
                volume_ratio = np.random.uniform(vol_min, vol_max)
                danger_segment = danger_segment * volume_ratio
                
                # ê³µì¥ì†Œë¦¬ì— ìœ„í—˜ì†Œë¦¬ ì¶”ê°€ (ë¯¹ì‹±)
                audio2 = audio1 + danger_segment
            else:
                audio2 = audio1
        
        # í˜ì´ë“œ ì „í™˜ ì ìš©
        result = np.zeros(length, dtype=np.float32)
        
        # ì „í™˜ ì „ êµ¬ê°„
        result[:transition_frame] = audio1[:transition_frame]
        
        # ì „í™˜ êµ¬ê°„ (í˜ì´ë“œ)
        fade_start = max(0, transition_frame - fade_samples // 2)
        fade_end = min(length, transition_frame + fade_samples // 2)
        fade_length = fade_end - fade_start
        
        if fade_length > 0:
            fade_curve = np.linspace(0, 1, fade_length)
            result[fade_start:fade_end] = (
                audio1[fade_start:fade_end] * (1 - fade_curve) +
                audio2[fade_start:fade_end] * fade_curve
            )
        
        # ì „í™˜ í›„ êµ¬ê°„
        if transition_frame < length:
            result[transition_frame:] = audio2[transition_frame:]
        
        # í”„ë ˆì„ë³„ ë¼ë²¨ ìƒì„±
        frames_per_audio = get_audio_frames_count()
        transition_frame_idx = int(transition_point * frames_per_audio)
        
        labels = np.zeros(frames_per_audio, dtype=int)
        
        if trans_type == 'silence_to_silence':
            labels[:] = ALL_CLASSES.index('silence')
        elif trans_type == 'silence_to_factory':
            labels[:transition_frame_idx] = ALL_CLASSES.index('silence')
            labels[transition_frame_idx:] = ALL_CLASSES.index('factory')
        elif trans_type == 'silence_to_danger':
            labels[:transition_frame_idx] = ALL_CLASSES.index('silence')
            danger_class = np.random.choice(ACTIVE_DANGER_CLASSES)
            labels[transition_frame_idx:] = ALL_CLASSES.index(danger_class)
        elif trans_type == 'factory_to_factory':
            labels[:] = ALL_CLASSES.index('factory')
        elif trans_type == 'factory_to_danger':
            labels[:transition_frame_idx] = ALL_CLASSES.index('factory')
            danger_class = np.random.choice(ACTIVE_DANGER_CLASSES)
            labels[transition_frame_idx:] = ALL_CLASSES.index(danger_class)
        
        return result, labels
    
    def extract_yamnet_embeddings(self, audio):
        """YAMNet ì„ë² ë”© ì¶”ì¶œ"""
        try:
            if self.yamnet_model is None:
                print("âš ï¸ YAMNet ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
                
            audio_tensor = tf.convert_to_tensor(audio, dtype=tf.float32)
            # YAMNet ëª¨ë¸ í˜¸ì¶œ
            _, embeddings, _ = self.yamnet_model(audio_tensor)
            return embeddings.numpy()
        except Exception as e:
            print(f"âš ï¸ YAMNet ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def generate_transition_sequences(self, total_base_sequences):
        """ì „í™˜ ë°ì´í„° ì‹œí€€ìŠ¤ ìƒì„± - ì„¤ì • ë¹„ìœ¨ì— ë§ê²Œ"""
        transition_sequences = []
        transition_labels = []
        transition_frame_counts = []
        
        # ì „í™˜ ë°ì´í„° ì´ ê°œìˆ˜ ê³„ì‚° (ì„¤ì • ë¹„ìœ¨ ì ìš©)
        transition_ratio = DATA_GENERATION_CONFIG['transition_data_ratio']
        total_transition_sequences = int(total_base_sequences * transition_ratio / (1 - transition_ratio))
        
        print(f"ğŸ”„ ì „í™˜ ë°ì´í„° ìƒì„±:")
        print(f"  - ê¸°ë³¸ ì‹œí€€ìŠ¤: {total_base_sequences}ê°œ")
        print(f"  - ì „í™˜ ë¹„ìœ¨: {transition_ratio:.1%}")
        print(f"  - ì „í™˜ ì‹œí€€ìŠ¤ ëª©í‘œ: {total_transition_sequences}ê°œ")
        
        # í™œì„±í™”ëœ ì „í™˜ íƒ€ì…ë“¤ì˜ ì´ ê°€ì¤‘ì¹˜ ê³„ì‚°
        enabled_types = {k: v for k, v in TRANSITION_CONFIG['types'].items() if v.get('enabled', True)}
        total_weight = sum(config['weight'] for config in enabled_types.values())
        
        # ê° ì „í™˜ íƒ€ì…ë³„ ìƒì„± ê°œìˆ˜ ê³„ì‚°
        for trans_type, config in enabled_types.items():
            type_weight = config['weight']
            samples_for_type = int(total_transition_sequences * type_weight / total_weight)
            
            print(f"  ğŸ”„ {trans_type}: ê°€ì¤‘ì¹˜ {type_weight:.1f} â†’ {samples_for_type}ê°œ ìƒì„±")
            
            for i in range(samples_for_type):
                try:
                    # ì „í™˜ ì˜¤ë””ì˜¤ ìƒì„±
                    if 'silence_to_' in trans_type:
                        # ë¬´ìŒì—ì„œ ë‹¤ë¥¸ ì†Œë¦¬ë¡œ ì „í™˜
                        target_class = trans_type.replace('silence_to_', '')
                        if target_class == 'danger':
                            target_class = np.random.choice(ACTIVE_DANGER_CLASSES)
                        
                        transition_audio, labels = self.generate_transition_audio(trans_type, 'silence', target_class)
                        
                    elif '_to_silence' in trans_type:
                        # ë‹¤ë¥¸ ì†Œë¦¬ì—ì„œ ë¬´ìŒìœ¼ë¡œ ì „í™˜
                        source_class = trans_type.replace('_to_silence', '')
                        if source_class == 'danger':
                            source_class = np.random.choice(ACTIVE_DANGER_CLASSES)
                            
                        transition_audio, labels = self.generate_transition_audio(trans_type, source_class, 'silence')
                        
                    elif 'danger_to_danger' in trans_type:
                        # ìœ„í—˜ ì†Œë¦¬ ê°„ ì „í™˜
                        source_class = np.random.choice(ACTIVE_DANGER_CLASSES)
                        target_class = np.random.choice(ACTIVE_DANGER_CLASSES)
                        while target_class == source_class:
                            target_class = np.random.choice(ACTIVE_DANGER_CLASSES)
                            
                        transition_audio, labels = self.generate_transition_audio(trans_type, source_class, target_class)
                    
                    elif 'factory_to_factory' in trans_type:
                        # ê³µì¥ ì†Œë¦¬ ë‚´ ì „í™˜
                        transition_audio, labels = self.generate_transition_audio(trans_type, 'factory', 'factory')
                        
                    elif 'factory_to_danger' in trans_type:
                        # ê³µì¥ì—ì„œ ìœ„í—˜ ì†Œë¦¬ë¡œ ì „í™˜
                        target_class = np.random.choice(ACTIVE_DANGER_CLASSES)
                        transition_audio, labels = self.generate_transition_audio(trans_type, 'factory', target_class)
                    
                    else:
                        continue
                    
                    if transition_audio is not None:
                        embeddings = self.extract_yamnet_embeddings(transition_audio)
                        if embeddings is not None:
                            transition_sequences.append(embeddings)
                            
                            # ì „í™˜ ì§€ì  ê¸°ì¤€ìœ¼ë¡œ ë¼ë²¨ ê²°ì •
                            transition_point = len(labels) // 2
                            if len(labels) > transition_point:
                                # ì „í™˜ í›„ í´ë˜ìŠ¤ë¥¼ ì£¼ ë¼ë²¨ë¡œ ì‚¬ìš©
                                main_label = labels[transition_point:]
                                if len(main_label) > 0:
                                    label_counts = np.bincount(main_label)
                                    majority_label = np.argmax(label_counts)
                                else:
                                    majority_label = labels[-1] if len(labels) > 0 else 0
                            else:
                                majority_label = labels[-1] if len(labels) > 0 else 0
                            
                            transition_labels.append(majority_label)
                            transition_frame_counts.append(embeddings.shape[0])
                            
                except Exception as e:
                    print(f"    âš ï¸ ì „í™˜ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
                    continue
            
            print(f"    âœ… {trans_type}: ìƒì„± ì™„ë£Œ")
        
        actual_generated = len(transition_sequences)
        print(f"  ğŸ“Š ì‹¤ì œ ìƒì„±ëœ ì „í™˜ ì‹œí€€ìŠ¤: {actual_generated}ê°œ")
        print(f"  ğŸ“Š ëª©í‘œ ëŒ€ë¹„ ë‹¬ì„±ë¥ : {actual_generated/total_transition_sequences*100:.1f}%")
        
        return transition_sequences, transition_labels, transition_frame_counts
    
    def generate_sequence_dataset(self, samples_per_class):
        """ì‹œí€€ìŠ¤ ê¸°ë°˜ ë°ì´í„°ì…‹ ìƒì„± (ì˜¤ë””ì˜¤ë³„ ì™„ì „í•œ ì‹œí€€ìŠ¤)"""
        print("\nğŸ­ ì‹œí€€ìŠ¤ ê¸°ë°˜ ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...")
        
        all_sequences = []
        all_labels = []
        all_frame_counts = []  # ê° ì‹œí€€ìŠ¤ì˜ ìœ íš¨ í”„ë ˆì„ ìˆ˜ ì €ì¥
        sequence_lengths = []  # íŒ¨ë”© ì „ ì‹¤ì œ ê¸¸ì´ ì €ì¥
        
        dataset_info = {
            'config': {
                'model_version': MODEL_CONFIG['version'],
                'audio_duration': MODEL_CONFIG['audio_duration'],
                'sample_rate': MODEL_CONFIG['sample_rate'],
                'num_classes': NUM_CLASSES,
                'class_names': ALL_CLASSES,
                'generation_mode': 'full_sequence'
            },
            'generation_stats': {},
            'files_used': {},
            'frame_statistics': {}
        }
        
        # ê° í´ë˜ìŠ¤ë³„ ì‹œí€€ìŠ¤ ìƒì„±
        for class_name, target_samples in samples_per_class.items():
            if target_samples == 0:
                continue
                
            print(f"\nğŸ“ {class_name} í´ë˜ìŠ¤ ìƒì„± ì¤‘... (ëª©í‘œ: {target_samples:,}ê°œ ì‹œí€€ìŠ¤)")
            
            class_idx = ALL_CLASSES.index(class_name)
            collected_samples = 0
            files_used = []
            class_frame_counts = []
            
            while collected_samples < target_samples:
                if class_name == 'silence':
                    # ë¬´ìŒ ë°ì´í„° ìƒì„±
                    audio = self.generate_silence_audio(MODEL_CONFIG['audio_duration'])
                    
                    # ë°ì´í„° ì¦ê°• ì ìš©
                    if AUGMENTATION_CONFIG.get(class_name, {}).get('enabled', False):
                        if np.random.random() < 0.3:
                            methods = AUGMENTATION_CONFIG[class_name]['methods']
                            method = np.random.choice(methods)
                            audio = self.apply_augmentation(audio, class_name, method)
                    
                    embeddings = self.extract_yamnet_embeddings(audio)
                    if embeddings is not None:
                        # ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ í•˜ë‚˜ì˜ ìƒ˜í”Œë¡œ ì €ì¥
                        all_sequences.append(embeddings)
                        all_labels.append(class_idx)
                        class_frame_counts.append(embeddings.shape[0])
                        sequence_lengths.append(embeddings.shape[0])  # ì‹¤ì œ ê¸¸ì´ ì €ì¥
                        collected_samples += 1
                        files_used.append(f"silence_{collected_samples}")
                
                elif class_name == 'factory':
                    # ê³µì¥ ì†Œë¦¬ ì²˜ë¦¬
                    if not self.audio_files.get(class_name):
                        print(f"âš ï¸ {class_name} ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                        break
                    
                    audio_file = np.random.choice(self.audio_files[class_name])
                    audio = self.load_audio_segment(audio_file, duration=MODEL_CONFIG['audio_duration'])
                    
                    if audio is not None:
                        if AUGMENTATION_CONFIG.get(class_name, {}).get('enabled', False):
                            if np.random.random() < 0.4:
                                methods = AUGMENTATION_CONFIG[class_name]['methods']
                                method = np.random.choice(methods)
                                audio = self.apply_augmentation(audio, class_name, method)
                        
                        embeddings = self.extract_yamnet_embeddings(audio)
                        if embeddings is not None:
                            all_sequences.append(embeddings)
                            all_labels.append(class_idx)
                            class_frame_counts.append(embeddings.shape[0])
                            sequence_lengths.append(embeddings.shape[0])  # ì‹¤ì œ ê¸¸ì´ ì €ì¥
                            collected_samples += 1
                            files_used.append(os.path.basename(audio_file))
                
                else:
                    # ìœ„í—˜ ì†Œë¦¬ í´ë˜ìŠ¤ë“¤
                    if not self.audio_files.get(class_name):
                        print(f"âš ï¸ {class_name} ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                        break
                    
                    audio_file = np.random.choice(self.audio_files[class_name])
                    audio = self.load_audio_segment(audio_file, duration=MODEL_CONFIG['audio_duration'])
                    
                    if audio is not None:
                        if AUGMENTATION_CONFIG.get(class_name, {}).get('enabled', False):
                            if np.random.random() < 0.6:
                                methods = AUGMENTATION_CONFIG[class_name]['methods']
                                method = np.random.choice(methods)
                                audio = self.apply_augmentation(audio, class_name, method)
                        
                        embeddings = self.extract_yamnet_embeddings(audio)
                        if embeddings is not None:
                            all_sequences.append(embeddings)
                            all_labels.append(class_idx)
                            class_frame_counts.append(embeddings.shape[0])
                            sequence_lengths.append(embeddings.shape[0])  # ì‹¤ì œ ê¸¸ì´ ì €ì¥
                            collected_samples += 1
                            files_used.append(os.path.basename(audio_file))
                
                # ë¬´í•œ ë£¨í”„ ë°©ì§€
                if len(files_used) > target_samples * 3:
                    print(f"âš ï¸ {class_name}: ë„ˆë¬´ ë§ì€ ì‹œë„ í›„ ì¤‘ë‹¨")
                    break
            
            print(f"  âœ… {class_name}: {collected_samples:,}ê°œ ì‹œí€€ìŠ¤ ìƒì„±")
            
            # í´ë˜ìŠ¤ë³„ í”„ë ˆì„ í†µê³„
            if class_frame_counts:
                total_frames = sum(class_frame_counts)
                avg_frames = np.mean(class_frame_counts)
                dataset_info['frame_statistics'][class_name] = {
                    'total_frames': total_frames,
                    'avg_frames_per_sequence': avg_frames,
                    'sequences_count': len(class_frame_counts),
                    'min_frames': min(class_frame_counts),
                    'max_frames': max(class_frame_counts)
                }
                all_frame_counts.extend(class_frame_counts)
            
            dataset_info['generation_stats'][class_name] = {
                'target_samples': target_samples,
                'actual_samples': collected_samples,
                'files_used': len(set(files_used))
            }
            dataset_info['files_used'][class_name] = list(set(files_used))
        
        # ì „í™˜ ë°ì´í„° ìƒì„±
        if TRANSITION_CONFIG['enabled']:
            print(f"\nğŸ”„ ì „í™˜ ë°ì´í„° ìƒì„± ì¤‘...")
            
            # ê¸°ë³¸ ì‹œí€€ìŠ¤ ì´ ê°œìˆ˜ ê³„ì‚°
            total_base_sequences = sum(samples_per_class.values())
            
            # ì„¤ì • ë¹„ìœ¨ì— ë§ê²Œ ì „í™˜ ë°ì´í„° ìƒì„±
            transition_sequences, transition_labels, transition_frame_counts = self.generate_transition_sequences(total_base_sequences)
            
            if transition_sequences:
                all_sequences.extend(transition_sequences)
                all_labels.extend(transition_labels)
                all_frame_counts.extend(transition_frame_counts)
                sequence_lengths.extend(transition_frame_counts)  # ì „í™˜ ë°ì´í„°ì˜ ì‹¤ì œ ê¸¸ì´ë„ ì €ì¥
                
                # ì „í™˜ ë°ì´í„° í”„ë ˆì„ í†µê³„
                dataset_info['frame_statistics']['transitions'] = {
                    'total_frames': sum(transition_frame_counts),
                    'avg_frames_per_sequence': np.mean(transition_frame_counts),
                    'sequences_count': len(transition_frame_counts),
                    'min_frames': min(transition_frame_counts),
                    'max_frames': max(transition_frame_counts)
                }
                
                print(f"  âœ… ì „í™˜ ë°ì´í„°: {len(transition_sequences):,}ê°œ ì‹œí€€ìŠ¤ ìƒì„±")
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ í†µì¼ (íŒ¨ë”©/ìë¥´ê¸°)
        if all_sequences:
            print("\nğŸ”„ ì‹œí€€ìŠ¤ ê¸¸ì´ í†µì¼ ì¤‘...")
            
            # ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶„ì„
            seq_lengths = [seq.shape[0] for seq in all_sequences]
            max_length = max(seq_lengths)
            target_length = max_length  # ìµœëŒ€ ê¸¸ì´ë¡œ í†µì¼
            
            print(f"  ğŸ“ ì‹œí€€ìŠ¤ ê¸¸ì´ í†µê³„:")
            print(f"    - ìµœì†Œ: {min(seq_lengths)}, ìµœëŒ€: {max_length}, í‰ê· : {np.mean(seq_lengths):.1f}")
            print(f"    - ëª©í‘œ ê¸¸ì´: {target_length} í”„ë ˆì„")
            
            # ì‹œí€€ìŠ¤ ê¸¸ì´ í†µì¼ (ì œë¡œ íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°)
            unified_sequences = []
            for seq in all_sequences:
                if seq.shape[0] < target_length:
                    # ì œë¡œ íŒ¨ë”©
                    pad_length = target_length - seq.shape[0]
                    padded_seq = np.pad(seq, ((0, pad_length), (0, 0)), mode='constant', constant_values=0)
                    unified_sequences.append(padded_seq)
                elif seq.shape[0] > target_length:
                    # ìë¥´ê¸° (ì•ë¶€ë¶„ ì‚¬ìš©)
                    unified_sequences.append(seq[:target_length])
                else:
                    unified_sequences.append(seq)
            
            X = np.array(unified_sequences)  # (samples, time_steps, features)
            y = np.array(all_labels)         # (samples,)
            original_lengths = np.array(sequence_lengths)  # (samples,) - íŒ¨ë”© ì „ ì‹¤ì œ ê¸¸ì´
            
            # ì…”í”Œ (ëª¨ë“  ë°°ì—´ì„ ë™ì¼í•˜ê²Œ)
            shuffle_indices = np.random.permutation(len(X))
            X = X[shuffle_indices]
            y = y[shuffle_indices]
            original_lengths = original_lengths[shuffle_indices]
            
            print(f"\nğŸ“Š ìµœì¢… ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ í†µê³„:")
            print(f"  - ì´ ì‹œí€€ìŠ¤ ìˆ˜: {len(X):,}ê°œ")
            print(f"  - ì‹œí€€ìŠ¤ í˜•íƒœ: {X.shape}")
            print(f"  - ì‹œê°„ ìŠ¤í…: {X.shape[1]}")
            print(f"  - íŠ¹ì„± ìˆ˜: {X.shape[2]}")
            
            # í´ë˜ìŠ¤ë³„ ë¶„í¬ í™•ì¸
            actual_class_samples = {}
            class_frame_totals = {}
            
            for class_idx, class_name in enumerate(ALL_CLASSES):
                count = np.sum(y == class_idx)
                actual_class_samples[class_name] = count
                percentage = count / len(y) * 100
                
                # í•´ë‹¹ í´ë˜ìŠ¤ì˜ ì´ í”„ë ˆì„ ìˆ˜ ê³„ì‚°
                if class_name in dataset_info['frame_statistics']:
                    total_frames = dataset_info['frame_statistics'][class_name]['total_frames']
                    class_frame_totals[class_name] = total_frames
                else:
                    class_frame_totals[class_name] = 0
                
                print(f"  - {class_name}: {count:,}ê°œ ì‹œí€€ìŠ¤ ({percentage:.1f}%), {class_frame_totals[class_name]:,} í”„ë ˆì„")
            
            # ì „í™˜ ë°ì´í„° ë¹„ìœ¨ í†µê³„ ì¶”ê°€
            total_sequences = len(X)
            base_sequences = sum(samples_per_class.values())
            actual_transition_sequences = total_sequences - base_sequences
            actual_transition_ratio = actual_transition_sequences / total_sequences if total_sequences > 0 else 0
            target_transition_ratio = DATA_GENERATION_CONFIG['transition_data_ratio']
            
            print(f"\nğŸ“Š ìµœì¢… ë°ì´í„° êµ¬ì„± ë¹„ìœ¨:")
            print(f"  - ê¸°ë³¸ ì‹œí€€ìŠ¤: {base_sequences:,}ê°œ ({(1-actual_transition_ratio):.1%})")
            print(f"  - ì „í™˜ ì‹œí€€ìŠ¤: {actual_transition_sequences:,}ê°œ ({actual_transition_ratio:.1%})")
            print(f"  - ëª©í‘œ ì „í™˜ ë¹„ìœ¨: {target_transition_ratio:.1%}")
            print(f"  - ì‹¤ì œ ì „í™˜ ë¹„ìœ¨: {actual_transition_ratio:.1%}")
            
            dataset_info['final_stats'] = {
                'total_sequences': len(X),
                'sequence_shape': list(X.shape),
                'target_length': target_length,
                'class_distribution': actual_class_samples,
                'class_frame_totals': class_frame_totals,
                'sequence_length_stats': {
                    'min_length': int(np.min(original_lengths)),
                    'max_length': int(np.max(original_lengths)),
                    'mean_length': float(np.mean(original_lengths)),
                    'padded_length': target_length
                },
                'transition_data_stats': {
                    'target_ratio': target_transition_ratio,
                    'actual_ratio': actual_transition_ratio,
                    'base_sequences': base_sequences,
                    'transition_sequences': actual_transition_sequences,
                    'total_sequences': total_sequences
                }
            }
            
            return X, y, dataset_info, original_lengths
        else:
            print("âŒ ìƒì„±ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None, None
        
        all_sequences = []
        all_labels = []
        dataset_info = {
            'config': {
                'model_version': MODEL_CONFIG['version'],
                'audio_duration': MODEL_CONFIG['audio_duration'],
                'sample_rate': MODEL_CONFIG['sample_rate'],
                'num_classes': NUM_CLASSES,
                'class_names': ALL_CLASSES,
                'generation_mode': 'sequence_based'
            },
            'generation_stats': {},
            'files_used': {}
        }
        
        # ê° í´ë˜ìŠ¤ë³„ ì‹œí€€ìŠ¤ ìƒì„±
        for class_name, target_samples in target_samples_per_class.items():
            if target_samples == 0:
                continue
                
            print(f"\nğŸ“ {class_name} í´ë˜ìŠ¤ ìƒì„± ì¤‘... (ëª©í‘œ: {target_samples:,}ê°œ ì‹œí€€ìŠ¤)")
            
            class_idx = ALL_CLASSES.index(class_name)
            collected_samples = 0
            files_used = []
            
            while collected_samples < target_samples:
                if class_name == 'silence':
                    # ë¬´ìŒ ë°ì´í„° ìƒì„±
                    audio = self.generate_silence_audio(MODEL_CONFIG['audio_duration'])
                    
                    # ë°ì´í„° ì¦ê°• ì ìš©
                    if AUGMENTATION_CONFIG.get(class_name, {}).get('enabled', False):
                        if np.random.random() < 0.3:
                            methods = AUGMENTATION_CONFIG[class_name]['methods']
                            method = np.random.choice(methods)
                            audio = self.apply_augmentation(audio, class_name, method)
                    
                    embeddings = self.extract_yamnet_embeddings(audio)
                    if embeddings is not None:
                        # ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ í•˜ë‚˜ì˜ ìƒ˜í”Œë¡œ ì €ì¥
                        all_sequences.append(embeddings)
                        all_labels.append(class_idx)
                        collected_samples += 1
                        files_used.append(f"silence_{collected_samples}")
                
                elif class_name == 'factory':
                    # ê³µì¥ ì†Œë¦¬ ì²˜ë¦¬
                    if not self.audio_files.get(class_name):
                        print(f"âš ï¸ {class_name} ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                        break
                    
                    audio_file = np.random.choice(self.audio_files[class_name])
                    audio = self.load_audio_segment(audio_file, duration=MODEL_CONFIG['audio_duration'])
                    
                    if audio is not None:
                        if AUGMENTATION_CONFIG.get(class_name, {}).get('enabled', False):
                            if np.random.random() < 0.4:
                                methods = AUGMENTATION_CONFIG[class_name]['methods']
                                method = np.random.choice(methods)
                                audio = self.apply_augmentation(audio, class_name, method)
                        
                        embeddings = self.extract_yamnet_embeddings(audio)
                        if embeddings is not None:
                            all_sequences.append(embeddings)
                            all_labels.append(class_idx)
                            collected_samples += 1
                            files_used.append(os.path.basename(audio_file))
                
                else:
                    # ìœ„í—˜ ì†Œë¦¬ í´ë˜ìŠ¤ë“¤
                    if not self.audio_files.get(class_name):
                        print(f"âš ï¸ {class_name} ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                        break
                    
                    audio_file = np.random.choice(self.audio_files[class_name])
                    audio = self.load_audio_segment(audio_file, duration=MODEL_CONFIG['audio_duration'])
                    
                    if audio is not None:
                        if AUGMENTATION_CONFIG.get(class_name, {}).get('enabled', False):
                            if np.random.random() < 0.6:
                                methods = AUGMENTATION_CONFIG[class_name]['methods']
                                method = np.random.choice(methods)
                                audio = self.apply_augmentation(audio, class_name, method)
                        
                        embeddings = self.extract_yamnet_embeddings(audio)
                        if embeddings is not None:
                            all_sequences.append(embeddings)
                            all_labels.append(class_idx)
                            collected_samples += 1
                            files_used.append(os.path.basename(audio_file))
                
                # ë¬´í•œ ë£¨í”„ ë°©ì§€
                if len(files_used) > target_samples * 3:
                    print(f"âš ï¸ {class_name}: ë„ˆë¬´ ë§ì€ ì‹œë„ í›„ ì¤‘ë‹¨")
                    break
            
            print(f"  âœ… {class_name}: {collected_samples:,}ê°œ ì‹œí€€ìŠ¤ ìƒì„±")
            dataset_info['generation_stats'][class_name] = {
                'target_samples': target_samples,
                'actual_samples': collected_samples,
                'files_used': len(set(files_used))
            }
            dataset_info['files_used'][class_name] = list(set(files_used))
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ í†µì¼ (íŒ¨ë”©/ìë¥´ê¸°)
        if all_sequences:
            print("\nğŸ”„ ì‹œí€€ìŠ¤ ê¸¸ì´ í†µì¼ ì¤‘...")
            
            # ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶„ì„
            seq_lengths = [seq.shape[0] for seq in all_sequences]
            max_length = max(seq_lengths)
            min_length = min(seq_lengths)
            avg_length = np.mean(seq_lengths)
            
            print(f"  ğŸ“ ì‹œí€€ìŠ¤ ê¸¸ì´ - ìµœì†Œ: {min_length}, ìµœëŒ€: {max_length}, í‰ê· : {avg_length:.1f}")
            
            # ëª©í‘œ ê¸¸ì´ ì„¤ì • (í‰ê·  ë˜ëŠ” ê°€ì¥ ì¼ë°˜ì ì¸ ê¸¸ì´)
            target_length = int(np.percentile(seq_lengths, 75))  # 75% ì§€ì  ì‚¬ìš©
            print(f"  ğŸ¯ ëª©í‘œ ê¸¸ì´: {target_length} í”„ë ˆì„")
            
            # ì‹œí€€ìŠ¤ ê¸¸ì´ í†µì¼
            unified_sequences = []
            for seq in all_sequences:
                if seq.shape[0] < target_length:
                    # íŒ¨ë”© (ì œë¡œ íŒ¨ë”©)
                    pad_length = target_length - seq.shape[0]
                    padded_seq = np.pad(seq, ((0, pad_length), (0, 0)), mode='constant')
                    unified_sequences.append(padded_seq)
                elif seq.shape[0] > target_length:
                    # ìë¥´ê¸° (ì•ë¶€ë¶„ ì‚¬ìš©)
                    unified_sequences.append(seq[:target_length])
                else:
                    unified_sequences.append(seq)
            
            X = np.array(unified_sequences)  # (samples, time_steps, features)
            y = np.array(all_labels)         # (samples,)
            
            # ì…”í”Œ
            X, y = shuffle(X, y, random_state=42)
            
            print(f"\nğŸ“Š ìµœì¢… ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ í†µê³„:")
            print(f"  - ì´ ì‹œí€€ìŠ¤ ìˆ˜: {len(X):,}ê°œ")
            print(f"  - ì‹œí€€ìŠ¤ í˜•íƒœ: {X.shape}")
            print(f"  - ì‹œê°„ ìŠ¤í…: {X.shape[1]}")
            print(f"  - íŠ¹ì„± ìˆ˜: {X.shape[2]}")
            
            # í´ë˜ìŠ¤ë³„ ë¶„í¬ í™•ì¸
            actual_class_samples = {}
            for class_idx, class_name in enumerate(ALL_CLASSES):
                count = np.sum(y == class_idx)
                actual_class_samples[class_name] = count
                percentage = count / len(y) * 100
                print(f"  - {class_name}: {count:,}ê°œ ({percentage:.1f}%)")
            
            dataset_info['final_stats'] = {
                'total_sequences': len(X),
                'sequence_shape': list(X.shape),
                'target_length': target_length,
                'class_distribution': actual_class_samples
            }
            
            return X, y, dataset_info
        else:
            print("âŒ ìƒì„±ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None, None
        """ëª©í‘œ í”„ë ˆì„ ìˆ˜ì— ë§ì¶° ì •í™•í•œ ë°ì´í„°ì…‹ ìƒì„±"""
        print("\nğŸ­ í”„ë ˆì„ ê¸°ë°˜ ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...")
        
        all_embeddings = []
        all_labels = []
        dataset_info = {
            'config': {
                'model_version': MODEL_CONFIG['version'],
                'audio_duration': MODEL_CONFIG['audio_duration'],
                'sample_rate': MODEL_CONFIG['sample_rate'],
                'num_classes': NUM_CLASSES,
                'class_names': ALL_CLASSES,
                'generation_mode': 'frame_based'
            },
            'generation_stats': {},
            'files_used': {}
        }
        
        total_target_frames = sum(target_frames_per_class.values())
        
        # ê° í´ë˜ìŠ¤ë³„ ì •í™•í•œ í”„ë ˆì„ ìˆ˜ ìƒì„±
        for class_name, target_frames in target_frames_per_class.items():
            if target_frames == 0:
                continue
                
            print(f"\nğŸ“ {class_name} í´ë˜ìŠ¤ ìƒì„± ì¤‘... (ëª©í‘œ: {target_frames:,}ê°œ í”„ë ˆì„)")
            
            class_idx = ALL_CLASSES.index(class_name)
            collected_frames = 0
            generated_samples = 0
            files_used = []
            
            # í”„ë ˆì„ ìˆ˜ì§‘ ë£¨í”„
            while collected_frames < target_frames:
                frames_needed = target_frames - collected_frames
                
                if class_name == 'silence':
                    # ë¬´ìŒ ë°ì´í„° ìƒì„±
                    audio = self.generate_silence_audio(MODEL_CONFIG['audio_duration'])
                    
                    # ë°ì´í„° ì¦ê°• ì ìš© (í™•ë¥ ì )
                    if AUGMENTATION_CONFIG.get(class_name, {}).get('enabled', False):
                        if np.random.random() < 0.5:  # 50% í™•ë¥ ë¡œ ì¦ê°•
                            methods = AUGMENTATION_CONFIG[class_name]['methods']
                            method = np.random.choice(methods)
                            audio = self.apply_augmentation(audio, class_name, method)
                    
                    embeddings = self.extract_yamnet_embeddings(audio)
                    if embeddings is not None:
                        available_frames = embeddings.shape[0]
                        frames_to_use = min(available_frames, frames_needed)
                        
                        # í•„ìš”í•œ í”„ë ˆì„ë§Œ ì„ íƒ
                        selected_embeddings = embeddings[:frames_to_use]
                        selected_labels = np.full(frames_to_use, class_idx, dtype=int)
                        
                        all_embeddings.extend(selected_embeddings)
                        all_labels.extend(selected_labels)
                        
                        collected_frames += frames_to_use
                        generated_samples += 1
                        files_used.append(f"generated_silence_{generated_samples}")
                        
                        print(f"\r  ì§„í–‰ë¥ : {collected_frames:,}/{target_frames:,} "
                              f"({collected_frames/target_frames*100:.1f}%)", end='')
                        
                else:
                    # ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ ì‚¬ìš©
                    if class_name not in self.audio_files or not self.audio_files[class_name]:
                        print(f"âš ï¸ {class_name} í´ë˜ìŠ¤ì˜ ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                        break
                    
                    # ëœë¤í•˜ê²Œ íŒŒì¼ ì„ íƒ
                    file_path = np.random.choice(self.audio_files[class_name])
                    audio = self.load_audio_file(file_path)
                    
                    if audio is None:
                        continue
                    
                    # ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
                    segment = self.extract_audio_segment(audio, MODEL_CONFIG['audio_duration'])
                    
                    # ë°ì´í„° ì¦ê°• ì ìš© (í•„ìš”ì‹œ)
                    if AUGMENTATION_CONFIG.get(class_name, {}).get('enabled', False):
                        # ìˆ˜ì§‘ëœ í”„ë ˆì„ì´ ëª©í‘œì˜ 50%ë¥¼ ë„˜ìœ¼ë©´ ì¦ê°• ì ìš©
                        if collected_frames > target_frames * 0.5:
                            methods = AUGMENTATION_CONFIG[class_name]['methods']
                            method = np.random.choice(methods)
                            segment = self.apply_augmentation(segment, class_name, method)
                    
                    embeddings = self.extract_yamnet_embeddings(segment)
                    if embeddings is not None:
                        available_frames = embeddings.shape[0]
                        frames_to_use = min(available_frames, frames_needed)
                        
                        # í•„ìš”í•œ í”„ë ˆì„ë§Œ ì„ íƒ
                        selected_embeddings = embeddings[:frames_to_use]
                        selected_labels = np.full(frames_to_use, class_idx, dtype=int)
                        
                        all_embeddings.extend(selected_embeddings)
                        all_labels.extend(selected_labels)
                        
                        collected_frames += frames_to_use
                        generated_samples += 1
                        files_used.append(f"{os.path.basename(file_path)}_seg{generated_samples}")
                        
                        print(f"\r  ì§„í–‰ë¥ : {collected_frames:,}/{target_frames:,} "
                              f"({collected_frames/target_frames*100:.1f}%)", end='')
            
            print()  # ì¤„ë°”ê¿ˆ
            
            # í´ë˜ìŠ¤ë³„ í†µê³„ ì €ì¥
            dataset_info['generation_stats'][class_name] = {
                'target_frames': target_frames,
                'actual_frames': collected_frames,
                'samples_generated': generated_samples,
                'accuracy': collected_frames / target_frames if target_frames > 0 else 0
            }
            dataset_info['files_used'][class_name] = files_used
            
            print(f"  âœ… {class_name}: {collected_frames:,}/{target_frames:,} í”„ë ˆì„ "
                  f"({collected_frames/target_frames*100:.1f}%)")
        
        # ì „í™˜ ë°ì´í„° ìƒì„± (ì„ íƒì )
        if TRANSITION_CONFIG['enabled']:
            print(f"\nğŸ”„ ì „í™˜ ë°ì´í„° ìƒì„± ì¤‘...")
            
            transition_ratio = DATA_GENERATION_CONFIG['transition_data_ratio']
            base_frames = len(all_embeddings)
            transition_frames_needed = int(base_frames * transition_ratio / (1 - transition_ratio))
            
            transition_collected = 0
            transition_files = []
            
            for trans_type, config in TRANSITION_CONFIG['types'].items():
                if not config['enabled']:
                    continue
                    
                type_frames_needed = int(transition_frames_needed * config['weight'] / 
                                       sum(c['weight'] for c in TRANSITION_CONFIG['types'].values() if c['enabled']))
                
                type_collected = 0
                sample_count = 0
                
                while type_collected < type_frames_needed:
                    frames_needed = type_frames_needed - type_collected
                    
                    audio, labels = self.generate_transition_audio(trans_type, None, None)
                    embeddings = self.extract_yamnet_embeddings(audio)
                    
                    if embeddings is not None:
                        available_frames = embeddings.shape[0]
                        frames_to_use = min(available_frames, frames_needed)
                        
                        # ë¼ë²¨ ê¸¸ì´ ì¡°ì •
                        if len(labels) != embeddings.shape[0]:
                            if len(labels) < embeddings.shape[0]:
                                labels = np.pad(labels, (0, embeddings.shape[0] - len(labels)), 
                                              mode='edge')
                            else:
                                labels = labels[:embeddings.shape[0]]
                        
                        # í•„ìš”í•œ í”„ë ˆì„ë§Œ ì„ íƒ
                        selected_embeddings = embeddings[:frames_to_use]
                        selected_labels = labels[:frames_to_use]
                        
                        all_embeddings.extend(selected_embeddings)
                        all_labels.extend(selected_labels)
                        
                        type_collected += frames_to_use
                        transition_collected += frames_to_use
                        sample_count += 1
                        transition_files.append(f"{trans_type}_{sample_count}")
                
                print(f"  - {trans_type}: {type_collected:,}ê°œ í”„ë ˆì„")
            
            dataset_info['generation_stats']['transitions'] = {
                'target_frames': transition_frames_needed,
                'actual_frames': transition_collected,
                'samples_generated': len(transition_files),
                'by_type': {}
            }
            
            # ì „í™˜ íƒ€ì…ë³„ í†µê³„
            for trans_type in TRANSITION_CONFIG['types']:
                type_count = sum(1 for f in transition_files if f.startswith(trans_type))
                if type_count > 0:
                    dataset_info['generation_stats']['transitions']['by_type'][trans_type] = type_count
        
        # ë°ì´í„° ì •ë¦¬
        print("\nğŸ”„ ë°ì´í„° ì •ë¦¬ ì¤‘...")
        
        if all_embeddings:
            X = np.array(all_embeddings)
            y = np.array(all_labels)
            
            # ì…”í”Œ
            X, y = shuffle(X, y, random_state=42)
            
            # ìµœì¢… í†µê³„
            print(f"\nğŸ“Š ìµœì¢… ë°ì´í„°ì…‹ í†µê³„:")
            print(f"  - ì´ í”„ë ˆì„ ìˆ˜: {len(X):,}ê°œ")
            
            # í´ë˜ìŠ¤ë³„ ì‹¤ì œ í”„ë ˆì„ ìˆ˜ í™•ì¸
            actual_class_frames = {}
            for class_idx, class_name in enumerate(ALL_CLASSES):
                count = np.sum(y == class_idx)
                percentage = count / len(y) * 100
                actual_class_frames[class_name] = count
                target = target_frames_per_class.get(class_name, 0)
                accuracy = (count / target * 100) if target > 0 else 0
                print(f"  - {class_name}: {count:,}ê°œ ({percentage:.1f}%) "
                      f"[ëª©í‘œ: {target:,}, ë‹¬ì„±ë¥ : {accuracy:.1f}%]")
            
            # ê· ë“±ì„± ê²€ì‚¬
            class_frame_counts = [actual_class_frames[name] for name in ALL_CLASSES 
                                if target_frames_per_class.get(name, 0) > 0]
            if class_frame_counts:
                max_diff = max(class_frame_counts) - min(class_frame_counts)
                if max_diff == 0:
                    print("âœ… ì™„ë²½í•œ í´ë˜ìŠ¤ ê· í˜• ë‹¬ì„±!")
                else:
                    print(f"ğŸ“Š í´ë˜ìŠ¤ê°„ ìµœëŒ€ í”„ë ˆì„ ì°¨ì´: {max_diff:,}ê°œ")
            
            dataset_info['final_stats'] = {
                'total_frames': len(X),
                'class_distribution': actual_class_frames,
                'target_vs_actual': {
                    name: {
                        'target': target_frames_per_class.get(name, 0),
                        'actual': actual_class_frames.get(name, 0),
                        'accuracy': (actual_class_frames.get(name, 0) / target_frames_per_class.get(name, 1) * 100) 
                                  if target_frames_per_class.get(name, 0) > 0 else 0
                    }
                    for name in ALL_CLASSES
                }
            }
            
            return X, y, dataset_info
        else:
            print("âŒ ìƒì„±ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None, None

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ YAMNet + LSTM ì‹œí€€ìŠ¤ ê¸°ë°˜ ë°ì´í„° ìƒì„±ê¸°")
    print("=" * 60)
    
    # ì„¤ì • ê²€ì¦
    errors = validate_config()
    if errors:
        print("âŒ ì„¤ì • ì˜¤ë¥˜:")
        for error in errors:
            print(f"  - {error}")
        return
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    create_output_directories()
    
    # ë°ì´í„° ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = DataGenerator()
    
    # ì‹œí€€ìŠ¤ ê¸°ë°˜ ë°ì´í„° ìƒì„±ì„ ìœ„í•œ ìƒ˜í”Œ ìˆ˜ ì…ë ¥
    print("\nğŸ“Š ì‹œí€€ìŠ¤ ê¸°ë°˜ ë°ì´í„° ìƒì„± ì„¤ì •")
    print("ê° í´ë˜ìŠ¤ë³„ë¡œ ìƒì„±í•  ì‹œí€€ìŠ¤ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("(í•œ ì‹œí€€ìŠ¤ëŠ” í•˜ë‚˜ì˜ ì™„ì „í•œ ì˜¤ë””ì˜¤ íŒŒì¼ì…ë‹ˆë‹¤)")
    
    samples_per_class = {}
    
    # ê¸°ë³¸ ì‹œí€€ìŠ¤ ìˆ˜ ì„¤ì •
    default_samples = {
        'silence': 500,
        'factory': 300,
        'fire': 200,
        'gas': 200,
        'scream': 200
    }
    
    print(f"\nğŸ¯ ê¶Œì¥ ì‹œí€€ìŠ¤ ìˆ˜:")
    for class_name in ALL_CLASSES:
        recommended = default_samples.get(class_name, 200)
        print(f"  - {class_name}: {recommended}ê°œ")
    
    print(f"\nâš™ï¸ ì‹œí€€ìŠ¤ ìˆ˜ ì„¤ì •:")
    use_defaults = input("ê¶Œì¥ ì„¤ì •ì„ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ").strip().lower()
    
    if use_defaults in ['', 'y', 'yes']:
        samples_per_class = default_samples.copy()
        print("âœ… ê¶Œì¥ ì„¤ì • ì ìš©ë¨")
    else:
        for class_name in ALL_CLASSES:
            while True:
                try:
                    default_val = default_samples.get(class_name, 200)
                    user_input = input(f"{class_name} ì‹œí€€ìŠ¤ ìˆ˜ (ê¸°ë³¸ê°’: {default_val}): ").strip()
                    if not user_input:
                        samples_per_class[class_name] = default_val
                    else:
                        samples_per_class[class_name] = max(0, int(user_input))
                    break
                except ValueError:
                    print("âš ï¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    # ì´ ì‹œí€€ìŠ¤ ìˆ˜ í™•ì¸
    total_sequences = sum(samples_per_class.values())
    print(f"\nğŸ“Š ì„¤ì •ëœ ì‹œí€€ìŠ¤ ìˆ˜:")
    for class_name, count in samples_per_class.items():
        percentage = (count / total_sequences) * 100 if total_sequences > 0 else 0
        print(f"  - {class_name}: {count:,}ê°œ ({percentage:.1f}%)")
    print(f"  ì´ {total_sequences:,}ê°œ ì‹œí€€ìŠ¤ ìƒì„± ì˜ˆì •")
    
    confirm = input("\nê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ").strip().lower()
    if confirm not in ['', 'y', 'yes']:
        print("ğŸ‘‹ ì¤‘ë‹¨ë¨")
        return
    
    # ì‹œí€€ìŠ¤ ê¸°ë°˜ ë°ì´í„°ì…‹ ìƒì„±
    result = generator.generate_sequence_dataset(samples_per_class)
    
    if len(result) == 4:
        X, y, dataset_info, original_lengths = result
    else:
        # ì´ì „ ë²„ì „ í˜¸í™˜ì„±
        X, y, dataset_info = result
        original_lengths = None
    
    if X is not None:
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
        print("\nâš–ï¸ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ì¤‘...")
        class_weights = {}
        
        # ê° í´ë˜ìŠ¤ë³„ ì‹¤ì œ í”„ë ˆì„ ìˆ˜ ê³„ì‚° (íŒ¨ë”© ì œì™¸)
        class_frame_counts = {class_name: 0 for class_name in ALL_CLASSES}
        
        for i, (label, length) in enumerate(zip(y, original_lengths)):
            class_name = ALL_CLASSES[label]
            class_frame_counts[class_name] += length  # ì‹¤ì œ ê¸¸ì´ë§Œ ë”í•¨
        
        # ì „ì²´ ì‹¤ì œ í”„ë ˆì„ ìˆ˜
        total_frames = sum(class_frame_counts.values())
        
        for class_idx, class_name in enumerate(ALL_CLASSES):
            frames = class_frame_counts[class_name]
            if frames > 0:
                # ì‹¤ì œ í”„ë ˆì„ ìˆ˜ì— ë°˜ë¹„ë¡€í•˜ëŠ” ê°€ì¤‘ì¹˜
                weight = total_frames / (len(ALL_CLASSES) * frames)
            else:
                weight = 1.0
            
            class_weights[class_idx] = weight
            print(f"  ğŸ“Š {class_name}: {frames:,} í”„ë ˆì„ (ì‹¤ì œ), ê°€ì¤‘ì¹˜: {weight:.3f}")
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì •ë³´ë¥¼ ë°ì´í„°ì…‹ ì •ë³´ì— ì¶”ê°€
        dataset_info['class_weights'] = {str(k): float(v) for k, v in class_weights.items()}
        dataset_info['actual_frame_counts'] = {class_name: int(count) for class_name, count in class_frame_counts.items()}
        
        # 3-way ë°ì´í„° ë¶„í•  (train/validation/test)
        print("\nğŸ”„ ë°ì´í„°ë¥¼ train/validation/testë¡œ ë¶„í•  ì¤‘...")
        if original_lengths is not None:
            X_train, X_val, X_test, y_train, y_val, y_test, lengths_train, lengths_val, lengths_test = split_dataset_3way_with_lengths(X, y, original_lengths)
        else:
            X_train, X_val, X_test, y_train, y_val, y_test = split_dataset_3way(X, y)
            lengths_train = lengths_val = lengths_test = None
        
        # ë¶„í• ëœ ë°ì´í„°ì…‹ ì €ì¥
        if lengths_train is not None:
            split_paths = save_dataset_splits_with_lengths(X_train, X_val, X_test, y_train, y_val, y_test, 
                                                          lengths_train, lengths_val, lengths_test)
        else:
            split_paths = save_dataset_splits(X_train, X_val, X_test, y_train, y_val, y_test)
        
        # ë¶„í•  í›„ í´ë˜ìŠ¤ë³„ í†µê³„
        print(f"\nğŸ“Š ë¶„í• ëœ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ë³„ í†µê³„:")
        for split_name, split_data in [('Train', y_train), ('Validation', y_val), ('Test', y_test)]:
            print(f"\n{split_name} ì…‹:")
            for class_idx, class_name in enumerate(ALL_CLASSES):
                count = np.sum(split_data == class_idx)
                percentage = count / len(split_data) * 100 if len(split_data) > 0 else 0
                print(f"  - {class_name}: {count:,}ê°œ ({percentage:.1f}%)")
        
        # ë°ì´í„°ì…‹ ì •ë³´ ì €ì¥ (JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜)
        dataset_path = get_dataset_save_path()
        
        # NumPy íƒ€ì…ë“¤ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        def convert_numpy_types(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            return obj
        
        dataset_info = convert_numpy_types(dataset_info)
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì •ë³´ ì¶”ê°€
        dataset_info['class_weights'] = {str(k): float(v) for k, v in class_weights.items()}
        
        dataset_info['split_info'] = {
            'train_samples': len(X_train),
            'validation_samples': len(X_val),
            'test_samples': len(X_test),
            'train_ratio': TRAINING_CONFIG['data_split']['train_ratio'],
            'validation_ratio': TRAINING_CONFIG['data_split']['validation_ratio'],
            'test_ratio': TRAINING_CONFIG['data_split']['test_ratio'],
            'split_files': split_paths,
            'split_class_distribution': {
                'train': {class_name: int(np.sum(y_train == class_idx)) 
                         for class_idx, class_name in enumerate(ALL_CLASSES)},
                'validation': {class_name: int(np.sum(y_val == class_idx)) 
                              for class_idx, class_name in enumerate(ALL_CLASSES)},
                'test': {class_name: int(np.sum(y_test == class_idx)) 
                        for class_idx, class_name in enumerate(ALL_CLASSES)}
            }
        }
        
        with open(dataset_path, 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ë°ì´í„°ì…‹ ì •ë³´ ì €ì¥: {dataset_path}")
        
        # ë¶„í• ëœ ë°ì´í„°ì…‹ í†µê³„ë¥¼ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
        split_stats_path = dataset_path.replace('dataset_info_', 'split_statistics_')
        
        # ë¶„í•  í†µê³„ ìƒì„¸ ì •ë³´ ìƒì„±
        split_statistics = {
            'metadata': {
                'creation_time': datetime.now().isoformat(),
                'model_version': MODEL_CONFIG['version'],
                'total_samples': len(X),
                'split_ratios': {
                    'train': TRAINING_CONFIG['data_split']['train_ratio'],
                    'validation': TRAINING_CONFIG['data_split']['validation_ratio'],
                    'test': TRAINING_CONFIG['data_split']['test_ratio']
                }
            },
            'split_summary': {
                'train': {
                    'total_samples': len(X_train),
                    'percentage': len(X_train) / len(X) * 100
                },
                'validation': {
                    'total_samples': len(X_val),
                    'percentage': len(X_val) / len(X) * 100
                },
                'test': {
                    'total_samples': len(X_test),
                    'percentage': len(X_test) / len(X) * 100
                }
            },
            'class_distribution': {
                'train': {},
                'validation': {},
                'test': {}
            },
            'class_statistics': {}
        }
        
        # ê° ë¶„í• ë³„ í´ë˜ìŠ¤ ë¶„í¬ ê³„ì‚°
        for split_name, y_split in [('train', y_train), ('validation', y_val), ('test', y_test)]:
            total_split_samples = len(y_split)
            
            for class_idx, class_name in enumerate(ALL_CLASSES):
                class_count = int(np.sum(y_split == class_idx))
                class_percentage = (class_count / total_split_samples) * 100 if total_split_samples > 0 else 0
                
                split_statistics['class_distribution'][split_name][class_name] = {
                    'count': class_count,
                    'percentage': round(class_percentage, 1)
                }
        
        # í´ë˜ìŠ¤ë³„ ì „ì²´ í†µê³„
        for class_idx, class_name in enumerate(ALL_CLASSES):
            train_count = int(np.sum(y_train == class_idx))
            val_count = int(np.sum(y_val == class_idx))
            test_count = int(np.sum(y_test == class_idx))
            total_class_count = train_count + val_count + test_count
            
            split_statistics['class_statistics'][class_name] = {
                'total_samples': total_class_count,
                'train': {
                    'count': train_count,
                    'percentage_of_class': round((train_count / total_class_count) * 100, 1) if total_class_count > 0 else 0,
                    'percentage_of_split': round((train_count / len(X_train)) * 100, 1) if len(X_train) > 0 else 0
                },
                'validation': {
                    'count': val_count,
                    'percentage_of_class': round((val_count / total_class_count) * 100, 1) if total_class_count > 0 else 0,
                    'percentage_of_split': round((val_count / len(X_val)) * 100, 1) if len(X_val) > 0 else 0
                },
                'test': {
                    'count': test_count,
                    'percentage_of_class': round((test_count / total_class_count) * 100, 1) if total_class_count > 0 else 0,
                    'percentage_of_split': round((test_count / len(X_test)) * 100, 1) if len(X_test) > 0 else 0
                }
            }
        
        # ë¶„í•  í†µê³„ íŒŒì¼ ì €ì¥
        with open(split_stats_path, 'w', encoding='utf-8') as f:
            json.dump(split_statistics, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š ë¶„í•  í†µê³„ ì €ì¥: {split_stats_path}")
        
        # ì „ì²´ ë°ì´í„°ë„ ì €ì¥ (í˜¸í™˜ì„± ìœ ì§€)
        data_path = dataset_path.replace('.json', '.npz')
        np.savez_compressed(data_path, X=X, y=y)
        print(f"ğŸ’¾ ì „ì²´ ë°ì´í„° ì €ì¥: {data_path}")
        
        # ìµœì¢… ìš”ì•½
        print(f"\nâœ… í”„ë ˆì„ ê¸°ë°˜ ë°ì´í„° ìƒì„± ë° ë¶„í•  ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ í”„ë ˆì„: {len(X):,}ê°œ")
        print(f"ğŸ“‚ Train: {len(X_train):,}ê°œ ({len(X_train)/len(X)*100:.1f}%)")
        print(f"ğŸ“‚ Validation: {len(X_val):,}ê°œ ({len(X_val)/len(X)*100:.1f}%)")
        print(f"ğŸ“‚ Test: {len(X_test):,}ê°œ ({len(X_test)/len(X)*100:.1f}%)")
        
        return data_path, dataset_path, split_paths
    else:
        print("\nâŒ ë°ì´í„° ìƒì„± ì‹¤íŒ¨")
        return None, None, None

if __name__ == "__main__":
    main()
