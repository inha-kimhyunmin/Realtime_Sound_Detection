"""
ë°ì´í„° ìƒì„± ëª¨ë“ˆ
===============

ì´ ëª¨ë“ˆì€ YAMNet + LSTM í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
- ê° í´ë˜ìŠ¤ë³„ ê· ë“±í•œ í”„ë ˆì„ ìˆ˜ ìƒì„±
- ë°ì´í„° ì¦ê°•ì„ í†µí•œ ë°ì´í„° ë¶€ì¡± í•´ê²°
- ë‹¤ì–‘í•œ ì „í™˜ ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° ìƒì„±
- í”„ë ˆì„ ë ˆë²¨ ë¼ë²¨ë§
"""

import os
import json
import numpy as np
import librosa
import soundfile as sf
from scipy import signal
from sklearn.utils import shuffle
import tensorflow as tf
import tensorflow_hub as hub
from tqdm import tqdm
import matplotlib.pyplot as plt
from config import *

class DataGenerator:
    def __init__(self):
        """ë°ì´í„° ìƒì„±ê¸° ì´ˆê¸°í™”"""
        self.yamnet_model = None
        self.audio_files = {}
        self.audio_durations = {}
        self.total_frames_available = {}
        self.load_yamnet()
        self.scan_audio_files()
        
    def load_yamnet(self):
        """YAMNet ëª¨ë¸ ë¡œë“œ"""
        print("ğŸ”„ YAMNet ëª¨ë¸ ë¡œë”© ì¤‘...")
        yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'
        self.yamnet_model = hub.load(yamnet_model_handle)
        print("âœ… YAMNet ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
    def scan_audio_files(self):
        """ì˜¤ë””ì˜¤ íŒŒì¼ ìŠ¤ìº” ë° í”„ë ˆì„ ìˆ˜ ê³„ì‚°"""
        print("ğŸ” ì˜¤ë””ì˜¤ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
        
        # ìœ„í—˜ ì†Œë¦¬ íŒŒì¼ ìŠ¤ìº”
        for class_name in ACTIVE_DANGER_CLASSES:
            class_dir = os.path.join(ENVSOUND_DIR, class_name)
            if not os.path.exists(class_dir):
                print(f"âš ï¸ ê²½ê³ : {class_dir} í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                continue
                
            files = [f for f in os.listdir(class_dir) if f.endswith(('.wav', '.mp3', '.flac'))]
            self.audio_files[class_name] = [os.path.join(class_dir, f) for f in files]
            
            # ê° íŒŒì¼ì˜ ê¸¸ì´ ê³„ì‚°
            total_duration = 0
            durations = []
            for file_path in self.audio_files[class_name]:
                try:
                    duration = librosa.get_duration(filename=file_path)
                    durations.append(duration)
                    total_duration += duration
                except Exception as e:
                    print(f"âš ï¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {file_path}: {e}")
                    
            self.audio_durations[class_name] = durations
            # YAMNet í”„ë ˆì„ ìˆ˜ ê³„ì‚° (0.48ì´ˆë‹¹ 1í”„ë ˆì„)
            self.total_frames_available[class_name] = int(total_duration / 0.48)
            
            print(f"  ğŸ“ {class_name}: {len(files)}ê°œ íŒŒì¼, {total_duration:.1f}ì´ˆ, {self.total_frames_available[class_name]}ê°œ í”„ë ˆì„")
        
        # ê³µì¥ ì†Œë¦¬ íŒŒì¼ ìŠ¤ìº”
        if os.path.exists(MIXTURE_DIR):
            files = [f for f in os.listdir(MIXTURE_DIR) if f.endswith(('.wav', '.mp3', '.flac'))]
            self.audio_files['factory'] = [os.path.join(MIXTURE_DIR, f) for f in files]
            
            total_duration = 0
            durations = []
            for file_path in self.audio_files['factory']:
                try:
                    duration = librosa.get_duration(filename=file_path)
                    durations.append(duration)
                    total_duration += duration
                except Exception as e:
                    print(f"âš ï¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {file_path}: {e}")
                    
            self.audio_durations['factory'] = durations
            self.total_frames_available['factory'] = int(total_duration / 0.48)
            
            print(f"  ğŸ“ factory: {len(files)}ê°œ íŒŒì¼, {total_duration:.1f}ì´ˆ, {self.total_frames_available['factory']}ê°œ í”„ë ˆì„")
        
        # ë¬´ìŒì€ ë¬´ì œí•œìœ¼ë¡œ ìƒì„± ê°€ëŠ¥
        self.total_frames_available['silence'] = 999999
        print(f"  ğŸ“ silence: ë¬´ì œí•œ ìƒì„± ê°€ëŠ¥")
        
    def calculate_optimal_samples(self):
        """í´ë˜ìŠ¤ë³„ ìµœì  ìƒ˜í”Œ ìˆ˜ ê³„ì‚°"""
        target_frames = DATA_GENERATION_CONFIG['target_frames_per_class']
        frames_per_audio = get_audio_frames_count()
        
        print("\nğŸ“Š í´ë˜ìŠ¤ë³„ ë°ì´í„° ë¶„ì„:")
        print("-" * 60)
        
        recommendations = {}
        
        for class_name in ALL_CLASSES:
            available_frames = self.total_frames_available.get(class_name, 0)
            base_samples_possible = available_frames // frames_per_audio
            
            # ë°ì´í„° ì¦ê°• ê³ ë ¤
            if AUGMENTATION_CONFIG.get(class_name, {}).get('enabled', False):
                max_aug = AUGMENTATION_CONFIG[class_name]['max_augmentations']
                augmented_samples_possible = base_samples_possible * (1 + max_aug)
            else:
                augmented_samples_possible = base_samples_possible
            
            # í•„ìš”í•œ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
            needed_samples = target_frames // frames_per_audio
            
            # ì „í™˜ ë°ì´í„° ê¸°ì—¬ë¶„ ê³„ì‚°
            transition_contribution = self._calculate_transition_contribution(class_name, frames_per_audio)
            actual_needed = max(0, needed_samples - transition_contribution)
            
            recommendations[class_name] = {
                'available_frames': available_frames,
                'base_samples_possible': base_samples_possible,
                'augmented_samples_possible': augmented_samples_possible,
                'needed_samples': needed_samples,
                'transition_contribution': transition_contribution,
                'actual_needed': actual_needed,
                'need_augmentation': actual_needed > base_samples_possible,
                'feasible': actual_needed <= augmented_samples_possible or class_name == 'silence'
            }
            
            status = "âœ…" if recommendations[class_name]['feasible'] else "âŒ"
            aug_status = "ì¦ê°•í•„ìš”" if recommendations[class_name]['need_augmentation'] else "ì›ë³¸ì¶©ë¶„"
            
            print(f"{status} {class_name:10} | ê°€ìš©: {base_samples_possible:4d} | í•„ìš”: {actual_needed:4d} | {aug_status}")
            
        print("-" * 60)
        
        # ì „ì²´ ì‹¤í˜„ ê°€ëŠ¥ì„± í™•ì¸
        all_feasible = all(rec['feasible'] for rec in recommendations.values())
        
        if all_feasible:
            print("âœ… ëª¨ë“  í´ë˜ìŠ¤ì˜ ëª©í‘œ í”„ë ˆì„ ìˆ˜ ë‹¬ì„± ê°€ëŠ¥")
        else:
            print("âŒ ì¼ë¶€ í´ë˜ìŠ¤ì˜ ëª©í‘œ í”„ë ˆì„ ìˆ˜ ë‹¬ì„± ë¶ˆê°€ëŠ¥")
            infeasible = [name for name, rec in recommendations.items() if not rec['feasible']]
            print(f"   ë¬¸ì œ í´ë˜ìŠ¤: {', '.join(infeasible)}")
            
        return recommendations
    
    def _calculate_transition_contribution(self, class_name, frames_per_audio):
        """ì „í™˜ ë°ì´í„°ë¡œë¶€í„° í•´ë‹¹ í´ë˜ìŠ¤ê°€ ì–»ì„ ìˆ˜ ìˆëŠ” í”„ë ˆì„ ìˆ˜ ê³„ì‚°"""
        if not TRANSITION_CONFIG['enabled']:
            return 0
            
        contribution = 0
        transition_ratio = DATA_GENERATION_CONFIG['transition_data_ratio']
        
        # ê° ì „í™˜ íƒ€ì…ì—ì„œ í•´ë‹¹ í´ë˜ìŠ¤ê°€ ì°¨ì§€í•˜ëŠ” í”„ë ˆì„ ê³„ì‚°
        for trans_type, config in TRANSITION_CONFIG['types'].items():
            if not config['enabled']:
                continue
                
            if trans_type == 'silence_to_silence' and class_name == 'silence':
                contribution += frames_per_audio * config['weight'] * transition_ratio
            elif trans_type == 'silence_to_factory' and class_name in ['silence', 'factory']:
                # ì „í™˜ ë°ì´í„°ì—ì„œ ê° í´ë˜ìŠ¤ê°€ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨ ì¶”ì • (50:50)
                contribution += frames_per_audio * config['weight'] * transition_ratio * 0.5
            elif trans_type == 'silence_to_danger' and (class_name == 'silence' or class_name in ACTIVE_DANGER_CLASSES):
                contribution += frames_per_audio * config['weight'] * transition_ratio * 0.5
            elif trans_type == 'factory_to_factory' and class_name == 'factory':
                contribution += frames_per_audio * config['weight'] * transition_ratio
            elif trans_type == 'factory_to_danger' and (class_name == 'factory' or class_name in ACTIVE_DANGER_CLASSES):
                contribution += frames_per_audio * config['weight'] * transition_ratio * 0.5
                
        return int(contribution)
    
    def get_user_input_for_samples(self, recommendations):
        """ì‚¬ìš©ìë¡œë¶€í„° ì‹¤ì œ ìƒì„±í•  ìƒ˜í”Œ ìˆ˜ ì…ë ¥ë°›ê¸°"""
        if not DATA_GENERATION_CONFIG['allow_user_input']:
            # ìë™ìœ¼ë¡œ ê¶Œì¥ê°’ ì‚¬ìš©
            return {name: rec['actual_needed'] for name, rec in recommendations.items()}
        
        print(f"\nğŸ¯ ê° í´ë˜ìŠ¤ë³„ ìƒì„±í•  ìƒ˜í”Œ ìˆ˜ë¥¼ ê²°ì •í•´ì£¼ì„¸ìš”:")
        print(f"ğŸ’¡ ëª©í‘œ: í´ë˜ìŠ¤ë‹¹ {DATA_GENERATION_CONFIG['target_frames_per_class']}ê°œ í”„ë ˆì„")
        print(f"ğŸ“ ì˜¤ë””ì˜¤ë‹¹ í”„ë ˆì„: {get_audio_frames_count()}ê°œ")
        print("-" * 60)
        
        user_samples = {}
        
        for class_name in ALL_CLASSES:
            rec = recommendations[class_name]
            
            print(f"\nğŸ“ {class_name} ({CLASS_NAMES.get(class_name, class_name)}):")
            print(f"  - ê°€ìš© ì›ë³¸ ìƒ˜í”Œ: {rec['base_samples_possible']}ê°œ")
            print(f"  - ì¦ê°• í¬í•¨ ìµœëŒ€: {rec['augmented_samples_possible']}ê°œ")
            print(f"  - ê¶Œì¥ ìƒì„± ìˆ˜: {rec['actual_needed']}ê°œ")
            print(f"  - ì „í™˜ ë°ì´í„° ê¸°ì—¬: {rec['transition_contribution']:.0f}ê°œ í”„ë ˆì„")
            
            while True:
                try:
                    user_input = input(f"  ğŸ‘‰ ìƒì„±í•  ìƒ˜í”Œ ìˆ˜ (ê¶Œì¥: {rec['actual_needed']}): ").strip()
                    
                    if user_input == "":
                        samples = rec['actual_needed']
                    else:
                        samples = int(user_input)
                    
                    if samples < 0:
                        print("     âŒ ìŒìˆ˜ëŠ” ì…ë ¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        continue
                    
                    if samples > rec['augmented_samples_possible'] and class_name != 'silence':
                        print(f"     âŒ ìµœëŒ€ {rec['augmented_samples_possible']}ê°œê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                        continue
                    
                    user_samples[class_name] = samples
                    print(f"     âœ… {samples}ê°œ ì„¤ì •ë¨")
                    break
                    
                except ValueError:
                    print("     âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                except KeyboardInterrupt:
                    print("\n\nğŸ‘‹ ì‚¬ìš©ì ì¤‘ë‹¨")
                    return None
        
        # ìµœì¢… í™•ì¸
        print(f"\nğŸ“‹ ìµœì¢… ë°ì´í„° ìƒì„± ê³„íš:")
        total_samples = sum(user_samples.values())
        for class_name, samples in user_samples.items():
            frames = samples * get_audio_frames_count() + self._calculate_transition_contribution(class_name, get_audio_frames_count())
            print(f"  - {class_name}: {samples}ê°œ ìƒ˜í”Œ â†’ ì•½ {frames:.0f}ê°œ í”„ë ˆì„")
        
        print(f"\nì´ {total_samples}ê°œ ìƒ˜í”Œ ìƒì„± ì˜ˆì •")
        
        confirm = input("\nê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if confirm not in ['y', 'yes']:
            print("ğŸ‘‹ ì¤‘ë‹¨ë¨")
            return None
            
        return user_samples
    
    def generate_silence_audio(self, duration):
        """ë¬´ìŒ ì˜¤ë””ì˜¤ ìƒì„± (ë‹¤ì–‘í•œ ë°°ê²½ë…¸ì´ì¦ˆ í¬í•¨)"""
        sr = MODEL_CONFIG['sample_rate']
        length = int(duration * sr)
        
        # ê¸°ë³¸ ì €ì¡ìŒ
        noise_level = np.random.uniform(0.001, 0.005)
        audio = np.random.normal(0, noise_level, length)
        
        # ë°°ê²½ ë…¸ì´ì¦ˆ ì¶”ê°€ (í™•ë¥ ì )
        if np.random.random() < 0.3:  # 30% í™•ë¥ ë¡œ ë°°ê²½ë…¸ì´ì¦ˆ ì¶”ê°€
            noise_type = np.random.choice(['white', 'pink', 'brown'])
            noise_level = np.random.uniform(0.005, 0.02)
            
            if noise_type == 'white':
                noise = np.random.normal(0, noise_level, length)
            elif noise_type == 'pink':
                # í•‘í¬ ë…¸ì´ì¦ˆ ìƒì„± (1/f íŠ¹ì„±)
                freqs = np.fft.fftfreq(length, 1/sr)
                freqs[0] = 1  # DC ì„±ë¶„ ë°©ì§€
                pink_filter = 1 / np.sqrt(np.abs(freqs))
                white_noise = np.random.normal(0, 1, length)
                pink_fft = np.fft.fft(white_noise) * pink_filter
                noise = np.real(np.fft.ifft(pink_fft)) * noise_level
            else:  # brown
                # ë¸Œë¼ìš´ ë…¸ì´ì¦ˆ ìƒì„± (1/f^2 íŠ¹ì„±)
                freqs = np.fft.fftfreq(length, 1/sr)
                freqs[0] = 1
                brown_filter = 1 / np.abs(freqs)
                white_noise = np.random.normal(0, 1, length)
                brown_fft = np.fft.fft(white_noise) * brown_filter
                noise = np.real(np.fft.ifft(brown_fft)) * noise_level
            
            audio += noise
        
        # í´ë¦¬í•‘ ë°©ì§€
        max_val = np.max(np.abs(audio))
        if max_val > 0.95:
            audio = audio * (0.95 / max_val)
            
        return audio.astype(np.float32)
    
    def load_audio_file(self, file_path):
        """ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ"""
        try:
            audio, sr = librosa.load(file_path, sr=MODEL_CONFIG['sample_rate'])
            return audio
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")
            return None
    
    def extract_audio_segment(self, audio, duration=None):
        """ì˜¤ë””ì˜¤ì—ì„œ ì§€ì •ëœ ê¸¸ì´ì˜ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ"""
        if duration is None:
            duration = MODEL_CONFIG['audio_duration']
            
        sr = MODEL_CONFIG['sample_rate']
        target_length = int(duration * sr)
        
        if len(audio) >= target_length:
            # ëœë¤ ìœ„ì¹˜ì—ì„œ ì¶”ì¶œ
            start_idx = np.random.randint(0, len(audio) - target_length + 1)
            return audio[start_idx:start_idx + target_length]
        else:
            # íŒ¨ë”© ë˜ëŠ” ë°˜ë³µ
            if len(audio) < target_length // 2:
                # ë„ˆë¬´ ì§§ìœ¼ë©´ ë°˜ë³µ
                repeat_count = (target_length // len(audio)) + 1
                audio = np.tile(audio, repeat_count)
            
            # ë¶€ì¡±í•œ ë¶€ë¶„ì€ ì œë¡œ íŒ¨ë”©
            padding = target_length - len(audio)
            if padding > 0:
                pad_left = padding // 2
                pad_right = padding - pad_left
                audio = np.pad(audio, (pad_left, pad_right), mode='constant')
            
            return audio[:target_length]
    
    def apply_augmentation(self, audio, class_name, method):
        """ë°ì´í„° ì¦ê°• ì ìš©"""
        config = AUGMENTATION_CONFIG.get(class_name, {})
        if not config.get('enabled', False):
            return audio
            
        sr = MODEL_CONFIG['sample_rate']
        augmented = audio.copy()
        
        if method == 'volume_change':
            vol_min, vol_max = config.get('volume_range', (0.7, 1.3))
            volume_factor = np.random.uniform(vol_min, vol_max)
            augmented = augmented * volume_factor
            
        elif method == 'noise_variation' and class_name == 'silence':
            # ë¬´ìŒ í´ë˜ìŠ¤ì˜ ë…¸ì´ì¦ˆ ë³€í™”
            noise_types = config.get('noise_types', ['white'])
            noise_type = np.random.choice(noise_types)
            noise_level = np.random.uniform(0.001, 0.01)
            
            if noise_type == 'white':
                noise = np.random.normal(0, noise_level, len(augmented))
            elif noise_type == 'pink':
                freqs = np.fft.fftfreq(len(augmented), 1/sr)
                freqs[0] = 1
                pink_filter = 1 / np.sqrt(np.abs(freqs))
                white_noise = np.random.normal(0, 1, len(augmented))
                pink_fft = np.fft.fft(white_noise) * pink_filter
                noise = np.real(np.fft.ifft(pink_fft)) * noise_level
            else:  # brown
                freqs = np.fft.fftfreq(len(augmented), 1/sr)
                freqs[0] = 1
                brown_filter = 1 / np.abs(freqs)
                white_noise = np.random.normal(0, 1, len(augmented))
                brown_fft = np.fft.fft(white_noise) * brown_filter
                noise = np.real(np.fft.ifft(brown_fft)) * noise_level
            
            augmented = noise  # ë¬´ìŒì˜ ê²½ìš° ë…¸ì´ì¦ˆë¡œ ëŒ€ì²´
            
        elif method == 'reverb':
            # ê°„ë‹¨í•œ ë¦¬ë²„ë¸Œ íš¨ê³¼
            decay = np.random.uniform(*config.get('reverb_decay', (0.1, 0.5)))
            delay_samples = int(0.05 * sr)  # 50ms ì§€ì—°
            
            reverb = np.zeros_like(augmented)
            for i in range(3):  # 3íšŒ ë°˜ë³µ
                delay = delay_samples * (i + 1)
                amplitude = decay ** (i + 1)
                if delay < len(augmented):
                    reverb[delay:] += augmented[:-delay] * amplitude
            
            augmented = augmented + reverb * 0.3
            
        elif method == 'room_effect':
            # ë£¸ íš¨ê³¼ (ê°„ë‹¨í•œ IIR í•„í„°)
            room_size = np.random.uniform(*config.get('room_size', (0.1, 0.9)))
            try:
                b, a = signal.butter(2, 0.1 + room_size * 0.4, btype='low')
                augmented = signal.filtfilt(b, a, augmented)
            except Exception:
                # í•„í„° ìƒì„± ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
                pass
            
        elif method == 'speed_change':
            # ì†ë„ ë³€í™” (ì‹œê°„ ìŠ¤íŠ¸ë ˆì¹­)
            speed_min, speed_max = config.get('speed_range', (0.9, 1.1))
            speed_factor = np.random.uniform(speed_min, speed_max)
            augmented = librosa.effects.time_stretch(augmented, rate=speed_factor)
            # ê¸¸ì´ ì¡°ì •
            augmented = self.extract_audio_segment(augmented, MODEL_CONFIG['audio_duration'])
            
        elif method == 'noise_add':
            # ë…¸ì´ì¦ˆ ì¶”ê°€
            noise_level = np.random.uniform(*config.get('noise_level', (0.01, 0.05)))
            noise = np.random.normal(0, noise_level, len(augmented))
            augmented = augmented + noise
            
        elif method == 'factory_mix':
            # ê³µì¥ì†Œë¦¬ì™€ í˜¼í•© (ìœ„í—˜ì†Œë¦¬ìš©)
            if 'factory' in self.audio_files and self.audio_files['factory']:
                factory_file = np.random.choice(self.audio_files['factory'])
                factory_audio = self.load_audio_file(factory_file)
                if factory_audio is not None:
                    factory_segment = self.extract_audio_segment(factory_audio, MODEL_CONFIG['audio_duration'])
                    
                    # SNR ê³„ì‚°
                    snr_min, snr_max = config.get('snr_range', (5, 20))
                    target_snr = np.random.uniform(snr_min, snr_max)
                    
                    # ì‹ í˜¸ì™€ ë…¸ì´ì¦ˆ(ê³µì¥ì†Œë¦¬) íŒŒì›Œ ê³„ì‚°
                    signal_power = np.mean(augmented ** 2)
                    noise_power = np.mean(factory_segment ** 2)
                    
                    if noise_power > 0:
                        # SNRì— ë§ëŠ” ìŠ¤ì¼€ì¼ë§ íŒ©í„° ê³„ì‚°
                        snr_linear = 10 ** (target_snr / 10)
                        noise_scale = np.sqrt(signal_power / (noise_power * snr_linear))
                        factory_segment = factory_segment * noise_scale
                        
                        augmented = augmented + factory_segment
        
        # í´ë¦¬í•‘ ë°©ì§€
        max_val = np.max(np.abs(augmented))
        if max_val > 0.95:
            augmented = augmented * (0.95 / max_val)
            
        return augmented
    
    def generate_transition_audio(self, trans_type, class1, class2=None):
        """ì „í™˜ ì˜¤ë””ì˜¤ ìƒì„±"""
        duration = MODEL_CONFIG['audio_duration']
        sr = MODEL_CONFIG['sample_rate']
        length = int(duration * sr)
        
        config = TRANSITION_CONFIG['types'][trans_type]
        trans_point_range = config['transition_point_range']
        transition_point = np.random.uniform(*trans_point_range)
        transition_frame = int(transition_point * length)
        
        fade_duration = TRANSITION_CONFIG['fade_duration']
        fade_samples = int(fade_duration * sr)
        
        # ê¸°ë³¸ ë¬´ìŒìœ¼ë¡œ ì´ˆê¸°í™”
        audio1 = self.generate_silence_audio(duration)
        audio2 = self.generate_silence_audio(duration)
        
        # ì˜¤ë””ì˜¤ ìƒì„±
        if trans_type == 'silence_to_silence':
            # ë‘ ì¢…ë¥˜ì˜ ë¬´ìŒ ìƒì„±
            audio1 = self.generate_silence_audio(duration)
            audio2 = self.generate_silence_audio(duration)
            
        elif trans_type == 'silence_to_factory':
            audio1 = self.generate_silence_audio(duration)
            if 'factory' in self.audio_files and self.audio_files['factory']:
                factory_file = np.random.choice(self.audio_files['factory'])
                factory_audio = self.load_audio_file(factory_file)
                audio2 = self.extract_audio_segment(factory_audio, duration)
            else:
                audio2 = self.generate_silence_audio(duration)
                
        elif trans_type == 'silence_to_danger':
            audio1 = self.generate_silence_audio(duration)
            danger_class = np.random.choice(ACTIVE_DANGER_CLASSES)
            if danger_class in self.audio_files and self.audio_files[danger_class]:
                danger_file = np.random.choice(self.audio_files[danger_class])
                danger_audio = self.load_audio_file(danger_file)
                audio2 = self.extract_audio_segment(danger_audio, duration)
            else:
                audio2 = self.generate_silence_audio(duration)
                
        elif trans_type == 'factory_to_factory':
            if 'factory' in self.audio_files and len(self.audio_files['factory']) >= 2:
                files = np.random.choice(self.audio_files['factory'], 2, replace=False)
                audio1 = self.extract_audio_segment(self.load_audio_file(files[0]), duration)
                audio2 = self.extract_audio_segment(self.load_audio_file(files[1]), duration)
            else:
                audio1 = self.generate_silence_audio(duration)
                audio2 = self.generate_silence_audio(duration)
                
        elif trans_type == 'factory_to_danger':
            if 'factory' in self.audio_files and self.audio_files['factory']:
                factory_file = np.random.choice(self.audio_files['factory'])
                factory_audio = self.load_audio_file(factory_file)
                audio1 = self.extract_audio_segment(factory_audio, duration)
            else:
                audio1 = self.generate_silence_audio(duration)
                
            danger_class = np.random.choice(ACTIVE_DANGER_CLASSES)
            if danger_class in self.audio_files and self.audio_files[danger_class]:
                danger_file = np.random.choice(self.audio_files[danger_class])
                danger_audio = self.load_audio_file(danger_file)
                danger_segment = self.extract_audio_segment(danger_audio, duration)
                
                # ìœ„í—˜ì†Œë¦¬ ë³¼ë¥¨ ì¡°ì ˆ
                vol_min, vol_max = config.get('danger_volume_ratio', (0.8, 1.5))
                volume_ratio = np.random.uniform(vol_min, vol_max)
                danger_segment = danger_segment * volume_ratio
                
                # ê³µì¥ì†Œë¦¬ì— ìœ„í—˜ì†Œë¦¬ ì¶”ê°€ (ë¯¹ì‹±)
                audio2 = audio1 + danger_segment
            else:
                audio2 = audio1
        
        # í˜ì´ë“œ ì „í™˜ ì ìš©
        result = np.zeros(length, dtype=np.float32)
        
        # ì „í™˜ ì „ êµ¬ê°„
        result[:transition_frame] = audio1[:transition_frame]
        
        # ì „í™˜ êµ¬ê°„ (í˜ì´ë“œ)
        fade_start = max(0, transition_frame - fade_samples // 2)
        fade_end = min(length, transition_frame + fade_samples // 2)
        fade_length = fade_end - fade_start
        
        if fade_length > 0:
            fade_curve = np.linspace(0, 1, fade_length)
            result[fade_start:fade_end] = (
                audio1[fade_start:fade_end] * (1 - fade_curve) +
                audio2[fade_start:fade_end] * fade_curve
            )
        
        # ì „í™˜ í›„ êµ¬ê°„
        if transition_frame < length:
            result[transition_frame:] = audio2[transition_frame:]
        
        # í”„ë ˆì„ë³„ ë¼ë²¨ ìƒì„±
        frames_per_audio = get_audio_frames_count()
        transition_frame_idx = int(transition_point * frames_per_audio)
        
        labels = np.zeros(frames_per_audio, dtype=int)
        
        if trans_type == 'silence_to_silence':
            labels[:] = ALL_CLASSES.index('silence')
        elif trans_type == 'silence_to_factory':
            labels[:transition_frame_idx] = ALL_CLASSES.index('silence')
            labels[transition_frame_idx:] = ALL_CLASSES.index('factory')
        elif trans_type == 'silence_to_danger':
            labels[:transition_frame_idx] = ALL_CLASSES.index('silence')
            danger_class = np.random.choice(ACTIVE_DANGER_CLASSES)
            labels[transition_frame_idx:] = ALL_CLASSES.index(danger_class)
        elif trans_type == 'factory_to_factory':
            labels[:] = ALL_CLASSES.index('factory')
        elif trans_type == 'factory_to_danger':
            labels[:transition_frame_idx] = ALL_CLASSES.index('factory')
            danger_class = np.random.choice(ACTIVE_DANGER_CLASSES)
            labels[transition_frame_idx:] = ALL_CLASSES.index(danger_class)
        
        return result, labels
    
    def extract_yamnet_embeddings(self, audio):
        """YAMNet ì„ë² ë”© ì¶”ì¶œ"""
        try:
            if self.yamnet_model is None:
                print("âš ï¸ YAMNet ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
                
            audio_tensor = tf.convert_to_tensor(audio, dtype=tf.float32)
            # YAMNet ëª¨ë¸ í˜¸ì¶œ
            _, embeddings, _ = self.yamnet_model(audio_tensor)
            return embeddings.numpy()
        except Exception as e:
            print(f"âš ï¸ YAMNet ì„ë² ë”© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def generate_dataset(self, sample_counts):
        """ì „ì²´ ë°ì´í„°ì…‹ ìƒì„±"""
        print("\nğŸ­ ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...")
        
        all_embeddings = []
        all_labels = []
        dataset_info = {
            'config': {
                'model_version': MODEL_CONFIG['version'],
                'audio_duration': MODEL_CONFIG['audio_duration'],
                'sample_rate': MODEL_CONFIG['sample_rate'],
                'num_classes': NUM_CLASSES,
                'class_names': ALL_CLASSES
            },
            'generation_stats': {},
            'files_used': {}
        }
        
        total_samples = sum(sample_counts.values())
        pbar = tqdm(total=total_samples, desc="ë°ì´í„° ìƒì„±")
        
        # ê° í´ë˜ìŠ¤ë³„ ê¸°ë³¸ ë°ì´í„° ìƒì„±
        for class_name, target_count in sample_counts.items():
            if target_count == 0:
                continue
                
            print(f"\nğŸ“ {class_name} í´ë˜ìŠ¤ ìƒì„± ì¤‘... (ëª©í‘œ: {target_count}ê°œ)")
            
            class_embeddings = []
            class_labels = []
            files_used = []
            
            class_idx = ALL_CLASSES.index(class_name)
            frames_per_audio = get_audio_frames_count()
            
            if class_name == 'silence':
                # ë¬´ìŒ ë°ì´í„° ìƒì„±
                for i in range(target_count):
                    audio = self.generate_silence_audio(MODEL_CONFIG['audio_duration'])
                    
                    # ë°ì´í„° ì¦ê°• ì ìš© (í™•ë¥ ì )
                    if AUGMENTATION_CONFIG.get(class_name, {}).get('enabled', False):
                        if np.random.random() < 0.5:  # 50% í™•ë¥ ë¡œ ì¦ê°•
                            methods = AUGMENTATION_CONFIG[class_name]['methods']
                            method = np.random.choice(methods)
                            audio = self.apply_augmentation(audio, class_name, method)
                    
                    embeddings = self.extract_yamnet_embeddings(audio)
                    if embeddings is not None:
                        labels = np.full(embeddings.shape[0], class_idx, dtype=int)
                        class_embeddings.append(embeddings)
                        class_labels.append(labels)
                        files_used.append(f"generated_silence_{i}")
                    
                    pbar.update(1)
                    
            else:
                # ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ ì‚¬ìš©
                if class_name not in self.audio_files or not self.audio_files[class_name]:
                    print(f"âš ï¸ {class_name} í´ë˜ìŠ¤ì˜ ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                files = self.audio_files[class_name]
                generated_count = 0
                
                # í•„ìš”í•œ ë§Œí¼ ë°˜ë³µ ìƒì„±
                while generated_count < target_count:
                    for file_path in files:
                        if generated_count >= target_count:
                            break
                            
                        audio = self.load_audio_file(file_path)
                        if audio is None:
                            continue
                        
                        # íŒŒì¼ì´ ì¶©ë¶„íˆ ê¸¸ë©´ ì—¬ëŸ¬ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ ê°€ëŠ¥
                        duration = MODEL_CONFIG['audio_duration']
                        sr = MODEL_CONFIG['sample_rate']
                        target_length = int(duration * sr)
                        
                        num_segments = max(1, len(audio) // target_length)
                        
                        for seg_idx in range(num_segments):
                            if generated_count >= target_count:
                                break
                            
                            # ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
                            if seg_idx == 0:
                                segment = self.extract_audio_segment(audio, duration)
                            else:
                                # ì•½ê°„ì”© ê²¹ì¹˜ê²Œ ì¶”ì¶œ
                                start_idx = int(seg_idx * target_length * 0.8)
                                segment = audio[start_idx:start_idx + target_length]
                                if len(segment) < target_length:
                                    segment = self.extract_audio_segment(audio, duration)
                            
                            # ë°ì´í„° ì¦ê°• ì ìš© (í•„ìš”ì‹œ)
                            if AUGMENTATION_CONFIG.get(class_name, {}).get('enabled', False):
                                # ì¦ê°• í•„ìš”ì„± íŒë‹¨
                                base_samples = len(files) * num_segments
                                if target_count > base_samples:
                                    methods = AUGMENTATION_CONFIG[class_name]['methods']
                                    method = np.random.choice(methods)
                                    segment = self.apply_augmentation(segment, class_name, method)
                            
                            embeddings = self.extract_yamnet_embeddings(segment)
                            if embeddings is not None:
                                labels = np.full(embeddings.shape[0], class_idx, dtype=int)
                                class_embeddings.append(embeddings)
                                class_labels.append(labels)
                                files_used.append(f"{os.path.basename(file_path)}_seg{seg_idx}")
                                generated_count += 1
                            
                            pbar.update(1)
            
            # í´ë˜ìŠ¤ë³„ í†µê³„ ì €ì¥
            if class_embeddings:
                total_frames = sum(emb.shape[0] for emb in class_embeddings)
                dataset_info['generation_stats'][class_name] = {
                    'samples_generated': len(class_embeddings),
                    'total_frames': total_frames,
                    'target_samples': target_count
                }
                dataset_info['files_used'][class_name] = files_used
                
                all_embeddings.extend(class_embeddings)
                all_labels.extend(class_labels)
        
        # ì „í™˜ ë°ì´í„° ìƒì„±
        if TRANSITION_CONFIG['enabled']:
            print(f"\nğŸ”„ ì „í™˜ ë°ì´í„° ìƒì„± ì¤‘...")
            
            transition_ratio = DATA_GENERATION_CONFIG['transition_data_ratio']
            base_samples = len(all_embeddings)
            transition_samples_needed = int(base_samples * transition_ratio / (1 - transition_ratio))
            
            transition_embeddings = []
            transition_labels = []
            transition_files = []
            
            for trans_type, config in TRANSITION_CONFIG['types'].items():
                if not config['enabled']:
                    continue
                    
                type_samples = int(transition_samples_needed * config['weight'] / 
                                 sum(c['weight'] for c in TRANSITION_CONFIG['types'].values() if c['enabled']))
                
                for i in range(type_samples):
                    audio, labels = self.generate_transition_audio(trans_type, None, None)
                    embeddings = self.extract_yamnet_embeddings(audio)
                    
                    if embeddings is not None:
                        # ë¼ë²¨ ê¸¸ì´ ì¡°ì •
                        if len(labels) != embeddings.shape[0]:
                            if len(labels) < embeddings.shape[0]:
                                # íŒ¨ë”©
                                labels = np.pad(labels, (0, embeddings.shape[0] - len(labels)), 
                                              mode='edge')
                            else:
                                # ìë¥´ê¸°
                                labels = labels[:embeddings.shape[0]]
                        
                        transition_embeddings.append(embeddings)
                        transition_labels.append(labels)
                        transition_files.append(f"{trans_type}_{i}")
                        
                        pbar.update(1)
            
            if transition_embeddings:
                all_embeddings.extend(transition_embeddings)
                all_labels.extend(transition_labels)
                
                dataset_info['generation_stats']['transitions'] = {
                    'total_samples': len(transition_embeddings),
                    'total_frames': sum(emb.shape[0] for emb in transition_embeddings),
                    'by_type': {}
                }
                
                for trans_type in TRANSITION_CONFIG['types']:
                    type_count = sum(1 for f in transition_files if f.startswith(trans_type))
                    if type_count > 0:
                        dataset_info['generation_stats']['transitions']['by_type'][trans_type] = type_count
        
        pbar.close()
        
        # ë°ì´í„° ì •ë¦¬ ë° ì…”í”Œ
        print("\nğŸ”„ ë°ì´í„° ì •ë¦¬ ì¤‘...")
        
        # ëª¨ë“  ì„ë² ë”©ê³¼ ë¼ë²¨ì„ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ í•©ì¹˜ê¸°
        if all_embeddings:
            # í”„ë ˆì„ë³„ ë°ì´í„°ë¡œ ë³€í™˜
            frame_embeddings = []
            frame_labels = []
            
            for embeddings, labels in zip(all_embeddings, all_labels):
                for frame_idx in range(embeddings.shape[0]):
                    frame_embeddings.append(embeddings[frame_idx])
                    frame_labels.append(labels[frame_idx])
            
            X = np.array(frame_embeddings)
            y = np.array(frame_labels)
            
            # ì…”í”Œ
            X, y = shuffle(X, y, random_state=42)
            
            # ìµœì¢… í†µê³„
            print(f"\nğŸ“Š ìµœì¢… ë°ì´í„°ì…‹ í†µê³„:")
            print(f"  - ì´ í”„ë ˆì„ ìˆ˜: {len(X):,}ê°œ")
            for class_idx, class_name in enumerate(ALL_CLASSES):
                count = np.sum(y == class_idx)
                percentage = count / len(y) * 100
                print(f"  - {class_name}: {count:,}ê°œ ({percentage:.1f}%)")
            
            dataset_info['final_stats'] = {
                'total_frames': len(X),
                'class_distribution': {
                    class_name: int(np.sum(y == class_idx))
                    for class_idx, class_name in enumerate(ALL_CLASSES)
                }
            }
            
            return X, y, dataset_info
        else:
            print("âŒ ìƒì„±ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None, None

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ YAMNet + LSTM ë°ì´í„° ìƒì„±ê¸°")
    print("=" * 60)
    
    # ì„¤ì • ê²€ì¦
    errors = validate_config()
    if errors:
        print("âŒ ì„¤ì • ì˜¤ë¥˜:")
        for error in errors:
            print(f"  - {error}")
        return
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    create_output_directories()
    
    # ë°ì´í„° ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = DataGenerator()
    
    # ìµœì  ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
    recommendations = generator.calculate_optimal_samples()
    
    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
    sample_counts = generator.get_user_input_for_samples(recommendations)
    if sample_counts is None:
        return
    
    # ë°ì´í„°ì…‹ ìƒì„±
    X, y, dataset_info = generator.generate_dataset(sample_counts)
    
    if X is not None:
        # 3-way ë°ì´í„° ë¶„í•  (train/validation/test)
        print("\nğŸ”„ ë°ì´í„°ë¥¼ train/validation/testë¡œ ë¶„í•  ì¤‘...")
        X_train, X_val, X_test, y_train, y_val, y_test = split_dataset_3way(X, y)
        
        # ë¶„í• ëœ ë°ì´í„°ì…‹ ì €ì¥
        split_paths = save_dataset_splits(X_train, X_val, X_test, y_train, y_val, y_test)
        
        # ë°ì´í„°ì…‹ ì •ë³´ ì €ì¥
        dataset_path = get_dataset_save_path()
        dataset_info['split_info'] = {
            'train_samples': len(X_train),
            'validation_samples': len(X_val),
            'test_samples': len(X_test),
            'train_ratio': TRAINING_CONFIG['data_split']['train_ratio'],
            'validation_ratio': TRAINING_CONFIG['data_split']['validation_ratio'],
            'test_ratio': TRAINING_CONFIG['data_split']['test_ratio'],
            'split_files': split_paths
        }
        
        with open(dataset_path, 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ë°ì´í„°ì…‹ ì •ë³´ ì €ì¥: {dataset_path}")
        
        # ì „ì²´ ë°ì´í„°ë„ ì €ì¥ (í˜¸í™˜ì„± ìœ ì§€)
        data_path = dataset_path.replace('.json', '.npz')
        np.savez_compressed(data_path, X=X, y=y)
        print(f"ğŸ’¾ ì „ì²´ ë°ì´í„° ì €ì¥: {data_path}")
        
        print("\nâœ… ë°ì´í„° ìƒì„± ë° ë¶„í•  ì™„ë£Œ!")
        return data_path, dataset_path, split_paths
    else:
        print("\nâŒ ë°ì´í„° ìƒì„± ì‹¤íŒ¨")
        return None, None, None

if __name__ == "__main__":
    main()
