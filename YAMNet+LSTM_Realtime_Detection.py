import numpy as np
import sounddevice as sd
import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.keras.models import load_model
import time
import os
from scipy import signal

# --- ì„¤ì • ---
SAMPLE_RATE = 16000
DURATION = 10.0  # LSTM ëª¨ë¸ ì…ë ¥ ê¸¸ì´ (10ì´ˆ)
THRESHOLD = 0.7  # ìœ„í—˜ ì†Œë¦¬ ê°ì§€ ì„ê³„ê°’

# --- í´ë˜ìŠ¤ ì •ë³´ (5-í´ë˜ìŠ¤) ---
CLASS_NAMES = ['ë¬´ìŒ', 'ì •ìƒ(ê³µì¥)', 'í™”ì¬', 'ê°€ìŠ¤ëˆ„ì¶œ', 'ë¹„ëª…']
CLASS_COLORS = {
    0: 'ğŸ”‡',  # ë¬´ìŒ
    1: 'ğŸŸ¢',  # ì •ìƒ(ê³µì¥)
    2: 'ğŸ”¥',  # í™”ì¬
    3: 'âš ï¸',  # ê°€ìŠ¤ëˆ„ì¶œ
    4: 'ğŸ˜±'   # ë¹„ëª…
}

# --- YAMNet ëª¨ë¸ ë¡œë“œ ---
print("YAMNet ëª¨ë¸ ë¡œë”© ì¤‘...")
YAMNET_MODEL_HANDLE = 'https://tfhub.dev/google/yamnet/1'
yamnet_model = hub.load(YAMNET_MODEL_HANDLE)
yamnet_fn = yamnet_model.signatures['serving_default']
print("YAMNet ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

# --- í•™ìŠµëœ LSTM ëª¨ë¸ ë¡œë“œ ---
model_path = 'yamnet_lstm_model_5class_with_silence.h5'
if os.path.exists(model_path):
    print("5í´ë˜ìŠ¤ LSTM ëª¨ë¸(ë¬´ìŒ í¬í•¨) ë¡œë”© ì¤‘...")
    lstm_model = load_model(model_path)
    print("5í´ë˜ìŠ¤ LSTM ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
else:
    print(f"ì˜¤ë¥˜: {model_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    print("ë¨¼ì € LSTM_Train_5Class.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    exit(1)

def get_yamnet_embedding(audio):
    waveform = tf.convert_to_tensor(audio, dtype=tf.float32)
    waveform = tf.squeeze(waveform)
    yamnet_output = yamnet_fn(waveform=waveform)
    embeddings = yamnet_output['output_1'].numpy()  # (frames, 1024)
    return embeddings

def get_audio_volume(audio):
    """ì˜¤ë””ì˜¤ì˜ RMS ë³¼ë¥¨ê³¼ ìµœëŒ€ ì ˆëŒ“ê°’ ê³„ì‚°"""
    rms = np.sqrt(np.mean(audio**2))
    max_val = np.max(np.abs(audio))
    return rms, max_val

def detect_clipping(audio, threshold=0.95):
    """ì˜¤ë””ì˜¤ í´ë¦¬í•‘ ê°ì§€"""
    clipped_samples = np.sum(np.abs(audio) >= threshold)
    clipping_ratio = clipped_samples / len(audio)
    return clipping_ratio > 0.01  # 1% ì´ìƒ í´ë¦¬í•‘ë˜ë©´ True

def apply_compressor(audio, threshold=0.7, ratio=4.0, attack=0.003, release=0.1, sample_rate=16000):
    """ê°„ë‹¨í•œ ì»´í”„ë ˆì„œ ì ìš© (ê³¼ë„í•œ ë³¼ë¥¨ ì œì–´)"""
    # ê°„ë‹¨í•œ í”¼í¬ ì œí•œ ì»´í”„ë ˆì„œ
    compressed = audio.copy()
    
    # ì„ê³„ê°’ì„ ë„˜ëŠ” ë¶€ë¶„ì„ ì••ì¶•
    mask = np.abs(audio) > threshold
    if np.any(mask):
        # ì••ì¶• ë¹„ìœ¨ ì ìš©
        compressed[mask] = np.sign(audio[mask]) * (
            threshold + (np.abs(audio[mask]) - threshold) / ratio
        )
    
    return compressed

def normalize_audio_adaptive(audio, target_rms=0.1, max_gain=3.0):
    """ì ì‘í˜• ì˜¤ë””ì˜¤ ì •ê·œí™”"""
    current_rms = np.sqrt(np.mean(audio**2))
    
    if current_rms < 1e-6:  # ê±°ì˜ ë¬´ìŒì¸ ê²½ìš°
        return audio
    
    # ëª©í‘œ RMSì— ë§ì¶° ê²Œì¸ ê³„ì‚°
    gain = target_rms / current_rms
    
    # ìµœëŒ€ ê²Œì¸ ì œí•œ
    gain = min(gain, max_gain)
    
    normalized = audio * gain
    
    # ìµœì¢… í´ë¦¬í•‘ ë°©ì§€
    max_val = np.max(np.abs(normalized))
    if max_val > 0.95:
        normalized = normalized * (0.95 / max_val)
    
    return normalized

def analyze_frequency_content(audio, sample_rate=16000):
    """ì£¼íŒŒìˆ˜ ë¶„ì„ìœ¼ë¡œ ì†Œë¦¬ íŠ¹ì„± íŒŒì•…"""
    # FFT ìˆ˜í–‰
    fft = np.fft.rfft(audio)
    freqs = np.fft.rfftfreq(len(audio), 1/sample_rate)
    magnitude = np.abs(fft)
    
    # ì£¼ìš” ì£¼íŒŒìˆ˜ ëŒ€ì—­ë³„ ì—ë„ˆì§€ ê³„ì‚°
    low_freq_energy = np.sum(magnitude[(freqs >= 50) & (freqs <= 500)])    # ì €ì£¼íŒŒ (ê¸°ê³„ìŒ)
    mid_freq_energy = np.sum(magnitude[(freqs >= 500) & (freqs <= 2000)])   # ì¤‘ì£¼íŒŒ (ì¼ë°˜ ì†ŒìŒ)
    high_freq_energy = np.sum(magnitude[(freqs >= 2000) & (freqs <= 8000)]) # ê³ ì£¼íŒŒ (ë¹„ëª… ë“±)
    
    total_energy = low_freq_energy + mid_freq_energy + high_freq_energy
    
    if total_energy < 1e-6:
        return 0.33, 0.33, 0.34  # ê· ë“± ë¶„ë°°
    
    # ë¹„ìœ¨ ê³„ì‚°
    low_ratio = low_freq_energy / total_energy
    mid_ratio = mid_freq_energy / total_energy
    high_ratio = high_freq_energy / total_energy
    
    return low_ratio, mid_ratio, high_ratio

def preprocess_audio(audio, sample_rate=16000):
    """í†µí•© ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    original_rms, original_max = get_audio_volume(audio)
    
    # 1. í´ë¦¬í•‘ ê°ì§€
    is_clipped = detect_clipping(audio)
    
    # 2. ì»´í”„ë ˆì„œ ì ìš© (ê³¼ë„í•œ ë³¼ë¥¨ ì œì–´)
    if original_max > 0.8 or is_clipped:
        audio = apply_compressor(audio, threshold=0.6, ratio=6.0)
    
    # 3. ì ì‘í˜• ì •ê·œí™”
    if original_rms > 0.001:  # ë¬´ìŒì´ ì•„ë‹Œ ê²½ìš°ë§Œ
        # ë³¼ë¥¨ì´ ë§¤ìš° ë†’ì€ ê²½ìš° ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì •ê·œí™”
        if original_rms > 0.3:
            target_rms = 0.08  # ë” ë‚®ì€ ëª©í‘œ
            max_gain = 1.5     # ê²Œì¸ ì œí•œ
        else:
            target_rms = 0.1
            max_gain = 3.0
            
        audio = normalize_audio_adaptive(audio, target_rms, max_gain)
    
    # 4. ìµœì¢… ì•ˆì „ì¥ì¹˜ (í•˜ë“œ ë¦¬ë¯¸í„°)
    audio = np.clip(audio, -0.95, 0.95)
    
    # ì „ì²˜ë¦¬ ì •ë³´ ë°˜í™˜
    processed_rms, processed_max = get_audio_volume(audio)
    preprocessing_info = {
        'original_rms': original_rms,
        'original_max': original_max,
        'processed_rms': processed_rms,
        'processed_max': processed_max,
        'was_clipped': is_clipped,
        'volume_reduced': original_rms > processed_rms * 1.5
    }
    
    return audio, preprocessing_info

def is_silence(audio, rms_threshold=0.005, max_threshold=0.01):
    """ì˜¤ë””ì˜¤ê°€ ë¬´ìŒì¸ì§€ íŒë‹¨"""
    rms, max_val = get_audio_volume(audio)
    return rms < rms_threshold and max_val < max_threshold

def record_audio(duration, sample_rate):
    print(f"{duration}ì´ˆ ë™ì•ˆ ë…¹ìŒ ì¤‘...")
    audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype='float32')
    sd.wait()
    audio = audio.flatten()
    
    # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì ìš©
    processed_audio, preprocessing_info = preprocess_audio(audio, sample_rate)
    
    return processed_audio, preprocessing_info

def predict_risk(audio, preprocessing_info):
    # ë¨¼ì € ì‹¤ì œ ë³¼ë¥¨ ì²´í¬
    rms, max_val = get_audio_volume(audio)
    if is_silence(audio):
        # ë¬´ìŒìœ¼ë¡œ ì§ì ‘ íŒë‹¨
        return 0, 0.95, np.array([0.95, 0.01, 0.01, 0.01, 0.01]), 0, rms, max_val
    
    # ì£¼íŒŒìˆ˜ ë¶„ì„ìœ¼ë¡œ ì†Œë¦¬ íŠ¹ì„± íŒŒì•…
    low_ratio, mid_ratio, high_ratio = analyze_frequency_content(audio)
    
    embeddings = get_yamnet_embedding(audio)  # (time_steps, 1024)
    
    # ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ íŒ¨ë”© (í›ˆë ¨ ì‹œì™€ ë™ì¼í•œ ê¸¸ì´ë¡œ)
    # ë§Œì•½ ì„ë² ë”©ì´ í›ˆë ¨ ì‹œë³´ë‹¤ ì§§ìœ¼ë©´ íŒ¨ë”©, ê¸¸ë©´ ìë¦„
    target_length = lstm_model.input_shape[1]  # ëª¨ë¸ì˜ time_steps ì°¨ì›
    current_length = embeddings.shape[0]
    
    if current_length < target_length:
        # íŒ¨ë”©
        pad_length = target_length - current_length
        embeddings = np.pad(embeddings, ((0, pad_length), (0, 0)), mode='constant')
    elif current_length > target_length:
        # ìë¥´ê¸°
        embeddings = embeddings[:target_length]
    
    embeddings = np.expand_dims(embeddings, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ (1, time_steps, 1024)
    preds = lstm_model.predict(embeddings, verbose=0)
    preds = preds[0]  # (time_steps, num_classes)
    
    # ê° í´ë˜ìŠ¤ì˜ ìµœëŒ€ í™•ë¥ ê³¼ ìœ„ì¹˜ ì°¾ê¸°
    max_probs = np.max(preds, axis=0)  # ê° í´ë˜ìŠ¤ë³„ ìµœëŒ€ í™•ë¥ 
    overall_max_prob = np.max(max_probs)
    predicted_class = np.argmax(max_probs)
    
    # í”„ë ˆì„ë³„ ì˜ˆì¸¡ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í”„ë ˆì„ ì°¾ê¸°
    max_frame_idx = np.argmax(np.max(preds, axis=1))
    frame_predictions = preds[max_frame_idx]  # í•´ë‹¹ í”„ë ˆì„ì˜ í´ë˜ìŠ¤ë³„ í™•ë¥ 
    
    # ì „ì²˜ë¦¬ ì •ë³´ ê¸°ë°˜ ë³´ì •
    confidence_adjustment = 1.0
    
    # 1. ë¬´ìŒ ìƒí™©ì¸ë° ëª¨ë¸ì´ ë‹¤ë¥¸ í´ë˜ìŠ¤ë¡œ ì˜ˆì¸¡í•œ ê²½ìš° ë³´ì •
    if rms < 0.01 and predicted_class != 0:
        frame_predictions = np.array([0.8, 0.15, 0.02, 0.02, 0.01])
        predicted_class = 0
        overall_max_prob = 0.8
    
    # 2. í´ë¦¬í•‘ì´ë‚˜ ê³¼ë„í•œ ë³¼ë¥¨ ê°ì§€ ì‹œ ì‹ ë¢°ë„ ì¡°ì •
    elif preprocessing_info['was_clipped'] or preprocessing_info['volume_reduced']:
        # ë¹„ëª… í´ë˜ìŠ¤(4ë²ˆ)ì— ëŒ€í•œ ì‹ ë¢°ë„ë¥¼ ë‚®ì¶¤
        if predicted_class == 4:  # ë¹„ëª…ìœ¼ë¡œ ì˜ˆì¸¡ëœ ê²½ìš°
            # ê³ ì£¼íŒŒ ë¹„ìœ¨ì´ ë‚®ìœ¼ë©´ (ê³µì¥ì†Œë¦¬ íŠ¹ì„±) ë¹„ëª… í™•ë¥ ì„ í¬ê²Œ ë‚®ì¶¤
            if high_ratio < 0.3:
                frame_predictions[4] *= 0.3  # ë¹„ëª… í™•ë¥  70% ê°ì†Œ
                frame_predictions[1] *= 2.0  # ì •ìƒ(ê³µì¥) í™•ë¥  ì¦ê°€
                # ì •ê·œí™”
                frame_predictions = frame_predictions / np.sum(frame_predictions)
                
                # ì¬í‰ê°€
                predicted_class = np.argmax(frame_predictions)
                overall_max_prob = np.max(frame_predictions)
        
        confidence_adjustment = 0.8  # ì „ë°˜ì ì¸ ì‹ ë¢°ë„ ë‚®ì¶¤
    
    # 3. ì£¼íŒŒìˆ˜ ë¶„ì„ ê¸°ë°˜ ì¶”ê°€ ê²€ì¦
    if predicted_class == 4:  # ë¹„ëª…ìœ¼ë¡œ ì˜ˆì¸¡ëœ ê²½ìš°
        # ê³µì¥ì†Œë¦¬ íŠ¹ì„± (ì €-ì¤‘ì£¼íŒŒ ìœ„ì£¼) ê°ì§€ ì‹œ ë³´ì •
        if low_ratio + mid_ratio > 0.7 and high_ratio < 0.25:
            # ê³µì¥ì†Œë¦¬ë¡œ ì¬ë¶„ë¥˜
            frame_predictions = np.array([0.1, 0.7, 0.1, 0.05, 0.05])
            predicted_class = 1
            overall_max_prob = 0.7
            confidence_adjustment = 0.9
    
    overall_max_prob *= confidence_adjustment
    
    return predicted_class, overall_max_prob, frame_predictions, max_frame_idx, rms, max_val

def main():
    window_length = DURATION
    stride = 5.0  # 5ì´ˆë§ˆë‹¤ ìœˆë„ìš° ì´ë™
    
    print("ëª¨ë¸ ì •ë³´: 5ê°œ í´ë˜ìŠ¤ (5-í´ë˜ìŠ¤: ë¬´ìŒ í¬í•¨)")
    print("ì‹¤ì‹œê°„ ìœ„í—˜ ì†Œë¦¬ ê°ì§€ ì‹œì‘ (Ctrl+Cë¡œ ì¢…ë£Œ)")
    print("=" * 50)
    
    try:
        while True:
            audio, preprocessing_info = record_audio(window_length, SAMPLE_RATE)
            
            # 5í´ë˜ìŠ¤ ëª¨ë¸ ì˜ˆì¸¡
            predicted_class, max_prob, frame_predictions, max_frame_idx, rms, max_val = predict_risk(audio, preprocessing_info)
            
            # ì£¼íŒŒìˆ˜ ë¶„ì„ ì •ë³´ ì¶”ê°€
            low_ratio, mid_ratio, high_ratio = analyze_frequency_content(audio)
            
            # ê²°ê³¼ ì¶œë ¥
            class_name = CLASS_NAMES[predicted_class] if predicted_class < len(CLASS_NAMES) else f"í´ë˜ìŠ¤{predicted_class}"
            class_icon = CLASS_COLORS.get(predicted_class, 'â“')
            
            print(f"ì‹œê°„: {time.strftime('%H:%M:%S')}")
            print(f"ì˜¤ë””ì˜¤ ë³¼ë¥¨: RMS={rms:.4f}, Max={max_val:.4f}")
            
            # ì „ì²˜ë¦¬ ì •ë³´ ì¶œë ¥
            if preprocessing_info['was_clipped']:
                print("âš ï¸ í´ë¦¬í•‘ ê°ì§€ë¨ - ì‹ ë¢°ë„ ì¡°ì •")
            if preprocessing_info['volume_reduced']:
                print(f"ğŸ”§ ë³¼ë¥¨ ì¡°ì •: {preprocessing_info['original_rms']:.3f} â†’ {preprocessing_info['processed_rms']:.3f}")
            
            print(f"ì£¼íŒŒìˆ˜ ë¶„ì„: ì €ìŒ{low_ratio:.2f} | ì¤‘ìŒ{mid_ratio:.2f} | ê³ ìŒ{high_ratio:.2f}")
            print(f"ì˜ˆì¸¡ ê²°ê³¼: {class_icon} {class_name} (í™•ë¥ : {max_prob:.3f})")
            print(f"ê°ì§€ í”„ë ˆì„: {max_frame_idx}")
            
            # ëª¨ë“  í´ë˜ìŠ¤ë³„ í™•ë¥  ì¶œë ¥
            print("í´ë˜ìŠ¤ë³„ í™•ë¥ :")
            for i, prob in enumerate(frame_predictions):
                if i < len(CLASS_NAMES):
                    name = CLASS_NAMES[i]
                    icon = CLASS_COLORS.get(i, 'â“')
                    print(f"  {icon} {name}: {prob:.3f}")
            
            # ìœ„í—˜ ì†Œë¦¬ ê°ì§€ ì—¬ë¶€ íŒë‹¨ (5í´ë˜ìŠ¤: ë¬´ìŒ(0), ì •ìƒ(1)ì´ ì•„ë‹Œ ê²½ìš°ê°€ ìœ„í—˜)
            is_dangerous = predicted_class >= 2 and max_prob > THRESHOLD
            
            if is_dangerous:
                print(f"\nğŸš¨ ìœ„í—˜ ê°ì§€! {class_icon} {class_name} - í™•ë¥ : {max_prob:.3f}")
                print("ğŸ”” ì•Œë¦¼: ì¦‰ì‹œ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤!")
            elif predicted_class == 0:
                print(f"\nğŸ”‡ ìƒíƒœ: ë¬´ìŒ")
            elif predicted_class == 1:
                print(f"\nâœ… ìƒíƒœ: ì •ìƒ")
            else:
                print(f"\nâš ï¸ ìƒíƒœ: {class_name} ê°ì§€ë¨ (ì„ê³„ê°’ ë¯¸ë§Œ)")
            
            print("-" * 50)
            time.sleep(max(0, stride - window_length))
            
    except KeyboardInterrupt:
        print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == '__main__':
    main()
